
---

## ЁЯУШ ржбржХрзБржорзЗржирзНржЯрзЗрж╢ржи: Computational Graph ржУ Backpropagation

### ЁЯФН ржкрж░рж┐ржЪрж┐рждрж┐

Computational Graph ржПржоржи ржПржХржЯрж┐ ржЧрж╛ржгрж┐рждрж┐ржХ ржХрж╛ржарж╛ржорзЛ, ржпрж╛ complex function-ржХрзЗ ржЫрзЛржЯ ржЫрзЛржЯ operation-ржП ржнрж╛ржЧ ржХрж░рзЗ ржжрзГрж╢рзНржпржорж╛ржиржнрж╛ржмрзЗ ржЙржкрж╕рзНржерж╛ржкржи ржХрж░рзЗред ржирж┐ржЙрж░рж╛рж▓ ржирзЗржЯржУрзЯрж╛рж░рзНржХрзЗ ржПржЯрж┐ forward pass ржУ backward pass ржЯрзНрж░рзНржпрж╛ржХ ржХрж░рждрзЗ ржмрзНржпржмрж╣рж╛рж░ рж╣рзЯред

---

### ЁЯзй ржЙржжрж╛рж╣рж░ржг: \( z = x \times y \)

ржПржХржЯрж┐ рж╕рж╛ржзрж╛рж░ржг computational graph:

```text
    x = 3
      \
       * ---> z = x ├Ч y = 3 ├Ч 4 = 12
      /
    y = 4
```

**Forward Pass:**
- ржЗржиржкрзБржЯ: x=3, y=4
- ржЕржкрж╛рж░рзЗрж╢ржи: z = x ├Ч y тЖТ z = 12

**Backward Pass (Gradient Calculation):**
- \( \frac{\partial z}{\partial x} = y = 4 \)
- \( \frac{\partial z}{\partial y} = x = 3 \)

ржПржЦрж╛ржирзЗ ржЖржорж░рж╛ local gradient ржкрзЗрзЯрзЗржЫрж┐ред

---

### ЁЯФБ Backpropagation ржПрж░ ржкржжржХрзНрж╖рзЗржк

ржирж┐ржЙрж░рж╛рж▓ ржирзЗржЯржУрзЯрж╛рж░рзНржХрзЗ ржмрзНржпрж╛ржХржкрзНрж░рзЛржкрж╛ржЧрзЗрж╢ржирзЗрж░ рзкржЯрж┐ ржзрж╛ржк:

1. **Forward Pass:**
   - ржЗржиржкрзБржЯ тЖТ ржЕржкрж╛рж░рзЗрж╢ржи тЖТ ржЖржЙржЯржкрзБржЯ

2. **Loss Function рж╣рж┐рж╕рж╛ржм:**
   - Prediction ржПрж░ рж╕рж╛ржерзЗ Actual ржПрж░ ржкрж╛рж░рзНржержХрзНржп ржмрзЗрж░ ржХрж░рзЗ

3. **Backward Pass (Gradient рж╣рж┐рж╕рж╛ржм):**
   - Loss ржХрзЗ respect ржХрж░рзЗ weight ржУ bias-ржПрж░ partial derivative ржирзЗржУрзЯрж╛ рж╣рзЯ

4. **Weight Update (Gradient Descent):**
   - $$ w_{\text{new}} = w_{\text{old}} - \eta \cdot \frac{\partial L}{\partial w} $$  
   - \( \eta \) = learning rate

---

### ЁЯУК Gradient Flow: Layer-wise Visualization

```text
Input --> Layer1 --> Layer2 --> ... --> Loss
             тЖС        тЖС
           dw1       dw2       тЖР Gradients flow backward
```

**Gradient Flow back ржХрж░рзЗ:** рж╢рзЗрж╖рзЗрж░ layer ржерзЗржХрзЗ ржкрзНрж░ржержо layer-ржПрж░ ржжрж┐ржХрзЗ weight, bias ржУ activation ржПрж░ respect ржП partial derivatives ржмрзЗрж░ ржХрж░рзЗред

---

### ЁЯза Practical Use: Neural Network Training

- Computational Graph ржмрзНржпрж╛ржХржкрзНрж░рзЛржкрж╛ржЧрзЗрж╢ржирзЗрж░ ржЬржирзНржп ржнрж┐рждрзНрждрж┐ ржкрзНрж░ржжрж╛ржи ржХрж░рзЗ
- Graph-ржПрж░ ржкрзНрж░рждрж┐ржЯрж┐ node-ржПрж░ gradient рж╣рж┐рж╕рж╛ржм ржХрж░рзЗ model рж╢рзЗржЦрзЗ
- ржПржЯрж┐ training efficiency ржПржмржВ scalability ржмрзГржжрзНржзрж┐ ржХрж░рзЗ

---

### ЁЯУЪ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржЯрж╛рж░рзНржорж╕

| Term | ржмрзНржпрж╛ржЦрзНржпрж╛ |
|------|---------|
| Node | ржПржХржЯрж┐ ржЕржкрж╛рж░рзЗрж╢ржи ржмрж╛ ржнрзЗрж░рж┐рзЯрзЗржмрж▓ |
| Edge | ржЕржкрж╛рж░рзЗрж╢ржирзЗрж░ input-output рж╕ржВржпрзЛржЧ |
| Local Gradient | ржПржХржЯрж┐ node ржПрж░ gradient |
| Global Gradient | ржЪрзВрзЬрж╛ржирзНржд loss respect ржП gradient |
| Chain Rule | gradient ржХрзЗ propagate ржХрж░рж╛рж░ ржирж┐рзЯржо |
| Loss Function | prediction accuracy ржкрж░рж┐ржорж╛ржкржХ |

---

### тЬЕ рж╢рзЗрж╖ ржХржерж╛

Computational Graph рж╣рж▓ ржирж┐ржЙрж░рж╛рж▓ ржирзЗржЯржУрзЯрж╛рж░рзНржХ рж╢рзЗржЦрж╛рж░ ржЬржирзНржп ржЧрж╛ржгрж┐рждрж┐ржХ ржорзНржпрж╛ржк, ржпрж╛рж░ ржорж╛ржзрзНржпржорзЗ ржмрзНржпрж╛ржХржкрзНрж░рзЛржкрж╛ржЧрзЗрж╢ржи ржЪрж╛рж▓рж┐рзЯрзЗ weight update ржХрж░рж╛ ржпрж╛рзЯред

