
---

## ЁЯУШ **ржмрж┐рж╕рзНрждрж╛рж░рж┐ржд ржбржХрзБржорзЗржирзНржЯрзЗрж╢ржи: Computational Graph ржУ Backpropagation**

---

### ЁЯФН **ржкрж░рж┐ржЪрж┐рждрж┐**

**Computational Graph** рж╣рж▓рзЛ ржПржХ ржзрж░ржирзЗрж░ ржЧрж╛ржгрж┐рждрж┐ржХ ржбрзЗржЯрж╛-рж╕рзНржЯрзНрж░рж╛ржХржЪрж╛рж░ ржпрж╛ ржПржХржЯрж┐ ржЬржЯрж┐рж▓ ржлрж╛ржВрж╢ржиржХрзЗ ржЫрзЛржЯ ржЫрзЛржЯ ржЧрж╛ржгрж┐рждрж┐ржХ ржЕржкрж╛рж░рзЗрж╢ржирзЗ ржнрж╛ржЧ ржХрж░рзЗ ржПржХржЯрж┐ ржЧрзНрж░рж╛ржлрзЗрж░ ржорждрзЛ ржХрж░рзЗ ржЙржкрж╕рзНржерж╛ржкржи ржХрж░рзЗред ржирж┐ржЙрж░рж╛рж▓ ржирзЗржЯржУрзЯрж╛рж░рзНржХрзЗ ржПржЯрж┐ ржмрзНржпржмрж╣рзГржд рж╣рзЯ ржорзВрж▓ржд:

* **Forward Pass**: ржЖржЙржЯржкрзБржЯ ржХрзАржнрж╛ржмрзЗ ржЖрж╕ржЫрзЗ, рждрж╛ ржирж┐рж░рзНржгрзЯ ржХрж░рждрзЗ
* **Backward Pass**: Error ржмрж╛ Loss ржХрж┐ржнрж╛ржмрзЗ propagate рж╣ржЪрзНржЫрзЗ, рждрж╛ ржмрзЛржЭрж╛рждрзЗ

ржкрзНрж░рждрж┐ржЯрж┐ **Node** ржмрзЛржЭрж╛рзЯ ржПржХржЯрж┐ ржЕржкрж╛рж░рзЗрж╢ржи (ржпрзЗржоржи ржпрзЛржЧ, ржЧрзБржг) ржЕржержмрж╛ ржнрзЗрж░рж┐рзЯрзЗржмрж▓ (ржпрзЗржоржи x, y)ред ржЖрж░ **Edge** ржмрзЛржЭрж╛рзЯ data flow ржмрж╛ ржЕржкрж╛рж░рзЗрж╢ржирзЗрж░ input/outputред

---

### ЁЯзо **ржЙржжрж╛рж╣рж░ржг рзз: $z = x \times y$**

**Computation Graph:**

```
     x = 3
       \
        * ---> z = x ├Ч y = 3 ├Ч 4 = 12
       /
     y = 4
```

#### тЮд Forward Pass:

* ржЗржиржкрзБржЯ: x = 3, y = 4
* ржЕржкрж╛рж░рзЗрж╢ржи: z = x ├Ч y
* ржЖржЙржЯржкрзБржЯ: z = 12

#### тЮд Backward Pass:

ржЖржорж░рж╛ ржЬрж╛ржирждрзЗ ржЪрж╛ржЗ, **Loss L** ржПрж░ respect-ржП $\frac{\partial L}{\partial x}$ ржПржмржВ $\frac{\partial L}{\partial y}$ ржХржд рж╣ржмрзЗред

ржПржЦрж╛ржирзЗ, ржпржжрж┐ ржЖржорж░рж╛ ржЬрж╛ржирж┐ $\frac{\partial L}{\partial z} = 1$, рждрж╛рж╣рж▓рзЗ Chain Rule ржЕржирзБржпрж╛рзЯрзА:

* $\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial x} = 1 \cdot y = 4$
* $\frac{\partial L}{\partial y} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial y} = 1 \cdot x = 3$

---

### ЁЯФБ **Backpropagation ржПрж░ ржзрж╛ржкрж╕ржорзВрж╣**

Backpropagation рж╣рж▓ ржПржоржи ржПржХржЯрж┐ ржЕрзНржпрж╛рж▓ржЧрж░рж┐ржжржо, ржпрж╛ Gradient Descent ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржирж┐ржЙрж░рж╛рж▓ ржирзЗржЯржУрзЯрж╛рж░рзНржХрзЗ ржУржЬржи (weights) рж╣рж╛рж▓ржирж╛ржЧрж╛ржж ржХрж░рзЗред

#### тЬЕ ржзрж╛ржк рзз: Forward Pass

ржЗржиржкрзБржЯ ржерзЗржХрзЗ ржЖржЙржЯржкрзБржЯ ржкрж░рзНржпржирзНржд ржлрж┐ржб-ржлрж░ржУрзЯрж╛рж░рзНржб рж╣рзЯ, ржкрзНрж░рждрзНржпрзЗржХржЯрж┐ рж▓рзЗрзЯрж╛рж░рзЗ ржЕрзНржпрж╛ржХрзНржЯрж┐ржнрзЗрж╢ржи ржУ ржУржЬржи ржкрзНрж░ржпрж╝рзЛржЧ ржХрж░рзЗред

#### тЬЕ ржзрж╛ржк рзи: Loss рж╣рж┐рж╕рж╛ржм

Prediction ржПрж░ рж╕рж╛ржерзЗ Actual Label ржПрж░ ржкрж╛рж░рзНржержХрзНржп ржорж╛ржкрж╛ рж╣рзЯ:

$$
L = \text{Loss}(y_{\text{true}}, y_{\text{pred}})
$$

#### тЬЕ ржзрж╛ржк рзй: Backward Pass (Gradient рж╣рж┐рж╕рж╛ржм)

Chain Rule ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ Loss ржПрж░ respect-ржП ржкрзНрж░рждрж┐ржЯрж┐ Weight ржПрж░ Partial Derivative ржмрзЗрж░ ржХрж░рж╛ рж╣рзЯ:

$$
\frac{\partial L}{\partial w_i}
$$

#### тЬЕ ржзрж╛ржк рзк: Weight Update (Gradient Descent):

ржкрзНрж░рждрж┐ржЯрж┐ weight ржирждрзБржиржнрж╛ржмрзЗ ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рж╛ рж╣рзЯ:

$$
w_{\text{new}} = w_{\text{old}} - \eta \cdot \frac{\partial L}{\partial w}
$$

ржПржЦрж╛ржирзЗ $\eta$ рж╣рж▓рзЛ Learning Rateред

---

### ЁЯФД **Chain Rule ржПржмржВ Gradient Flow**

ржзрж░рж┐, ржПржХржЯрж┐ ржирзЗржЯржУрзЯрж╛рж░рзНржХ ржЖржЫрзЗ:

```text
x --> [Layer1] --> [Layer2] --> ... --> Loss
        тЖС           тЖС
       dw1         dw2        тЖР Backward direction
```

Chain Rule ржЕржирзБрж╕рж╛рж░рзЗ:

$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial y} \cdot \frac{\partial y}{\partial x}
$$

ржПржнрж╛ржмрзЗ Gradient ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ propagate рж╣рзЯред

---

### ЁЯУК **ржПржХржЯрж┐ ржмрзЬ Computational Graph ржПрж░ ржЙржжрж╛рж╣рж░ржг**

ржзрж░рж┐:

$$
a = x \times y \\
b = a + z \\
c = \text{ReLU}(b)
$$

**Forward Pass:**

1. $a = x \times y$
2. $b = a + z$
3. $c = \max(0, b)$

**Backward Pass:**

1. $\frac{\partial c}{\partial b} = 1$ if $b > 0$, otherwise 0
2. $\frac{\partial b}{\partial a} = 1$, $\frac{\partial b}{\partial z} = 1$
3. $\frac{\partial a}{\partial x} = y$, $\frac{\partial a}{\partial y} = x$

**Final Gradient (Chain Rule):**

$$
\frac{\partial c}{\partial x} = \frac{\partial c}{\partial b} \cdot \frac{\partial b}{\partial a} \cdot \frac{\partial a}{\partial x} = \delta \cdot 1 \cdot y
$$

---

### ЁЯУЪ **ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржЯрж╛рж░рзНржорж╕**

| ржЯрж╛рж░рзНржо               | ржмрзНржпрж╛ржЦрзНржпрж╛                                              |
| ------------------- | ----------------------------------------------------- |
| **Node**            | ржПржХржЯрж┐ ржЕржкрж╛рж░рзЗрж╢ржи ржмрж╛ ржорж╛ржи (ржпрзЗржоржи: x, +, log)                 |
| **Edge**            | ржПржХржЯрж┐ ржнрзНржпрж╛рж░рж┐рзЯрзЗржмрж▓ ржпрзЗржЯрж╛ ржжрзБржЯрж┐ Node ржПрж░ ржоржзрзНржпрзЗ ржлрзНрж▓рзЛ ржХрж░рзЗ      |
| **Local Gradient**  | ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржПржХржЯрж┐ ржЕржкрж╛рж░рзЗрж╢ржи ржПрж░ gradient                    |
| **Global Gradient** | Loss function respect-ржП ржЪрзВрзЬрж╛ржирзНржд gradient              |
| **Chain Rule**      | Derivative propagate ржХрж░рж╛рж░ ржЬржирзНржп ржмрзНржпржмрж╣рзГржд ржирж┐рзЯржо           |
| **Loss Function**   | Prediction ржХрждржЯрж╛ ржнрзБрж▓ рж╣рзЯрзЗржЫрзЗ рждрж╛ ржирж┐рж░рзНржгрзЯржХрж╛рж░рзА ржлрж╛ржВрж╢ржи         |
| **Learning Rate**   | Weight ржкрж░рж┐ржмрж░рзНрждржирзЗрж░ ржЧрждрж┐ ржирж┐рзЯржирзНрждрзНрж░ржгржХрж╛рж░рзА рж╣рж╛ржЗржкрж╛рж░ржкрзНржпрж╛рж░рж╛ржорж┐ржЯрж╛рж░ |

---

### ЁЯза **Practical ржмрзНржпржмрж╣рж╛рж░: ржирж┐ржЙрж░рж╛рж▓ ржирзЗржЯржУрзЯрж╛рж░рзНржХ ржЯрзНрж░рзЗржирж┐ржВ**

* **Computational Graph** ржирж┐ржЙрж░рж╛рж▓ ржирзЗржЯржУрзЯрж╛рж░рзНржХ рж╢рзЗржЦрж╛рж░ рж╕ржорзЯ ржкрзНрж░рждрж┐ржЯрж┐ ржзрж╛ржкрзЗ ржХрзЛржерж╛рзЯ ржХрзА ржЕржкрж╛рж░рзЗрж╢ржи рж╣ржЪрзНржЫрзЗ рждрж╛ ржмрзЛржЭрж╛рзЯред
* **Backpropagation** ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржкрзНрж░рждрж┐ржЯрж┐ Layer ржПрж░ weight/bias update ржХрж░рзЗред
* **Auto-differentiation (ржпрзЗржоржи PyTorch, TensorFlow)** computational graph ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ gradient рж╕рзНржмрзЯржВржХрзНрж░рж┐рзЯржнрж╛ржмрзЗ ржмрзЗрж░ ржХрж░рзЗред

---

### тЬЕ **рж╢рзЗрж╖ ржХржерж╛**

* Computational Graph рж╣рж▓рзЛ ржирж┐ржЙрж░рж╛рж▓ ржирзЗржЯржУрзЯрж╛рж░рзНржХрзЗрж░ ржорж╕рзНрждрж┐рж╖рзНржХрж╕рзНржмрж░рзВржк, ржпрж╛ ржкрзНрж░рждрж┐ржЯрж┐ ржЧрж╛ржгрж┐рждрж┐ржХ ржЕржкрж╛рж░рзЗрж╢ржиржХрзЗ ржЯрзНрж░рзНржпрж╛ржХ ржХрж░рзЗред
* Backpropagation ржПржЗ Graph ржПрж░ ржЙржкрж░ ржнрж┐рждрзНрждрж┐ ржХрж░рзЗржЗ ржХрж╛ржЬ ржХрж░рзЗ Gradient Flow рж╣рж┐рж╕рж╛ржм ржХрж░рзЗред
* ржПржЯрж┐ Model Train ржХрж░рж╛рж░ efficiency ржПржмржВ scalability ржЕржирзЗржХ ржЧрзБржг ржмрж╛рзЬрж┐рзЯрзЗ ржжрзЗрзЯред

---


---
