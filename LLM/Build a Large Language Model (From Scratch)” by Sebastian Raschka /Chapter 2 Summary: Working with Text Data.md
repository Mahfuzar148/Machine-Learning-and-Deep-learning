
---

## ðŸ“˜ Chapter 2 Summary: Working with Text Data

à¦à¦‡ à¦…à¦§à§à¦¯à¦¾à§Ÿà§‡ à¦²à§‡à¦–à¦• à¦¦à§‡à¦–à¦¿à§Ÿà§‡à¦›à§‡à¦¨ à¦•à§€à¦­à¦¾à¦¬à§‡ à¦•à¦¾à¦à¦šà¦¾ à¦Ÿà§‡à¦•à§à¦¸à¦Ÿ à¦¡à§‡à¦Ÿà¦¾à¦•à§‡ LLM-à¦à¦° à¦œà¦¨à§à¦¯ à¦ªà§à¦°à¦¸à§‡à¦¸ à¦•à¦°à¦¤à§‡ à¦¹à§Ÿ â€” tokenization à¦¥à§‡à¦•à§‡ à¦¶à§à¦°à§ à¦•à¦°à§‡ embedding à¦“ positional encoding à¦ªà¦°à§à¦¯à¦¨à§à¦¤à¥¤ à¦à¦Ÿà¦¿ LLM-à¦à¦° input pipeline à¦—à¦ à¦¨à§‡à¦° à¦®à§‚à¦² à¦­à¦¿à¦¤à§à¦¤à¦¿à¥¤

---

### ðŸ”¹ 2.1 Understanding Word Embeddings

* Embedding à¦¹à¦²à§‹ à¦Ÿà§‹à¦•à§‡à¦¨à¦•à§‡ dense à¦­à§‡à¦•à§à¦Ÿà¦° à¦¹à¦¿à¦¸à§‡à¦¬à§‡ à¦‰à¦ªà¦¸à§à¦¥à¦¾à¦ªà¦¨ à¦•à¦°à¦¾à¦° à¦‰à¦ªà¦¾à§Ÿà¥¤
* à¦à¦° à¦®à¦¾à¦§à§à¦¯à¦®à§‡ à¦¶à¦¬à§à¦¦à§‡à¦° à¦®à¦§à§à¦¯à§‡ à¦…à¦¨à§à¦¤à¦°à§à¦¨à¦¿à¦¹à¦¿à¦¤ à¦¸à¦®à§à¦ªà¦°à§à¦• à¦“ context à¦¬à§‹à¦à¦¾ à¦¯à¦¾à§Ÿà¥¤
* Embeddings learnable â€” training à¦à¦° à¦®à¦¾à¦§à§à¦¯à¦®à§‡ à¦­à§‡à¦•à§à¦Ÿà¦° à¦®à¦¾à¦¨ à¦¶à¦¿à¦–à§‡ à¦¨à§‡à§Ÿà¥¤

---

### ðŸ”¹ 2.2 Tokenizing Text

* Raw à¦Ÿà§‡à¦•à§à¦¸à¦Ÿ â†’ à¦›à§‹à¦Ÿ à¦›à§‹à¦Ÿ token-à¦ à¦­à¦¾à¦— à¦•à¦°à¦¾ à¦¹à§Ÿà¥¤
* à¦²à§‡à¦–à¦• à¦à¦–à¦¾à¦¨à§‡ tokenizer à¦¤à§ˆà¦°à¦¿à¦° à¦œà¦¨à§à¦¯ Python à¦•à§‹à¦¡ à¦¦à§‡à¦–à¦¿à§Ÿà§‡à¦›à§‡à¦¨à¥¤
* tokenizer whitespace, punctuation, à¦¶à¦¬à§à¦¦à¦­à¦¿à¦¤à§à¦¤à¦¿à¦• à¦Ÿà§‹à¦•à§‡à¦¨ à¦­à¦¾à¦— à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¥¤

---

### ðŸ”¹ 2.3 Converting Tokens into Token IDs

* à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ token à¦•à§‡ vocabulary-à¦à¦° à¦®à¦¾à¦§à§à¦¯à¦®à§‡ à¦à¦•à¦Ÿà¦¿ **à¦¸à¦‚à¦–à§à¦¯à¦¾à¦—à¦¤ ID** à¦¤à§‡ à¦°à§‚à¦ªà¦¾à¦¨à§à¦¤à¦° à¦•à¦°à¦¾ à¦¹à§Ÿà¥¤
* à¦¯à§‡à¦®à¦¨: `["hello", "world"] â†’ [101, 209]`
* Vocabulary mapping list à¦¬à¦¾ dictionary à¦†à¦•à¦¾à¦°à§‡ à¦¥à¦¾à¦•à§‡à¥¤

---

### ðŸ”¹ 2.4 Adding Special Context Tokens

* à¦‰à¦¦à¦¾à¦¹à¦°à¦£: `[BOS]` (Beginning of sequence), `[EOS]` (End of sequence)
* à¦à¦‡ à¦Ÿà§‹à¦•à§‡à¦¨à¦—à§à¦²à§‹ à¦®à¦¡à§‡à¦²à¦•à§‡ à¦‡à¦¨à¦ªà§à¦Ÿà§‡à¦° à¦¶à§à¦°à§ à¦“ à¦¶à§‡à¦· à¦¬à§‹à¦à¦¾à¦¤à§‡ à¦¸à¦¾à¦¹à¦¾à¦¯à§à¦¯ à¦•à¦°à§‡à¥¤
* GPT-à¦à¦° à¦•à§à¦·à§‡à¦¤à§à¦°à§‡ context block à¦¤à§ˆà¦°à¦¿ à¦•à¦°à¦¤à§‡ à¦à¦—à§à¦²à§‹ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£à¥¤

---

### ðŸ”¹ 2.5 Byte Pair Encoding (BPE)

* Subword tokenization-à¦à¦° à¦œà¦¨à§à¦¯ à¦¬à§à¦¯à¦¬à¦¹à§ƒà¦¤ à¦¹à§Ÿà¥¤
* à¦¬à¦¾à¦°à§‡ à¦¬à¦¾à¦°à§‡ à¦†à¦¸à¦¾ à¦šà¦°à¦¿à¦¤à§à¦° à¦œà§‹à§œà¦¾à¦—à§à¦²à§‹ merge à¦•à¦°à§‡ compact token à¦¤à§ˆà¦°à¦¿ à¦•à¦°à§‡à¥¤
* à¦²à§‡à¦–à¦• à¦¦à§‡à¦–à¦¿à§Ÿà§‡à¦›à§‡à¦¨ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦¨à¦¿à¦œà§‡à¦° BPE tokenizer à¦•à§‹à¦¡ à¦•à¦°à¦¤à§‡ à¦¹à§Ÿà¥¤

---

### ðŸ”¹ 2.6 Data Sampling with a Sliding Window

* Long text â†’ overlapping chunks-à¦ à¦­à¦¾à¦— à¦•à¦°à¦¾ à¦¹à§Ÿ (sliding window technique)à¥¤
* à¦•à¦¾à¦°à¦£: à¦®à¦¡à§‡à¦² à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦¬à§œ context à¦¨à¦¿à¦¤à§‡ à¦ªà¦¾à¦°à§‡ à¦¨à¦¾, à¦¤à¦¾à¦‡ window size (ex: 512 tokens) fix à¦•à¦°à§‡à¥¤

**à¦‰à¦¦à¦¾à¦¹à¦°à¦£:**

```
Text: [1â€“512], [257â€“768], [513â€“1024] â†’ overlap
```

---

### ðŸ”¹ 2.7 Creating Token Embeddings

* Token ID â†’ Embedding Matrix â†’ Dense Vector
* Embedding matrix à¦à¦•à¦Ÿà¦¿ learnable layer (PyTorch nn.Embedding)
* à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ ID à¦à¦•à¦Ÿà¦¿ à¦¨à¦¿à¦°à§à¦¦à¦¿à¦·à§à¦Ÿ vector-à¦à¦° à¦®à¦¾à¦§à§à¦¯à¦®à§‡ à¦‰à¦ªà¦¸à§à¦¥à¦¾à¦ªà¦¿à¦¤ à¦¹à§Ÿà¥¤

---

### ðŸ”¹ 2.8 Encoding Word Positions

* Token à¦à¦° à¦ªà¦œà¦¿à¦¶à¦¨ à¦¬à§‹à¦à¦¾à¦¤à§‡ positional encoding à¦¬à§à¦¯à¦¬à¦¹à§ƒà¦¤ à¦¹à§Ÿà¥¤
* GPT static à¦¬à¦¾ sinusoidal encoding à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à¦¤à§‡ à¦ªà¦¾à¦°à§‡à¥¤

**à¦‰à¦¦à¦¾à¦¹à¦°à¦£:**

* à¦Ÿà§‹à¦•à§‡à¦¨ "I", "am", "fine" â†’ 0,1,2 à¦ªà¦œà¦¿à¦¶à¦¨à§‡
* Sin/cos à¦«à¦¾à¦‚à¦¶à¦¨ à¦¦à¦¿à§Ÿà§‡ position vector à¦¤à§ˆà¦°à¦¿ à¦•à¦°à¦¾ à¦¹à§Ÿà¥¤

---

### ðŸ”¹ 2.9 Summary

à¦à¦‡ à¦…à¦§à§à¦¯à¦¾à§Ÿà§‡ à¦†à¦ªà¦¨à¦¿ à¦¶à¦¿à¦–à¦²à§‡à¦¨:

| à¦§à¦¾à¦ª                 | à¦‰à¦¦à§à¦¦à§‡à¦¶à§à¦¯                      |
| ------------------- | ----------------------------- |
| Tokenization        | à¦Ÿà§‡à¦•à§à¦¸à¦Ÿ â†’ subword token à¦ à¦­à¦¾à¦™à¦¾ |
| Vocabulary Mapping  | token â†’ token ID              |
| Embedding           | ID â†’ dense vector             |
| Positional Encoding | à¦Ÿà§‹à¦•à§‡à¦¨à§‡à¦° order à¦¬à§‹à¦à¦¾à¦¨à§‹          |
| Special Tokens      | Context understanding         |
| Sliding Window      | Long text chunking            |

---

## âœ… à¦…à¦§à§à¦¯à¦¾à§Ÿà§‡à¦° à¦šà§‚à§œà¦¾à¦¨à§à¦¤ à¦²à¦•à§à¦·à§à¦¯:

> "To take raw natural language input, convert it into structured, numerical input that a neural network can process."

---
