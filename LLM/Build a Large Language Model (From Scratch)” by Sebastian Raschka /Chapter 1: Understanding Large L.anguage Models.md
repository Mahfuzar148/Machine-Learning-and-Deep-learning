

---

## ЁЯФ╣ рзз.рзз: LLM ржХрзА?

**Large Language Model (LLM)** рж╣рж▓рзЛ ржПржХ ржзрж░ржирзЗрж░ **ржбрж┐ржк ржирж┐ржЙрж░рж╛рж▓ ржирзЗржЯржУрзЯрж╛рж░рзНржХ**, ржпрж╛ ржорж╛ржирзБрж╖рзЗрж░ ржнрж╛рж╖рж╛ ржмрзБржЭрждрзЗ, ржмрж┐рж╢рзНрж▓рзЗрж╖ржг ржХрж░рждрзЗ ржПржмржВ ржирждрзБржиржнрж╛ржмрзЗ рждрзИрж░рж┐ ржХрж░рждрзЗ ржкрж╛рж░рзЗред

### ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржкрзЯрзЗржирзНржЯ:

* ржПржЧрзБрж▓рзЛ **ржмрж┐ржкрзБрж▓ ржкрж░рж┐ржорж╛ржг ржЯрзЗржХрзНрж╕ржЯ ржбрзЗржЯрж╛** ржерзЗржХрзЗ рж╢рзЗржЦрзЗ (ржЗржирзНржЯрж╛рж░ржирзЗржЯ, ржмржЗ, ржЙржЗржХрж┐ржкрж┐ржбрж┐рзЯрж╛ ржЗрждрзНржпрж╛ржжрж┐)ред
* ржоржбрзЗрж▓рзЗрж░ тАЬ**large**тАЭ ржХржерж╛ржЯрж┐ ржмрзЛржЭрж╛рзЯ:

  * ржЕржирзЗржХ **ржкрзНржпрж╛рж░рж╛ржорж┐ржЯрж╛рж░** (ржпрзЗржоржи GPT-3 ржП 175 ржмрж┐рж▓рж┐рзЯржи)
  * ржмрж┐рж╢рж╛рж▓ **ржбрзЗржЯрж╛рж╕рзЗржЯ**
* ржЯрзНрж░рзЗржирж┐ржВрзЯрзЗрж░ рж╕ржорзЯ ржПржЧрзБрж▓рзЛ **next-word prediction** рж╢рзЗржЦрзЗ:

  * ржпрзЗржоржи: тАЬржЖржорж┐ ржЖржЬ ржмрж╛ржЬрж╛рж░рзЗ ржЧрж┐рзЯрзЗржЫрж┐ ржПржмржВ...тАЭ тЖТ ржкрж░ржмрж░рзНрждрзА рж╢ржмрзНржж ржХрзА рж╣ржмрзЗ? ржоржбрзЗрж▓ ржПржЯрж┐ ржЕржирзБржорж╛ржи ржХрж░рзЗред

ЁЯСЙ **ржорзМрж▓рж┐ржХ ржХрж╛ржЬ**: ржнрж╛рж╖рж╛рж░ ржЧржаржи, ржЕрж░рзНрже, ржУ ржкрзНрж░рж╕ржЩрзНржЧ ржмрзЛржЭрж╛ред

---

## ЁЯФ╣ рзз.рзи: LLM-ржПрж░ ржмрзНржпржмрж╣рж╛рж░ (Applications)

**LLM ржПржЦржи ржмрж┐ржнрж┐ржирзНржи ржЬржЧрждрзЗ ржмрж┐ржкрзНрж▓ржм ржПржирзЗржЫрзЗред** ржпрзЗржоржи:

| ржХрж╛ржЬ            | ржЙржжрж╛рж╣рж░ржг                                       |
| -------------- | -------------------------------------------- |
| тЬНя╕П рж▓рзЗржЦрж╛ рждрзИрж░рж┐   | ржХржмрж┐рждрж╛, ржЧрж▓рзНржк, ржЖрж░рзНржЯрж┐ржХрзЗрж▓, ржХрзЛржб                   |
| ЁЯМР ржЕржирзБржмрж╛ржж      | ржЗржВрж░рзЗржЬрж┐ тЖТ ржмрж╛ржВрж▓рж╛                               |
| ЁЯдЦ ржЪрзНржпрж╛ржЯржмржЯ     | ChatGPT, Google Gemini                       |
| ЁЯУД рж╕рж╛рж░рж╛ржВрж╢ рждрзИрж░рж┐ | рж▓ржорзНржмрж╛ ржЖрж░рзНржЯрж┐ржХрзЗрж▓рзЗрж░ рж╕ржВржХрзНрж╖рж┐ржкрзНржд рж░рзВржк               |
| ЁЯУК ржмрж┐рж╢рзНрж▓рзЗрж╖ржг    | рж╕рзЗржирзНржЯрж┐ржорзЗржирзНржЯ ржмрж┐рж╢рзНрж▓рзЗрж╖ржг (ржЗржорзЗржЗрж▓ ржкржЬрж┐ржЯрж┐ржн/ржирзЗржЧрзЗржЯрж┐ржн?) |
| ЁЯза ржЬрзНржЮрж╛ржи ржЦрзЛржБржЬрж╛ | ржорзЗржбрж┐ржХрзЗрж▓, ржЖржЗржи рж╕ржВржХрзНрж░рж╛ржирзНржд ржЬржмрж╛ржм                  |

ЁЯУМ **ржмрж┐рж╢рзЗрж╖рждрзНржм:** LLM ржЧрзБрж▓рзЛ **ржХржиржЯрзЗржХрзНрж╕ржЯ ржзрж░рзЗ рж░рж╛ржЦрждрзЗ ржкрж╛рж░рзЗ** ржПржмржВ ржмрж┐ржнрж┐ржирзНржи ржЯрж╛рж╕рзНржХрзЗ ржПржХрж╕рж╛ржерзЗ ржкрж╛рж░ржжрж░рзНрж╢рзА рж╣рждрзЗ ржкрж╛рж░рзЗред

---

## ЁЯФ╣ рзз.рзй: ржХрж┐ржнрж╛ржмрзЗ LLM ржмрж╛ржирж╛ржирзЛ рж╣рзЯ (Stages of Building LLM)

LLM ржмрж╛ржирж╛ржирзЛрж░ рж╕рж╛ржзрж╛рж░ржг ржзрж╛ржк ржжрзБржЯрж┐:

### рзз. Pretraining:

* **Unlabeled text** (label ржЫрж╛рзЬрж╛) ржирж┐рзЯрзЗ ржоржбрзЗрж▓ ржЯрзНрж░рзЗржирж┐ржВ рж╣рзЯред
* ржЙржжрж╛рж╣рж░ржг: GPT-3 ржХрзЗ ржЗржирзНржЯрж╛рж░ржирзЗржЯ ржерзЗржХрзЗ рж╕ржВржЧрзГрж╣рзАржд рж▓рзЗржЦрж╛ ржжрж┐рзЯрзЗ ржЯрзНрж░рзЗржЗржи ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗред

### рзи. Finetuning:

* ржоржбрзЗрж▓ржХрзЗ **ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржХрж╛ржЬрзЗрж░ ржЬржирзНржп** ржирждрзБржи ржХрж░рзЗ ржкрзНрж░рж╢рж┐ржХрзНрж╖ржг ржжрзЗржУрзЯрж╛ рж╣рзЯред
* ржЙржжрж╛рж╣рж░ржг:

  * ржлрж╛ржЗржирзНржпрж╛ржирзНрж╕ ржмрж┐рж╖рзЯрзЗ BloombergGPT
  * ржорзЗржбрж┐ржХрзЗрж▓ ржмрж┐рж╖рзЯрзЗ ржПржХржЯрж┐ ржорзЗржбрж┐ржХрзЗрж▓ LLM

ЁЯФР **ржЙржкржХрж╛рж░рж┐рждрж╛:**

* ржирж┐ржЬрж╕рзНржм ржбрзЗржЯрж╛рждрзЗ ржЯрзНрж░рзЗржЗржи ржХрж░рж▓рзЗ **privacy control** ржерж╛ржХрзЗред
* ржЪрж╛ржЗрж▓рзЗ ржлрзЛржи/рж▓рзНржпрж╛ржкржЯржкрзЗржУ LLM рж░рж╛ржи ржХрж░рж╛ржирзЛ рж╕ржорзНржнржмред
* latency ржХржорзЗ, cloud ржирж╛ рж▓рж╛ржЧрж▓рзЗржУ ржЪрж▓рзЗред

ЁЯУК ржЪрж┐рждрзНрж░ ржЕржирзБржпрж╛рзЯрзА (Figure 1.3), pretraining ржХрж░рж╛рж░ ржкрж░ ржоржбрзЗрж▓ржХрзЗ instruction ржмрж╛ classification ржЯрж╛рж╕рзНржХрзЗ finetune ржХрж░рж╛ рж╣рзЯред

---

## ЁЯФ╣ рзз.рзк: Transformer Architecture ржкрж░рж┐ржЪрж┐рждрж┐

ржПржЗ ржЕржВрж╢рзЗ ржмрзНржпрж╛ржЦрзНржпрж╛ ржХрж░рж╛ рж╣рзЯ, ржХрзЗржи transformer ржПржд ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг:

### Transformer-ржПрж░ ржжрзБржЗ ржЕржВрж╢:

1. **Encoder** тАФ ржЗржиржкрзБржЯ ржмрзБржЭрзЗ, ржЯрзЛржХрзЗржи рж░рзВржкрзЗ рж░рзВржкрж╛ржирзНрждрж░ ржХрж░рзЗред
2. **Decoder** тАФ ржЖржЙржЯржкрзБржЯ рждрзИрж░рж┐ ржХрж░рзЗ (ржпрзЗржоржи GPT ржХрж░рзЗ)ред

### GPT ржоржбрзЗрж▓ рж╢рзБржзрзБ **decoder** ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗред

### Self-Attention:

* ржкрзНрж░рждрж┐ржЯрж┐ рж╢ржмрзНржж ржХрзАржнрж╛ржмрзЗ ржмрж╛ржХрзНржпржЯрж┐рж░ ржЕржирзНржп рж╢ржмрзНржжрзЗрж░ рж╕рж╛ржерзЗ рж╕ржорзНржкрж░рзНржХрж┐ржд рждрж╛ ржмрзЛржЭрзЗред
* ржЙржжрж╛рж╣рж░ржг: тАЬрж╕рзЗ ржХрж▓рж╛ ржЦрзЗрзЯрзЗржЫрзЗ ржХрж╛рж░ржг рж╕рзЗ ржХрзНрж╖рзБржзрж╛рж░рзНржд ржЫрж┐рж▓редтАЭ тЖТ тАЬрж╕рзЗтАЭ ржХрж╛ржХрзЗ ржирж┐рж░рзНржжрзЗрж╢ ржХрж░ржЫрзЗ, ржПржЗ рж╕ржорзНржкрж░рзНржХ ржмрзЛржЭрзЗред

ЁЯУМ GPT рж╣рж▓рзЛ **auto-regressive decoder**: ржкрзНрж░рждрж┐ржмрж╛рж░ ржПржХрзЗржХржЯрж╛ рж╢ржмрзНржж рждрзИрж░рж┐ ржХрж░рзЗ, ржЖржЧрзЗрж░ржЧрзБрж▓рзЛрж░ ржЙржкрж░ ржнрж┐рждрзНрждрж┐ ржХрж░рзЗред

---

## ЁЯФ╣ рзз.рзл: ржмрзЬ ржбрзЗржЯрж╛рж╕рзЗржЯ ржмрзНржпржмрж╣рж╛рж░рзЗрж░ ржЧрзБрж░рзБрждрзНржм

GPT-3 ржПрж░ training dataset ржЫрж┐рж▓ ржПржЗ ржзрж░ржирзЗрж░:

| ржбрзЗржЯрж╛рж╕рзЗржЯ     | ржЯрзЛржХрзЗржи рж╕ржВржЦрзНржпрж╛ | ржкрж╛рж░рзНрж╕рзЗржирзНржЯрзЗржЬ |
| ----------- | ------------ | ----------- |
| CommonCrawl | 410B         | 60%         |
| WebText2    | 19B          | 22%         |
| Books1      | 12B          | 8%          |
| Books2      | 55B          | 8%          |
| Wikipedia   | 3B           | 3%          |

### Token ржорж╛ржирзЗ ржХрзА?

* Token = рж╢ржмрзНржж ржмрж╛ punctuation (ржпрзЗржоржи: "!" "?" ржЗрждрзНржпрж╛ржжрж┐)

ЁЯУМ GPT-3 ржПрж░ ржЯрзНрж░рзЗржирж┐ржВ ржЦрж░ржЪ тЙИ \$4.6 ржорж┐рж▓рж┐рзЯржи!

---

## ЁЯФ╣ рзз.рзм: GPT ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░рзЗрж░ ржмрж┐рж╢рзНрж▓рзЗрж╖ржг

### GPT ржорж╛ржирзЗ: **Generative Pretrained Transformer**

* GPT-3 = ржмрзЬ рж╕ржВрж╕рзНржХрж░ржг
* ChatGPT = GPT-3 ржХрзЗ **instruction dataset** ржжрж┐рзЯрзЗ finetune ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗ
* ржкрзНрж░рж╢рж┐ржХрзНрж╖ржг ржХрж╛ржЬ = **next-word prediction**

ЁЯСЙ GPT рж╣рж▓рзЛ **decoder-only transformer**
ЁЯСЙ **Autoregressive** ржоржбрзЗрж▓ = ржПржХ ржПржХ ржХрж░рзЗ рж╢ржмрзНржж рждрзИрж░рж┐ ржХрж░рзЗ
ЁЯСЙ ChatGPT-ржПрж░ ржХрзНрж╖ржорждрж╛ ржЕржирзЗржХ **emergent** тАФ ржпрзЗржоржи ржЕржирзБржмрж╛ржж, ржмрзНржпрж╛ржЦрзНржпрж╛, ржХрзМрж╢рж▓ ржмрзЛржЭрж╛ тАФ ржпрж╛ ржЖрж▓рж╛ржжрж╛ ржХрж░рзЗ рж╢рзЗржЦрж╛ржирзЛ рж╣рзЯржирж┐!

---

## ЁЯФ╣ рзз.рзн: ржмржЗрждрзЗ ржЖржорж░рж╛ ржХрзА ржХрзА рж╢рж┐ржЦржм?

ржПржЗ ржмржЗржЯрж┐ рждрж┐ржиржЯрж┐ ржзрж╛ржкрзЗ LLM рж╢рзЗржЦрж╛ржмрзЗ:

### рзз. Data + Attention:

* Tokenization, Embedding
* Attention mechanism (Self, Causal, Multi-head)

### рзи. Pretraining:

* Next-word prediction рж╢рзЗржЦрж╛ржирзЛ
* ржЫрзЛржЯ ржбрзЗржЯрж╛рзЯ ржкрзНрж░рзНржпрж╛ржХржЯрж┐рж╕ ржХрж░рзЗ рж╢рзЗржЦрж╛

### рзй. Finetuning:

* Instruction following (ChatGPT-ржПрж░ ржорждрзЛ)
* Text classification

---

## ЁЯФ╣ рзз.рзо: рж╕рж╛рж░рж╛ржВрж╢ (Summary)

ржПржЗ ржЕржзрзНржпрж╛рзЯ ржерзЗржХрзЗ ржпрж╛ ржЬрж╛ржирж╛ ржЧрзЗрж▓:

* LLMs = ржмрзЬ transformer-based deep models
* Self-supervised pretraining тЖТ Finetuning
* GPT models use decoder block only
* LLMs ржПржЦржи **multi-task** ржжржХрзНрж╖рждрж╛ ржЕрж░рзНржЬржи ржХрж░рзЗржЫрзЗ
* ржЖржорж░рж╛ рж╢рж┐ржЦржм ржХрзАржнрж╛ржмрзЗ ржПржЯрж╛ **ржирж┐ржЬрзЗ ржХрзЛржб ржХрж░рзЗ ржмрж╛ржирж╛ржирзЛ ржпрж╛рзЯ**

---



---

## ЁЯФ╣ рзз.рзз: ржПржХржЯрж┐ LLM (Large Language Model) ржХрзА?

### ЁЯФ░ рж╕ржВржЬрзНржЮрж╛:

ржПржХржЯрж┐ **LLM (Large Language Model)** рж╣рж▓рзЛ **ржбрж┐ржк рж▓рж╛рж░рзНржирж┐ржВ ржнрж┐рждрзНрждрж┐ржХ ржирж┐ржЙрж░рж╛рж▓ ржирзЗржЯржУрзЯрж╛рж░рзНржХ**, ржпрзЗржЯрж┐ **ржмрж┐ржкрзБрж▓ ржкрж░рж┐ржорж╛ржг ржнрж╛рж╖рж╛ржнрж┐рждрзНрждрж┐ржХ ржбрзЗржЯрж╛** ржерзЗржХрзЗ рж╢рзЗржЦрзЗ ржПржмржВ **ржорж╛ржирзБрж╖рзЗрж░ ржорждрзЛ ржЯрзЗржХрзНрж╕ржЯ рждрзИрж░рж┐ ржХрж░рждрзЗ ржкрж╛рж░рзЗ, ржмрзБржЭрждрзЗ ржкрж╛рж░рзЗ ржУ ржмрж┐рж╢рзНрж▓рзЗрж╖ржг ржХрж░рждрзЗ ржкрж╛рж░рзЗ**ред

---

### ЁЯФН LLM-ржПрж░ ржорзВрж▓ ржмрзИрж╢рж┐рж╖рзНржЯрзНржп:

| ржмрзИрж╢рж┐рж╖рзНржЯрзНржп              | ржмрзНржпрж╛ржЦрзНржпрж╛                                                                         |
| ---------------------- | -------------------------------------------------------------------------------- |
| **Large (ржмрзГрж╣рзО)**       | ржПрж░ ржжрзБржЯрж┐ ржжрж┐ржХ ржЖржЫрзЗ: (рзз) ржоржбрзЗрж▓рзЗрж░ ржЖржХрж╛рж░ (parameter рж╕ржВржЦрзНржпрж╛) ржПржмржВ (рзи) ржбрзЗржЯрж╛рж╕рзЗржЯрзЗрж░ ржкрж░рж┐ржорж╛ржг     |
| **Training Objective** | LLM рж╕рж╛ржзрж╛рж░ржгржд **next-word prediction** ржмрж╛ **ржкрж░ржмрж░рзНрждрзА рж╢ржмрзНржж ржЕржирзБржорж╛ржи** ржХрж░рж╛рж░ ржХрж╛ржЬ рж╢рзЗржЦрзЗ    |
| **Generative AI**      | ржХрж╛рж░ржг ржПржЯрж┐ ржирждрзБржи ржХрж┐ржЫрзБ **ржЙрзОржкрж╛ржжржи ржХрж░рждрзЗ ржкрж╛рж░рзЗ** (рж▓рзЗржЦрж╛, ржХрзЛржб, ржЙрждрзНрждрж░ ржЗрждрзНржпрж╛ржжрж┐)               |
| **Transformer-based**  | ржПржЧрзБрж▓рзЛ ржЯрзНрж░рзЗржЗржи рж╣рзЯ **transformer ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░** ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ (attention mechanism рж╕рж╣) |

---

### ЁЯФм ржЙржжрж╛рж╣рж░ржг:

> ржЖржкржирж┐ ржпржжрж┐ рж▓рж┐ржЦрзЗржи:
> **"ржЖржорж┐ ржЖржЬ ржжрзБржкрзБрж░рзЗ..."**
> рждрж╛рж╣рж▓рзЗ LLM рж╢рзЗржЦрзЗ ржХрзАржнрж╛ржмрзЗ ржПржЦрж╛ржирзЗ ржЙржкржпрзБржХрзНржд рж╢ржмрзНржж ржпрзЗржоржи **"ржЦрж╛ржмрж╛рж░ ржЦрзЗрзЯрзЗржЫрж┐"** ржмрж╛ **"ржмрж╛ржЬрж╛рж░рзЗ ржЧрж┐рзЯрзЗржЫрж┐"** ржмрж╕рж╛ржирзЛ ржпрж╛рзЯред

---

### тЪЩя╕П ржХрж┐ржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░рзЗ?

1. **ржмрж┐ржкрзБрж▓ ржкрж░рж┐ржорж╛ржг ржЯрзЗржХрзНрж╕ржЯ ржбрзЗржЯрж╛** (ржЗржирзНржЯрж╛рж░ржирзЗржЯ, ржмржЗ, ржлрзЛрж░рж╛ржо, ржХрзЛржб, ржЦржмрж░ ржЗрждрзНржпрж╛ржжрж┐) ржерзЗржХрзЗ рж╢рзЗржЦрзЗред
2. **ржбрж┐ржк ржирж┐ржЙрж░рж╛рж▓ ржирзЗржЯржУрзЯрж╛рж░рзНржХрзЗ ржЯрзНрж░рзЗржЗржи рж╣рзЯ** тАФ ржпрзЗржЦрж╛ржирзЗ ржХрзЛржЯрж┐ ржХрзЛржЯрж┐ **ржкрзНржпрж╛рж░рж╛ржорж┐ржЯрж╛рж░** ржерж╛ржХрзЗред
3. **"Next-word prediction"** рж╢рзЗржЦрж╛рж░ ржорж╛ржзрзНржпржорзЗ ржоржбрзЗрж▓ **ржнрж╛рж╖рж╛рж░ ржЧржаржи ржУ ржЕрж░рзНрже** ржмрзБржЭрждрзЗ рж╢рзЗржЦрзЗред

> ЁЯза ржЙржжрж╛рж╣рж░ржгрж╕рзНржмрж░рзВржк GPT-3 ржП ржЖржЫрзЗ ржкрзНрж░рж╛рзЯ **рззрзнрзл ржмрж┐рж▓рж┐рзЯржи ржкрзНржпрж╛рж░рж╛ржорж┐ржЯрж╛рж░**ред

---

### ЁЯУИ ржоржЬрж╛рж░ ржжрж┐ржХ:

* ржпржжрж┐ржУ "next-word prediction" рж╢рзБржирждрзЗ рж╕рж╣ржЬ, ржХрж┐ржирзНрждрзБ ржПржЯрж┐ ржжрж┐рзЯрзЗ ржкрзНрж░рж╢рж┐ржХрзНрж╖рж┐ржд ржоржбрзЗрж▓ ржЕрж╕рж╛ржзрж╛рж░ржг ржнрж╛рж╖рж╛ржЧржд ржжржХрзНрж╖рждрж╛ ржжрзЗржЦрж╛рждрзЗ ржкрж╛рж░рзЗред
* ржПржЗ ржкржжрзНржзрждрж┐рждрзЗ ржЯрзНрж░рзЗржЗржи ржХрж░рж╛ ржоржбрзЗрж▓ржЧрзБрж▓рзЛ ржЖржЬржХрзЗрж░ **ржЪрзНржпрж╛ржЯржмржЯ, рж╕рж╛рж░рж╛ржВрж╢ рждрзИрж░рж┐, ржЕржирзБржмрж╛ржж, ржкрзНрж░рж╢рзНржирзЛрждрзНрждрж░** ржЗрждрзНржпрж╛ржжрж┐рждрзЗ ржЕржнрзВрждржкрзВрж░рзНржм рж╕ржлрж▓рждрж╛ ржжрзЗржЦрж╛ржЪрзНржЫрзЗред

---

### ЁЯзй LLM, Deep Learning ржУ AI-ржПрж░ рж╕ржорзНржкрж░рзНржХ:

* AI (Artificial Intelligence) = ржмрзБржжрзНржзрж┐ржорждрзНрждрж╛ рж╕ржорзНржкржирзНржи ржпржирзНрждрзНрж░ рждрзИрж░рж┐рж░ ржмрж┐ржЬрзНржЮрж╛ржи
* ржПрж░ ржПржХржЯрж┐ ржЙржкрж╢рж╛ржЦрж╛ тЖТ **Machine Learning**
* Machine Learning ржПрж░ ржЖрж░ржУ ржЧржнрзАрж░ рж╢рж╛ржЦрж╛ тЖТ **Deep Learning**
* LLM рж╣рж▓рзЛ Deep Learning ржнрж┐рждрзНрждрж┐ржХ ржПржоржи ржПржХ ржоржбрзЗрж▓ ржпрж╛ ржнрж╛рж╖рж╛ ржирж┐рзЯрзЗ ржХрж╛ржЬ ржХрж░рзЗред

ЁЯУМ ржПржЯрж╛ ржЖржорж░рж╛ Figure 1.1-ржПржУ ржжрзЗржЦрждрзЗ ржкрж╛ржЗ ржмржЗрзЯрзЗрж░ ржоржзрзНржпрзЗ (hierarchy diagram), ржпрзЗржЦрж╛ржирзЗ ржжрзЗржЦрж╛ржирзЛ рж╣рзЯрзЗржЫрзЗ:

```
AI тЖТ Machine Learning тЖТ Deep Learning тЖТ LLM
```

---

### ЁЯза рж╕рж╛ржзрж╛рж░ржг ржорзЗрж╢рж┐ржи рж▓рж╛рж░рзНржирж┐ржВ ржмржирж╛ржо LLM:

| рж╕рж╛ржзрж╛рж░ржг ржорзЗрж╢рж┐ржи рж▓рж╛рж░рзНржирж┐ржВ              | LLM                                             |
| --------------------------------- | ----------------------------------------------- |
| рж╣рзНржпрж╛ржирзНржбржХрзНрж░рж╛ржлржЯрзЗржб ржлрж┐ржЪрж╛рж░ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ | End-to-end learning ржХрж░рзЗ                         |
| ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржЯрж╛рж╕рзНржХрзЗ рж╕рзАржорж╛ржмржжрзНржз         | ржорж╛рж▓рзНржЯрж┐-ржЯрж╛рж╕рзНржХрзЗ рж╕ржХрзНрж╖ржо (ржЕржирзБржмрж╛ржж, ржкрзНрж░рж╢рзНржирзЛрждрзНрждрж░, рж▓рзЗржЦрж╛) |
| ржЫрзЛржЯ ржбрзЗржЯрж╛рж╕рзЗржЯрзЗ ржЯрзНрж░рзЗржЗржи               | ржмрж┐рж▓рж┐рзЯржи ржЯрзЛржХрзЗржи ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржЯрзНрж░рзЗржЗржи                 |
| ржлрж┐ржЪрж╛рж░ ржорзНржпрж╛ржирзБрзЯрж╛рж▓рж┐ рждрзИрж░рж┐ ржХрж░рждрзЗ рж╣рзЯ     | LLM ржирж┐ржЬрзЗржЗ ржПржоржмрзЗржбрж┐ржВ ржПржмржВ ржлрж┐ржЪрж╛рж░ рж╢рж┐ржЦрзЗ ржирзЗрзЯ            |

---

### ЁЯдЦ ржЙржжрж╛рж╣рж░ржг ржжрж┐рзЯрзЗ ржмрзЛржЭрж╛ ржпрж╛ржХ:

ЁЯФ╕ ржЖржЧрзЗрж░ NLP ржоржбрзЗрж▓ ржкрж╛рж░рждрзЛ:
тЬФя╕П ржЗржорзЗржЗрж▓ рж╕рзНржкрзНржпрж╛ржо ржбрж┐ржЯрзЗржХрж╢ржи
тЬФя╕П рж╕рж╣ржЬ ржЯрзЗржХрзНрж╕ржЯ ржХрзНрж▓рж╛рж╕рж┐ржлрж┐ржХрзЗрж╢ржи

ЁЯФ╕ ржХрж┐ржирзНрждрзБ рждрж╛рж░рж╛ ржкрж╛рж░рждрзЛ ржирж╛:
тЭМ ржЧрж▓рзНржк рж▓рзЗржЦрж╛
тЭМ ржкрзНрж░рж╢рзНржирзЗрж░ ржЕрж░рзНржержкрзВрж░рзНржг ржЙрждрзНрждрж░ ржжрзЗржУрзЯрж╛
тЭМ ржнрж┐ржирзНржи ржкрзНрж░рж╕ржЩрзНржЧрзЗ ржорж╛ржирзБрж╖рзЗрж░ ржорждрзЛ ржХржерзЛржкржХржержи ржХрж░рж╛

ЁЯСЙ ржЖржЬржХрзЗрж░ LLM ржПрж╕ржм **рж╕рж╣ржЬрзЗржЗ ржХрж░рждрзЗ ржкрж╛рж░рзЗ**ред

---

### ЁЯУМ ржПржЗ ржЕржВрж╢рзЗрж░ ржорзВрж▓ ржХржерж╛:

* LLM рж╣рж▓рзЛ **ржмрзЬ ржирж┐ржЙрж░рж╛рж▓ ржирзЗржЯржУрзЯрж╛рж░рзНржХ**, ржпрж╛ **ржкрж░ржмрж░рзНрждрзА рж╢ржмрзНржж ржЕржирзБржорж╛ржи** ржХрж░рждрзЗ рж╢рж┐ржЦрзЗред
* ржПржЯрж┐ ржорж╛ржирзБрж╖рзЗрж░ ржнрж╛рж╖рж╛рж░ **ржЧржаржи ржУ ржкрзНрж░рж╕ржЩрзНржЧ** ржмрзЛржЭрзЗред
* ржЖржорж░рж╛ ржПржЗ ржоржбрзЗрж▓ ржирж┐ржЬрзЗрж░ рж╣рж╛рждрзЗ ржмрж╛ржирж╛ржирзЛ рж╢рж┐ржЦржм ржкрзБрж░рзЛ ржмржЗ ржЬрзБрзЬрзЗред

---


---

## ЁЯФ╣ рзз.рзи: LLM-ржПрж░ ржмрзНржпржмрж╣рж╛рж░ (Applications of LLMs)

LLM-ржПрж░ рж╕ржмржЪрзЗржпрж╝рзЗ ржмрзЬ рж╢ржХрзНрждрж┐ рж╣рж▓рзЛ: ржПржЯрж┐ **ржЕрж╕ржоржЧржарж┐ржд (unstructured)** ржЯрзЗржХрзНрж╕ржЯ ржмрж┐рж╢рзНрж▓рзЗрж╖ржг ржХрж░рждрзЗ ржПржмржВ ржмрзБржЭрзЗ ржирждрзБржиржнрж╛ржмрзЗ рждрзИрж░рж┐ ржХрж░рждрзЗ ржкрж╛рж░рзЗред

### ЁЯТб ржорзВрж▓ ржХржерж╛:

> "LLMs can parse and understand unstructured text and be applied to a wide range of language tasks."

---

### тЬЕ ржмрзНржпржмрж╣рж╛рж░ рзз: **ржорзЗрж╢рж┐ржи ржЕржирзБржмрж╛ржж (Machine Translation)**

* ржнрж╛рж╖рж╛ржирзНрждрж░ ржПржЦржи LLM ржжрж┐рзЯрзЗ ржЕрждрзНржпржирзНржд рж╕рзНржмрж╛ржнрж╛ржмрж┐ржХржнрж╛ржмрзЗ ржХрж░рж╛ ржпрж╛ржпрж╝ред
* ржпрзЗржоржи: ржЗржВрж░рзЗржЬрж┐ ржерзЗржХрзЗ ржмрж╛ржВрж▓рж╛, ржмрж╛ ржЬрж╛рж░рзНржорж╛ржи ржерзЗржХрзЗ ржлрж░рж╛рж╕рж┐ ржЕржирзБржмрж╛ржжред
* GPT-ржПрж░ ржорждрзЛ ржоржбрзЗрж▓ ржПржоржиржХрж┐ **zero-shot translation** ржХрж░рждрзЗржУ ржкрж╛рж░рзЗ (рж╢рзЗржЦрж╛ржирзЛ ржЫрж╛ржбрж╝рж╛ржЗ)ред

ЁЯУМ ржЖржЧрзЗрж░ translation ржоржбрзЗрж▓ржЧрзБрж▓рзЛ ржЖрж▓рж╛ржжрж╛ ржХрж░рзЗ ржмрж╛ржирж╛рждрзЗ рж╣рждрзЛ, ржПржЦржи LLM ржирж┐ржЬрзЗржЗ ржПржЯрж╛ ржХрж░рждрзЗ ржкрж╛рж░рзЗред

---

### тЬЕ ржмрзНржпржмрж╣рж╛рж░ рзи: **ржирждрзБржи рж▓рзЗржЦрж╛ рждрзИрж░рж┐ (Text Generation)**

* ржХржмрж┐рждрж╛, ржЧрж▓рзНржк, ржЖрж░рзНржЯрж┐ржХрзЗрж▓, ржмрзНрж▓ржЧ, ржПржоржиржХрж┐ рж╕рзЛрж╢рзНржпрж╛рж▓ ржорж┐ржбрж┐ржпрж╝рж╛ ржкрзЛрж╕рзНржЯ ржкрж░рзНржпржирзНржд рж▓рзЗржЦрж╛ ржпрж╛ржпрж╝ред
* Figure 1.2-ржП ржжрзЗржЦрж╛ржирзЛ рж╣ржпрж╝рзЗржЫрзЗ ржХрзАржнрж╛ржмрзЗ ржПржХржЬржи ржмрзНржпржмрж╣рж╛рж░ржХрж╛рж░рзА **ChatGPT**-ржХрзЗ ржПржХржЯрж┐ ржХржмрж┐рждрж╛ рж▓рж┐ржЦрждрзЗ ржмрж▓ржЫрзЗ тАФ ржПржмржВ ржоржбрзЗрж▓ рждрзОржХрзНрж╖ржгрж╛рзО рж▓рж┐ржЦрзЗ ржжрж┐ржЪрзНржЫрзЗред

ЁЯОи тЬНя╕П ржПржЯрж┐ рж╣рж▓ **creative writing**-ржП LLM-ржПрж░ ржмрж┐рж╢рж╛рж▓ ржнрзВржорж┐ржХрж╛ред

---

### тЬЕ ржмрзНржпржмрж╣рж╛рж░ рзй: **рж╕рж╛рж░рж╛ржВрж╢ рждрзИрж░рж┐ (Text Summarization)**

* ржмржбрж╝ ржЖрж░рзНржЯрж┐ржХрзЗрж▓, рж░рж┐ржкрзЛрж░рзНржЯ ржмрж╛ ржиржерж┐ ржерзЗржХрзЗ ржжрзНрж░рзБржд **рж╕ржВржХрзНрж╖рж┐ржкрзНржд рж╕рж╛рж░ржорж░рзНржо** рждрзИрж░рж┐ ржХрж░рждрзЗ ржкрж╛рж░рзЗред
* ржпрзЗржоржи: ржирж┐ржЙржЬ, ржЧржмрзЗрж╖ржгрж╛ржкрждрзНрж░ ржмрж╛ ржЧрж▓рзНржкрзЗрж░ рж╕рж╛рж░рж╛ржВрж╢ред

---

### тЬЕ ржмрзНржпржмрж╣рж╛рж░ рзк: **рж╕рзЗржирзНржЯрж┐ржорзЗржирзНржЯ ржмрж┐рж╢рзНрж▓рзЗрж╖ржг (Sentiment Analysis)**

* ржЯрзЗржХрзНрж╕ржЯ ржерзЗржХрзЗ ржмрзЛржЭрзЗ рж▓рзЗржЦрж╛ржЯрж┐ **ржЗржорзЛрж╢ржирж╛рж▓рж┐ ржкржЬрж┐ржЯрж┐ржн, ржирзЗржЧрзЗржЯрж┐ржн ржирж╛ ржирж┐ржЙржЯрзНрж░рж╛рж▓**ред
* ржЙржжрж╛рж╣рж░ржг: рж░рж┐ржнрж┐ржЙ, ржлрж┐ржбржмрзНржпрж╛ржХ, рж╕рзЛрж╢рзНржпрж╛рж▓ ржорж┐ржбрж┐ржпрж╝рж╛рж░ ржкрзЛрж╕рзНржЯ ржмрж┐рж╢рзНрж▓рзЗрж╖ржгред

---

### тЬЕ ржмрзНржпржмрж╣рж╛рж░ рзл: **ржЪрзНржпрж╛ржЯржмржЯ ржУ ржнрж╛рж░рзНржЪрзБржпрж╝рж╛рж▓ ржЕрзНржпрж╛рж╕рж┐рж╕рзНржЯрзНржпрж╛ржирзНржЯ**

* OpenAI ChatGPT, Google Gemini (ржкрзБрж░ржирзЛ ржирж╛ржо Bard), Microsoft CopilotтАФрж╕ржмржЧрзБрж▓рзЛ LLM ржнрж┐рждрзНрждрж┐ржХред
* рждрж╛рж░рж╛ ржорж╛ржирзБрж╖ржХрзЗ ржмрзЛржЭрзЗ, ржЙрждрзНрждрж░ ржжрзЗржпрж╝ ржПржмржВ ржЖрж▓рж╛ржк ржЪрж╛рж▓рж┐ржпрж╝рзЗ ржпрзЗрждрзЗ ржкрж╛рж░рзЗред

ЁЯза ржПржЗ ржмрзНржпржмрж╕рзНржерж╛ржЧрзБрж▓рзЛ ржЕржирзЗржХржЯрж╛ржЗ **ржорж╛ржиржмрж╕ржжрзГрж╢ ржЖрж▓рж╛ржк ржЪрж╛рж▓рж╛рждрзЗ ржкрж╛рж░рзЗ**ред

---

### тЬЕ ржмрзНржпржмрж╣рж╛рж░ рзм: **рж╕рж╛рж░рзНржЪ ржЗржЮрзНржЬрж┐ржирзЗрж░ рж╢ржХрзНрждрж┐ржмрзГржжрзНржзрж┐**

* Google Search ржмрж╛ Bing ржПржЦржи LLM ржЗржирзНржЯрж┐ржЧрзНрж░рзЗржЯрзЗржб рж╣рзЯрзЗ ржЧрзЗржЫрзЗред
* ржкрзНрж░рж╢рзНржи ржХрж░рж▓рзЗ рж╕рж░рж╛рж╕рж░рж┐ ржЙрждрзНрждрж░ ржжрзЗрзЯ тАФ рж╢рзБржзрзБ рж▓рж┐ржЩрзНржХ ржжрзЗржЦрж╛рзЯ ржирж╛ред

---

### тЬЕ ржмрзНржпржмрж╣рж╛рж░ рзн: **ржмрж┐рж╢рзЗрж╖рж╛рзЯрж┐ржд ржХрзНрж╖рзЗрждрзНрж░рзЗ рждржерзНржп ржЖрж╣рж░ржг (Domain-specific Retrieval)**

* ржпрзЗржоржи: ржорзЗржбрж┐ржХрзЗрж▓, ржЖржЗржи ржмрж╛ ржЧржмрзЗрж╖ржгрж╛рзЯ LLM ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржмрж┐рж╢рж╛рж▓ ржбрзЗржЯрж╛ржмрзЗрж╕ ржерзЗржХрзЗ ржЙрждрзНрждрж░ ржмрзЗрж░ ржХрж░рзЗ ржЖржирж╛ред
* **ржиржерж┐ ржерзЗржХрзЗ рждржерзНржп ржЦрзЛржБржЬрж╛, ржЬржЯрж┐рж▓ ржкрзНрж░рж╢рзНржирзЗрж░ ржЙрждрзНрждрж░ ржжрзЗржУрзЯрж╛, ржжрзАрж░рзНржШ рж▓рзЗржЦрж╛ рж╕ржВржХрзНрж╖рж┐ржкрзНржд ржХрж░рж╛** ржЗрждрзНржпрж╛ржжрж┐ ржХрж╛ржЬ ржХрж░ржЫрзЗред

---

### тЬЕ ржмрзНржпржмрж╣рж╛рж░ рзо: **ржХрзЛржб рж▓рзЗржЦрж╛ ржУ ржмрзНржпрж╛ржЦрзНржпрж╛ ржХрж░рж╛**

* GPT-ржПрж░ ржорждрзЛ ржоржбрзЗрж▓ ржПржЦржи ржХрзЛржб рж▓рж┐ржЦрзЗ ржжрж┐рждрзЗ ржкрж╛рж░рзЗред
* StackOverflow ржмрж╛ GitHub Copilot-ржПрж░ ржорждрзЛ ржЯрзБрж▓ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржкрзНрж░рзЛржЧрзНрж░рж╛ржорж╛рж░рж░рж╛ рж╕рж╛рж╣рж╛ржпрзНржп ржирж┐ржЪрзНржЫрзЗред

---

### ЁЯУМ ржЪрзВрзЬрж╛ржирзНржд ржоржирзНрждржмрзНржп (From book):

> тАЬIn short, LLMs are invaluable for automating almost any task that involves parsing and generating text.тАЭ

ЁЯУО ржорж╛ржирзЗ рж╣рж▓рзЛ: ржпрзЗржЦрж╛ржирзЗ-ржЗ **ржЯрзЗржХрзНрж╕ржЯ ржмрзЛржЭрж╛ ржмрж╛ рждрзИрж░рж┐ ржХрж░рж╛рж░** ржжрж░ржХрж╛рж░, рж╕рзЗржЦрж╛ржирзЗржЗ LLM ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ ржпрж╛ржпрж╝ред

---

### ЁЯУЪ ржПржЗ ржмржЗ ржХрзА ржХрж░ржмрзЗ?

ржПржЗ ржЕржзрзНржпрж╛рзЯрзЗрж░ рж╢рзЗрж╖рзЗ рж▓рзЗржЦржХ ржмрж▓рзЗржи:

* ржЖржорж░рж╛ ржПржЗ ржмржЗрзЯрзЗ **ржирж┐ржЬ рж╣рж╛рждрзЗ ржПржХржЯрж╛ LLM ржмрж╛ржирж┐рзЯрзЗ рж╢рж┐ржЦржмрзЛ** ржХрзАржнрж╛ржмрзЗ:

  * ржЯрзЗржХрзНрж╕ржЯ рждрзИрж░рж┐ ржХрж░рждрзЗ рж╣рзЯ
  * ржкрзНрж░рж╢рзНржирзЗрж░ ржЙрждрзНрждрж░ ржжрж┐рждрзЗ рж╣рзЯ
  * рж╕рж╛рж░рж╛ржВрж╢ рждрзИрж░рж┐ ржХрж░рждрзЗ рж╣рзЯ
  * ржЕржирзБржмрж╛ржж ржХрж░рждрзЗ рж╣рзЯ

ЁЯОп ржЖржорж░рж╛ рж╢рж┐ржЦржмрзЛ GPT-ржПрж░ ржорждрзЛ ржЕрзНржпрж╛рж╕рж┐рж╕рзНржЯрзНржпрж╛ржирзНржЯ **ржХрзЛржб ржХрж░рзЗ рждрзИрж░рж┐ ржХрж░рждрзЗ**ред

---

## тЬЕ рж╕рж╛рж░рж╛ржВрж╢:

| ржХрж╛ржЬрзЗрж░ ржзрж░ржи        | LLM ржХрзАржнрж╛ржмрзЗ рж╕рж╛рж╣рж╛ржпрзНржп ржХрж░рзЗ                          |
| ---------------- | ----------------------------------------------- |
| ржнрж╛рж╖рж╛ ржЕржирзБржмрж╛ржж      | zero-shot ржмрж╛ few-shot ржП ржХрж╛ржЬ ржХрж░рзЗ                 |
| рж▓рзЗржЦрж╛ рждрзИрж░рж┐        | рж╕рзГржЬржирж╢рзАрж▓ рж▓рзЗржЦрж╛ ржерзЗржХрзЗ рж╢рзБрж░рзБ ржХрж░рзЗ рж░рж┐ржкрзЛрж░рзНржЯ рж▓рзЗржЦрж╛ ржкрж░рзНржпржирзНржд |
| рждржерзНржп ржмрж┐рж╢рзНрж▓рзЗрж╖ржг    | рж╕рзЗржирзНржЯрж┐ржорзЗржирзНржЯ, ржерж┐ржо, ржЕржнрж┐ржкрзНрж░рж╛рзЯ                      |
| ржЪрзНржпрж╛ржЯ ржУ рж╕рж╛ржкрзЛрж░рзНржЯ  | рж╕рзНржорж╛рж░рзНржЯ ржЪрзНржпрж╛ржЯржмржЯ ржУ рж╕рж╛рж░рзНржЪ ржЕрзНржпрж╛рж╕рж┐рж╕рзНржЯрзНржпрж╛ржирзНржЯ         |
| ржЯрзЗржХржирж┐ржХрзНржпрж╛рж▓ ржлрж┐рж▓рзНржб | ржЖржЗржи, ржЪрж┐ржХрж┐рзОрж╕рж╛, ржХрзЛржб рж▓рзЗржЦрж╛                          |

---



ржЪрж▓ ржПржмрж╛рж░ ржЖржорж░рж╛ ржмрж┐рж╕рзНрждрж╛рж░рж┐рждржнрж╛ржмрзЗ ржЖрж▓рзЛржЪржирж╛ ржХрж░рж┐ **Chapter 1.3: Stages of Building and Using LLMs (ржПржХржЯрж┐ LLM рждрзИрж░рж┐ ржУ ржмрзНржпржмрж╣рж╛рж░рзЗрж░ ржзрж╛ржкрж╕ржорзВрж╣)**ред

ржПржЗ ржЕржВрж╢рзЗ рж▓рзЗржЦржХ ржжрзЗржЦрж┐рзЯрзЗржЫрзЗржи тАФ ржПржХржЯрж┐ LLM ржХрзАржнрж╛ржмрзЗ **рждрзИрж░рж┐ ржХрж░рж╛ рж╣рзЯ**, **ржЯрзНрж░рзЗржирж┐ржВ ржХрж░рж╛ рж╣рзЯ**, ржПржмржВ **ржмрзНржпржмрж╣рж╛рж░ржпрзЛржЧрзНржп ржЕрзНржпрж╛рж╕рж┐рж╕рзНржЯрзНржпрж╛ржирзНржЯрзЗ рж░рзВржкрж╛ржирзНрждрж░** ржХрж░рж╛ рж╣рзЯред

---

## ЁЯФ╣ рзз.рзй: ржПржХржЯрж┐ LLM ржмрж╛ржирж╛ржирзЛрж░ ржзрж╛ржкрж╕ржорзВрж╣

---

### тЪЩя╕П ржкрзНрж░рж╢рзНржи ржЙржарждрзЗ ржкрж╛рж░рзЗ:

**ржЖржорж░рж╛ ржХрзЗржи ржирж┐ржЬрзЗрж░ ржПржХржЯрж┐ LLM ржмрж╛ржирж╛рждрзЗ ржЪрж╛ржЗ?**

> ржПржЗ ржЕржзрзНржпрж╛рзЯрзЗрж░ рж╢рзБрж░рзБрждрзЗржЗ рж▓рзЗржЦржХ ржмрж▓рзЗржи:
> тАЬBuilding your own LLM helps you understand its mechanics, limitations, and gives you control.тАЭ

---

### ЁЯОп ржорзВрж▓ ржЙржжрзНржжрзЗрж╢рзНржп:

* **LLM рж╢рзЗржЦрж╛рж░ рж╕ржмржЪрзЗрзЯрзЗ ржнрж╛рж▓рзЛ ржЙржкрж╛рзЯ** рж╣рж▓рзЛ рж╕рзЗржЯрж┐ржХрзЗ **ржирж┐ржЬрзЗ рждрзИрж░рж┐** ржХрж░рж╛ред
* ржПржЗ ржЕржнрж┐ржЬрзНржЮрждрж╛ рждрзЛржорж╛ржХрзЗ рж╕ржХрзНрж╖ржо ржХрж░рзЗ:

  * ржмрж┐ржнрж┐ржирзНржи рж╕ржорж╕рзНржпрж╛рж░ ржХрзНрж╖рзЗрждрзНрж░рзЗ LLM ржХрж╛рж╕рзНржЯржорж╛ржЗржЬ ржХрж░рждрзЗ
  * ржирж┐ржЬрж╕рзНржм ржбрзЗржЯрж╛рждрзЗ finetune ржХрж░рждрзЗ
  * security/privacy ржарж┐ржХ рж░рзЗржЦрзЗ LLM ржмрзНржпржмрж╣рж╛рж░ ржХрж░рждрзЗ

---

## тЬЕ ржжрзБржЯрж┐ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржзрж╛ржк:

LLM рждрзИрж░рж┐ ржУ ржмрзНржпржмрж╣рж╛рж░рзЗрж░ ржХрзНрж╖рзЗрждрзНрж░рзЗ ржжрзБржЯрж┐ ржзрж╛ржк рж╕рж░рзНржмрж╛ржзрж┐ржХ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг тАФ **Pretraining** ржПржмржВ **Finetuning**ред

---

### ЁЯзк ржзрж╛ржк рзз: Pretraining (ржкрзНрж░рж╛ржержорж┐ржХ ржкрзНрж░рж╢рж┐ржХрзНрж╖ржг)

* ржоржбрзЗрж▓ржХрзЗ ржЯрзНрж░рзЗржирж┐ржВ ржжрзЗрзЯрж╛ рж╣рзЯ **label ржЫрж╛рзЬрж╛ржЗ ржмрж┐рж╢рж╛рж▓ ржбрзЗржЯрж╛рждрзЗ** (ржпрзЗржоржи ржУрзЯрзЗржм ржЯрзЗржХрзНрж╕ржЯ, ржмржЗ, ржЙржЗржХрж┐ржкрж┐ржбрж┐рзЯрж╛)ред
* ржорзВрж▓ржд ржПржЯрж┐ рж╢рзЗржЦрзЗ ржХрзАржнрж╛ржмрзЗ **ржкрж░ржмрж░рзНрждрзА рж╢ржмрзНржж ржЕржирзБржорж╛ржи ржХрж░рж╛ ржпрж╛рзЯ**ред
* ржПржЯрж┐ржЗ **base/foundation model** рждрзИрж░рж┐ ржХрж░рзЗред

ЁЯУШ ржЙржжрж╛рж╣рж░ржг: GPT-3

* GPT-3 pretrain ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗ рж╕рж╛ржзрж╛рж░ржг рж▓рзЗржЦрж╛ ржжрж┐рзЯрзЗред
* ржоржбрзЗрж▓ рж╢рж┐ржЦрзЗржЫрзЗ: sentence ржХрзАржнрж╛ржмрзЗ ржЧржарж┐ржд рж╣рзЯ, ржХрзЛржи рж╢ржмрзНржжрзЗрж░ ржкрж░ ржХрзЛржи рж╢ржмрзНржж ржЖрж╕рзЗ, context ржХрзА ржЗрждрзНржпрж╛ржжрж┐ред

ЁЯУМ ржПржЗ ржкрзНрж░ржХрзНрж░рж┐рзЯрж╛ржХрзЗ ржмрж▓рзЗ **self-supervised learning**ред

---

### ЁЯЫая╕П ржзрж╛ржк рзи: Finetuning (ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржЯрж╛рж╕рзНржХрзЗрж░ ржЬржирзНржп ржЯрж┐ржЙржи ржХрж░рж╛)

ржкрзНрж░рж┐-ржЯрзНрж░рзЗржЗржиржб LLM ржХрзЗ **ржмрж┐рж╢рзЗрж╖ ржХрж╛ржЬрзЗрж░ ржЬржирзНржп finetune** ржХрж░рж╛ рж╣рзЯред

ЁЯФ╕ ржжрзБржЗ ржзрж░ржирзЗрж░ finetuning:

| ржзрж░ржи                           | ржЙржжрж╛рж╣рж░ржг                       | ржмрзНржпрж╛ржЦрзНржпрж╛                           |
| ----------------------------- | ---------------------------- | ---------------------------------- |
| **Classification finetuning** | рж╕рзНржкрзНржпрж╛ржо ржЗржорзЗржЗрж▓ ржбрж┐ржЯрзЗржХрж╢ржи        | ржбрзЗржЯрж╛ ржерж╛ржХрзЗ тЖТ \[ржЯрзЗржХрзНрж╕ржЯ, ржХрзНрж▓рж╛рж╕ рж▓рзЗржмрзЗрж▓] |
| **Instruction finetuning**    | ржкрзНрж░рж╢рзНржирзЗрж░ ржЙрждрзНрждрж░ ржжрзЗржУрзЯрж╛, ржЕржирзБржмрж╛ржж | ржбрзЗржЯрж╛ ржерж╛ржХрзЗ тЖТ \[ржЗржирж╕рзНржЯрзНрж░рж╛ржХрж╢ржи, ржЙрждрзНрждрж░]  |

ЁЯУМ ChatGPT рждрзИрж░рж┐ рж╣рзЯрзЗржЫрзЗ GPT-3-ржХрзЗ **instruction dataset** ржжрж┐рзЯрзЗ finetune ржХрж░рж╛рж░ ржорж╛ржзрзНржпржорзЗред

---

## ЁЯТ╝ ржХрзЗржи ржХрж╛рж╕рзНржЯржо LLM ржжрж░ржХрж╛рж░?

> рж▓рзЗржЦржХ ржПржЦрж╛ржирзЗ **privacy, latency ржУ independence**-ржПрж░ ржжрж┐ржХ рждрзБрж▓рзЗ ржзрж░рзЗржЫрзЗржиред

### ЁЯзй ржХрж╛рж╕рзНржЯржо LLM-ржПрж░ ржЙржкржХрж╛рж░рж┐рждрж╛:

| ржЙржкржХрж╛рж░рж┐рждрж╛                    | ржмрзНржпрж╛ржЦрзНржпрж╛                                                      |
| --------------------------- | ------------------------------------------------------------- |
| ЁЯФТ **Privacy**              | рж╕ржВржмрзЗржжржирж╢рзАрж▓ ржбрзЗржЯрж╛ 3rd-party ржоржбрзЗрж▓рзЗ ржирж╛ ржкрж╛ржарж┐рзЯрзЗржУ ржоржбрзЗрж▓ ржЯрзНрж░рзЗржЗржи ржХрж░рж╛ ржпрж╛рзЯ |
| ЁЯТ╗ **On-device Deployment** | ржлрзЛржи ржмрж╛ рж▓рзНржпрж╛ржкржЯржкрзЗржЗ ржЫрзЛржЯ LLM ржЪрж╛рж▓рж╛ржирзЛ рж╕ржорзНржнржм                         |
| ЁЯЫая╕П **Control**             | ржбрзЗржнрзЗрж▓ржкрж╛рж░ ржирж┐ржЬрзЗрж░ ржорждрзЛ ржХрж░рзЗ ржоржбрзЗрж▓ ржЖржкржбрзЗржЯ ржУ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рждрзЗ ржкрж╛рж░рзЗ         |
| тЪб **Latency ржХржорзЗ**           | cloud API call ржирж╛ ржХрж░рзЗ рж▓рзЛржХрж╛рж▓рзЗржЗ inference ржирзЗржУрзЯрж╛ ржпрж╛рзЯ             |

---

## ЁЯУК ржмржЗрзЯрзЗрж░ Figure 1.3 ржЕржирзБрж╕рж╛рж░рзЗ рж╕рзНржЯрзНрж░рж╛ржХржЪрж╛рж░:

```
                      [ Raw text data ]
                            тЖУ
                  тЮд Pretraining (next-word prediction)
                            тЖУ
         [ Pretrained base model / foundation model ]
                            тЖУ
          тЮд Finetuning (classification ржмрж╛ instruction)
                            тЖУ
                [ Task-specific fine-tuned model ]
```

---

## тЬи ржмрж╛рж╕рзНрждржм ржЙржжрж╛рж╣рж░ржг:

| ржоржбрзЗрж▓                      | ржХрж╛ржЬ                                            |
| ------------------------- | ---------------------------------------------- |
| **BloombergGPT**          | ржлрж╛ржЗржирзНржпрж╛ржирзНрж╕ ржмрж┐рж╖рзЯржХ ржХрж╛ржЬ                           |
| **Med-PaLM, BioGPT**      | ржорзЗржбрж┐ржХрзЗрж▓ ржкрзНрж░рж╢рзНржирзЗрж░ ржЙрждрзНрждрж░                         |
| **LLMs on Apple Devices** | рж▓рзЛржХрж╛рж▓ ржбрж┐ржнрж╛ржЗрж╕рзЗ рж░рж╛ржи ржХрж░рж╛рж░ ржЬржирзНржп ржлрж╛ржЗржиржЯрж┐ржЙржиржб ржЫрзЛржЯ ржоржбрзЗрж▓ |

---

## ЁЯУМ рж╕рж╛рж░рж╛ржВрж╢:

* ржПржХржЯрж┐ LLM ржЧржаржирзЗрж░ ржжрзБржЯрж┐ ржзрж╛ржк:
  **рзз) Pretraining тЖТ рзи) Finetuning**
* Pretraining рж╢рзЗржЦрж╛рзЯ ржнрж╛рж╖рж╛рж░ рж╕рж╛ржзрж╛рж░ржг ржЧржаржиред
* Finetuning рж╢рзЗржЦрж╛рзЯ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржХрж╛ржЬ тАФ ржпрзЗржоржи ржХрзНрж▓рж╛рж╕рж┐ржлрж┐ржХрзЗрж╢ржи ржмрж╛ ржкрзНрж░рж╢рзНржирзЗрж░ ржЙрждрзНрждрж░ред

---

ЁЯУО ржПржЗ ржЕржзрзНржпрж╛рзЯ ржЖржорж╛ржжрзЗрж░ ржкрзНрж░рж╕рзНрждрзБржд ржХрж░рзЗ ржжрж┐ржЪрзНржЫрзЗ ржкрж░ржмрж░рзНрждрзА ржзрж╛ржкрзЗрж░ ржЬржирзНржп:
**GPT-рж╕рзНржЯрж╛ржЗрж▓ ржоржбрзЗрж▓ рждрзИрж░рж┐рж░ ржкржержЪрж▓рж╛ рж╢рзБрж░рзБ рж╣ржмрзЗ ржПржЦрж╛ржи ржерзЗржХрзЗржЗ!**

---



ржжрж╛рж░рзБржг! ржПржмрж╛рж░ ржЖржорж░рж╛ **Pretraining** ржПржмржВ **Finetuning** тАФ ржПржЗ ржжрзБржЯрж┐ ржзрж╛ржк **ржмрж┐рж╕рзНрждрж╛рж░рж┐рждржнрж╛ржмрзЗ ржмрж╛ржВрж▓рж╛рзЯ** ржмрж┐рж╢рзНрж▓рзЗрж╖ржг ржХрж░ржмрзЛ, ржпрзЗржЧрзБрж▓рзЛ ржПржХржЯрж┐ **Large Language Model (LLM)** рждрзИрж░рж┐рж░ рж╕ржмржЪрзЗржпрж╝рзЗ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржЕржВрж╢ред

ржПржЧрзБрж▓рзЛ Chapter 1.3, 1.6, ржУ ржкрж░ржмрж░рзНрждрзА ржЕржзрзНржпрж╛рзЯржЧрзБрж▓рж┐рждрзЗ ржмрж┐рж╕рзНрждрж╛рж░рж┐рждржнрж╛ржмрзЗ ржЖрж▓рзЛржЪржирж╛ ржХрж░рж╛ рж╣ржпрж╝рзЗржЫрзЗ, ржПржмржВ ржкрзБрж░рзЛ ржмржЗржЯрж┐рждрзЗ ржПржЗ ржзрж╛рж░ржгрж╛ржЧрзБрж▓рж┐рж░ ржЙржкрж░ ржнрж┐рждрзНрждрж┐ ржХрж░рзЗржЗ ржХрж╛ржЬ ржХрж░рж╛ рж╣ржпрж╝ред

---

## ЁЯФ╢ рззя╕ПтГг Pretraining: ржкрзНрж░рж╛ржержорж┐ржХ ржкрзНрж░рж╢рж┐ржХрзНрж╖ржг

### ЁЯУМ рж╕ржВржЬрзНржЮрж╛:

**Pretraining** рж╣рж▓рзЛ ржоржбрзЗрж▓ржХрзЗ ржмрж┐рж╢рж╛рж▓ ржкрж░рж┐ржорж╛ржг **label-less ржЯрзЗржХрзНрж╕ржЯ** (ржЕрж░рзНржерж╛рзО ржЯрзНржпрж╛ржЧ ржмрж╛ ржХрзНржпрж╛ржЯрж╛ржЧрж░рж┐ ржЫрж╛рзЬрж╛) ржжрж┐рзЯрзЗ ржЯрзНрж░рзЗржи ржХрж░рж╛, ржпрж╛рждрзЗ ржоржбрзЗрж▓ **ржнрж╛рж╖рж╛рж░ ржЧржаржи, рж╢ржмрзНржжрзЗрж░ ржмрзНржпржмрж╣рж╛рж░ ржУ ржкрзНрж░рж╕ржЩрзНржЧ** ржмрзБржЭрждрзЗ ржкрж╛рж░рзЗред

---

### ЁЯза ржХрзА рж╢рзЗржЦрж╛ржирзЛ рж╣рзЯ?

* ржорзВрж▓ржд **next-word prediction**:

  > "ржЖржорж┐ ржЖржЬ ржмрж╛ржЬрж╛рж░рзЗ..." тЖТ ржкрж░ржмрж░рзНрждрзА рж╢ржмрзНржж ржХрзА? ржпрзЗржоржи: "ржЧрж┐рзЯрзЗржЫрж┐рж▓рж╛ржо"

* ржПржЯрж┐ржХрзЗ ржмрж▓рзЗ **Self-supervised Learning**:

  * ржХрж╛рж░ржг: ржЯрзЗржХрзНрж╕ржЯ ржирж┐ржЬрзЗржЗ ржирж┐ржЬрзЗрж░ рж▓рзЗржмрзЗрж▓ рждрзИрж░рж┐ ржХрж░рзЗ (рж╢рзБржзрзБ ржкрж░ржмрж░рзНрждрзА рж╢ржмрзНржж рж╢рзЗржЦрзЗ)
  * label ржжрж┐рждрзЗ рж╣рзЯ ржирж╛

---

### ЁЯзй ржЯрзЗржХржирж┐ржХрзНржпрж╛рж▓ ржкрзНрж░ржХрзНрж░рж┐рзЯрж╛:

1. **ржЯрзЗржХрзНрж╕ржЯ тЖТ ржЯрзЛржХрзЗржи** (BPE ржмрж╛ ржЕржирзНржп ржЯрзЛржХрзЗржирж╛ржЗржЬрж╛рж░ ржжрж┐рзЯрзЗ)
2. **ржЯрзЛржХрзЗржи тЖТ token IDs тЖТ embedding vectors**
3. ржПрж░ржкрж░ ржоржбрзЗрж▓ рж╢рзЗржЦрзЗ ржХрж┐ржнрж╛ржмрзЗ ржкрж░ржмрж░рзНрждрзА ржЯрзЛржХрзЗржи ржЕржирзБржорж╛ржи ржХрж░рждрзЗ рж╣рзЯред

---

### ЁЯТ╛ ржбрзЗржЯрж╛ ржХрзЗржоржи рж╣рзЯ?

GPT-3-ржПрж░ pretraining dataset ржЙржжрж╛рж╣рж░ржг:

| ржбрзЗржЯрж╛рж╕рзЗржЯ     | ржЯрзЛржХрзЗржи рж╕ржВржЦрзНржпрж╛ | ржЙрзОрж╕            |
| ----------- | ------------ | -------------- |
| CommonCrawl | 410B         | ржУрзЯрзЗржм           |
| Books1/2    | 67B          | ржЕржирж▓рж╛ржЗржи ржмржЗ      |
| Wikipedia   | 3B           | ржЬрзНржЮрж╛ржиржнрж┐рждрзНрждрж┐ржХ   |
| WebText     | 19B          | Reddit ржнрж┐рждрзНрждрж┐ржХ |

ЁЯУК ржорзЛржЯ: тЙИ 300 ржмрж┐рж▓рж┐рзЯржи ржЯрзЛржХрзЗржи ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ GPT-3 рждрзИрж░рж┐

---

### ЁЯТ░ ржЦрж░ржЪ ржУ ржЪрзНржпрж╛рж▓рзЗржЮрзНржЬ:

* GPT-3 ржПрж░ ржкрзНрж░рж┐-ржЯрзНрж░рзЗржЗржирж┐ржВ ржЦрж░ржЪ тЙИ \$4.6 ржорж┐рж▓рж┐ржпрж╝ржи
* ржкрзНрж░ржЪрзБрж░ ржХржорзНржкрж┐ржЙржЯрзЗрж╢ржи ржУ GPU рж╕ржорзЯ рж▓рж╛ржЧрзЗ
* рждрж╛ржЗ ржмржЗржЯрж┐рждрзЗ ржмрж▓рж╛ рж╣ржпрж╝рзЗржЫрзЗ:
  ржЖржорж░рж╛ **ржЫрзЛржЯ ржбрзЗржЯрж╛рждрзЗ ржУ ржЫрзЛржЯ ржоржбрзЗрж▓рзЗ ржкрзНрж░рзНржпрж╛ржХржЯрж┐рж╕ ржХрж░ржмрзЛ** рж╢рж┐ржХрзНрж╖рж╛рж░ ржЬржирзНржп

---

## тЬЕ Pretraining рж╢рзЗрж╖рзЗ ржХрзА рж╣рзЯ?

> ржПржХржЯрж┐ **foundation model** ржмрж╛ **base LLM** рждрзИрж░рж┐ рж╣рзЯ
> тЖТ ржпрж╛ ржЕржирзЗржХ ржЬрзЗржирзЗрж░рж┐ржХ ржнрж╛рж╖рж╛ржЧржд ржЯрж╛рж╕рзНржХрзЗ ржнрж╛рж▓рзЛ ржкрж╛рж░ржлрж░рзНржо ржХрж░рзЗ
> ржХрж┐ржирзНрждрзБ ржХрж┐ржЫрзБ ржХрзНрж╖рзЗрждрзНрж░рзЗ **ржмрж┐рж╢рзЗрж╖ржнрж╛ржмрзЗ рж╢рзЗржЦрж╛ржирзЛ ржжрж░ржХрж╛рж░** тАФ рждржЦржи ржжрж░ржХрж╛рж░...

---

## ЁЯФ╖ рзия╕ПтГг Finetuning: ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржХрж╛ржЬрзЗ ржжржХрзНрж╖рждрж╛ ржЕрж░рзНржЬржи

### ЁЯУМ рж╕ржВржЬрзНржЮрж╛:

Pretrained ржоржбрзЗрж▓ржХрзЗ **ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржХрзЛржирзЛ ржХрж╛ржЬ ржмрж╛ ржбрзЛржорзЗржЗржирзЗ ржЖржмрж╛рж░ ржкрзНрж░рж╢рж┐ржХрзНрж╖ржг ржжрзЗржУрзЯрж╛ рж╣рзЯ**, ржпрж╛рждрзЗ рж╕рзЗржЯрж┐ ржУржЗ ржХрж╛ржЬрзЗ ржмрж┐рж╢рзЗрж╖ ржкрж╛рж░ржжрж░рзНрж╢рзА рж╣рзЯред

---

### ЁЯФБ ржХрзЗржи ржжрж░ржХрж╛рж░?

Pretraining-ржП ржоржбрзЗрж▓ **рж╕рж╛ржзрж╛рж░ржг ржнрж╛рж╖рж╛рж░ ржзрж╛рж░ржгрж╛** рж╢рзЗржЦрзЗ,
ржХрж┐ржирзНрждрзБ Finetuning рж╢рзЗржЦрж╛рзЯ:

* ржкрзНрж░рж╢рзНржирзЗрж░ рж╕ржарж┐ржХ ржЙрждрзНрждрж░ ржжрзЗржУрзЯрж╛
* ржЯрзЗржХрзНрж╕ржЯ ржХрзНрж▓рж╛рж╕рж┐ржлрж┐ржХрзЗрж╢ржи
* ржирж┐рж░рзНржжрзЗрж╢ ржорж╛ржирж╛ (instruction following)

---

### ЁЯЫая╕П Finetuning-ржПрж░ ржжрзБржЗ ржзрж░ржи:

| ржзрж░ржи                        | ржЙржжрж╛рж╣рж░ржг                   | ржбрзЗржЯрж╛ рж╕рзНржЯрзНрж░рж╛ржХржЪрж╛рж░          |
| -------------------------- | ------------------------ | ------------------------ |
| **Classification**         | рж╕рзНржкрзНржпрж╛ржо/ржиржи-рж╕рзНржкрзНржпрж╛ржо ржЗржорзЗржЗрж▓ | \[ржЯрзЗржХрзНрж╕ржЯ, рж▓рзЗржмрзЗрж▓]         |
| **Instruction Finetuning** | ржкрзНрж░рж╢рзНржи тЖТ ржЙрждрзНрждрж░           | \[Instruction, Response] |

---

### ЁЯУН ржЙржжрж╛рж╣рж░ржг:

| ржоржбрзЗрж▓         | ржЙржжрзНржжрзЗрж╢рзНржп                     |
| ------------ | ---------------------------- |
| ChatGPT      | GPT-3 ржХрзЗ Finetune ржХрж░рзЗ ржмрж╛ржирж╛ржирзЛ |
| BloombergGPT | ржлрж╛ржЗржирзНржпрж╛ржирзНрж╕ ржмрж┐рж╖рзЯрзЗ ржкрж╛рж░ржжрж░рзНрж╢рзА    |
| Med-PaLM     | ржорзЗржбрж┐ржХрзЗрж▓ рждржерзНржп ржмрж┐рж╢рзНрж▓рзЗрж╖ржг        |

---

### ЁЯФБ Instruction Finetuning:

ржПржЦрж╛ржирзЗ ржоржбрзЗрж▓ржХрзЗ рж╢рзЗржЦрж╛ржирзЛ рж╣рзЯ:

| Instruction                    | Response                 |
| ------------------------------ | ------------------------ |
| "Translate 'hello' to Spanish" | "Hola"                   |
| "Summarize this article..."    | "The main points are..." |

ржПржЗ ржзрж░ржгрзЗрж░ ржбрзЗржЯрж╛ ржжрж┐рзЯрзЗ ржоржбрзЗрж▓ рж╢рзЗржЦрзЗ **user instruction ржЕржирзБрж╕рж░ржг ржХрж░рж╛**ред

---

### тЪЩя╕П ржЯрзЗржХржирж┐ржХрзНржпрж╛рж▓ржнрж╛ржмрзЗ ржХрзА рж╣рзЯ?

* Pretrained ржоржбрзЗрж▓ рж▓рзЛржб ржХрж░рж╛ рж╣рзЯ
* Output layer ржкрж░рж┐ржмрж░рзНрждржи ржХрж░рж╛ рж╣рзЯ (ржпржжрж┐ ржжрж░ржХрж╛рж░ рж╣рзЯ)
* ржирждрзБржи labeled ржбрзЗржЯрж╛рждрзЗ ржЖржмрж╛рж░ ржЯрзНрж░рзЗржЗржи ржХрж░рж╛ рж╣рзЯ
* рж╕рж╛ржзрж╛рж░ржгржд ржХржо рж╕ржорзЯ ржУ ржХржо ржбрзЗржЯрж╛ рж▓рж╛ржЧрзЗ

---

### ЁЯУЙ Parameter-efficient Finetuning (LoRA):

ржмржЗрзЯрзЗрж░ Appendix E рждрзЗ ржЖрж▓рзЛржЪрж┐ржд ржПржХржЯрж┐ ржЙржирзНржиржд ржЯрзЗржХржирж┐ржХ:

* ржкрзБрж░рзЛ ржоржбрзЗрж▓ ржЯрзНрж░рзЗржЗржи ржирж╛ ржХрж░рзЗ рж╢рзБржзрзБржорж╛рждрзНрж░ ржЫрзЛржЯ ржХрж┐ржЫрзБ рж▓рзЗрзЯрж╛рж░рзЗ finetune ржХрж░рж╛ рж╣рзЯ
* GPU/ржорзЗржорзЛрж░рж┐ ржХржо рж▓рж╛ржЧрзЗ

ЁЯСЙ ржмрзНржпржмрж╣рж╛рж░ рж╣рзЯ ржЕржирзЗржХ ржХржо рж░рж┐рж╕рзЛрж░рзНрж╕рзЗ instruction-tuned LLM рждрзИрж░рж┐ ржХрж░рждрзЗред

---

## ЁЯОп Pretraining vs Finetuning рждрзБрж▓ржирж╛:

| ржмрзИрж╢рж┐рж╖рзНржЯрзНржп       | Pretraining         | Finetuning          |
| --------------- | ------------------- | ------------------- |
| ржбрзЗржЯрж╛ ржЯрж╛ржЗржк       | Unlabeled           | Labeled             |
| ржЙржжрзНржжрзЗрж╢рзНржп        | рж╕рж╛ржзрж╛рж░ржг ржнрж╛рж╖рж╛ рж╢рзЗржЦрж╛    | ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржХрж╛ржЬ рж╢рзЗржЦрж╛  |
| рж░рж┐рж╕рзЛрж░рзНрж╕ ржкрзНрж░рзЯрзЛржЬржи | ржкрзНрж░ржЪрзБрж░ (GPU, ржЯрзЛржХрзЗржи) | рждрзБрж▓ржирж╛ржорзВрж▓ржХ ржХржо        |
| рж╕ржоржпрж╝            | ржЕржирзЗржХ ржмрзЗрж╢рж┐           | ржХржо                  |
| ржкрзНрж░ржнрж╛ржм          | General model       | Task-specific model |

---

## ЁЯУМ рж╕рж╛рж░рж╛ржВрж╢:

* **Pretraining**: ржнрж╛рж╖рж╛ ржмрзБржЭрждрзЗ рж╢рзЗржЦрж╛рзЯ (next-word prediction)
* **Finetuning**: ржХрж╛ржЬ ржмрзБржЭрждрзЗ рж╢рзЗржЦрж╛рзЯ (classification, instruction)
* ржЙржнрзЯ ржзрж╛ржк ржорж┐рж▓рзЗржЗ LLM рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА рж╣рзЯ

---

---

## ЁЯФ╖ Chapter 1.4: Transformer ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░ ржкрж░рж┐ржЪрж┐рждрж┐

---

### ЁЯФ░ ржорзВрж▓ ржХржерж╛:

> ржЖржЬржХрзЗрж░ ржЕржзрж┐ржХрж╛ржВрж╢ LLM рждрзИрж░рж┐ рж╣рзЯ **Transformer Architecture**-ржПрж░ ржЙржкрж░ ржнрж┐рждрзНрждрж┐ ржХрж░рзЗред
> ржПржЯрж┐ ржкрзНрж░ржержо ржЪрж╛рж▓рзБ рж╣рзЯ рзирзжрззрзн рж╕рж╛рж▓рзЗ ржкрзНрж░ржХрж╛рж╢рж┐ржд ржЧржмрзЗрж╖ржгрж╛ржкрждрзНрж░:
> **тАЬAttention Is All You NeedтАЭ**

---

### ЁЯза ржорзВрж▓ ржзрж╛рж░ржгрж╛: Self-Attention

Transformer ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░рзЗрж░ ржкрзНрж░рж╛ржг рж╣ржЪрзНржЫрзЗ **self-attention mechanism** тАФ ржпрж╛ ржоржбрзЗрж▓ржХрзЗ ржЗржиржкрзБржЯ ржЯрзЛржХрзЗржиржЧрзБрж▓рзЛрж░ ржоржзрзНржпрзЗ рж╕ржорзНржкрж░рзНржХ ржмрзБржЭрждрзЗ ржжрзЗрзЯред

---

## ЁЯУК Figure 1.4 тАУ Transformer ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░рзЗрж░ ржЪрж┐рждрзНрж░

ржПржЗ ржЪрж┐рждрзНрж░рзЗ ржжрзЗржЦрж╛ржирзЛ рж╣ржпрж╝рзЗржЫрзЗ:

* **Encoder**: ржЗржиржкрзБржЯ ржЯрзЛржХрзЗржиржЧрзБрж▓рзЛржХрзЗ ржкрзНрж░рж╕рзЗрж╕ ржХрж░рзЗ context-aware vector-ржП рж░рзВржкрж╛ржирзНрждрж░ ржХрж░рзЗ
* **Decoder**: ржПржЗ vector ржЗржиржкрзБржЯ ржирж┐рзЯрзЗ ржЖржЙржЯржкрзБржЯ (ржпрзЗржоржи ржЕржирзБржмрж╛ржж, ржЙрждрзНрждрж░) рждрзИрж░рж┐ ржХрж░рзЗ

ЁЯФБ ржПржЯрж┐ ржорзВрж▓ржд **Machine Translation**-ржПрж░ ржЬржирзНржп рждрзИрж░рж┐ рж╣рзЯрзЗржЫрж┐рж▓:
ржпрзЗржоржи: тАЬThis is an exampleтАЭ тЖТ тАЬDas ist ein BeispielтАЭ (ржЬрж╛рж░рзНржорж╛ржи)

ржЪрж┐рждрзНрж░ ржЕржирзБржпрж╛рзЯрзА:

* **тАЬDas ist einтАЭ**: ржЖржВрж╢рж┐ржХ ржЖржЙржЯржкрзБржЯ
* Decoder ржмрж╛ржХрж┐ рж╢ржмрзНржж **тАЬBeispielтАЭ** рждрзИрж░рж┐ ржХрж░ржЫрзЗ

---

## ЁЯзй Encoder-Decoder ржмрзНржпрж╛ржЦрзНржпрж╛:

* **Encoder**: ржЗржиржкрзБржЯ ржмрж╛ржХрзНржпрзЗрж░ ржкрзНрж░рждрж┐ржЯрж┐ ржЯрзЛржХрзЗржи ржкрзНрж░рж╕рзЗрж╕ ржХрж░рзЗ **embedding vector** рждрзИрж░рж┐ ржХрж░рзЗ
* **Decoder**: ржкрзНрж░рждрж┐ржЯрж┐ ржЖржЙржЯржкрзБржЯ ржЯрзЛржХрзЗржи ржзрж╛ржкрзЗ ржзрж╛ржкрзЗ рждрзИрж░рж┐ ржХрж░рзЗ, ржкрзВрж░рзНржмржмрж░рзНрждрзА ржЯрзЛржХрзЗржиржЧрзБрж▓рзЛрж░ ржЙржкрж░ ржнрж┐рждрзНрждрж┐ ржХрж░рзЗ

ЁЯУМ Transformer ржПрж░ Encoder ржПржмржВ Decoder ржЙржнржпрж╝рзЗржЗ ржЧржарж┐ржд рж╣рзЯ:

* **Self-Attention** рж▓рзЗрзЯрж╛рж░
* **Multi-head Attention**
* **Feed-forward Network**
* **Layer Normalization**
* **Residual Connections**

ржПржЗ ржЕржзрзНржпрж╛ржпрж╝рзЗ рж▓рзЗржЦржХ ржмрж┐рж╕рзНрждрж╛рж░рж┐ржд ржХрзЛржб ржирж╛ ржжрж┐рж▓рзЗржУ, ржзрж╛рж░ржгрж╛ржЧрзБрж▓рзЛрж░ ржнрж┐рждрзНрждрж┐ ржкрж░рж┐рж╖рзНржХрж╛рж░ ржХрж░рзЗржЫрзЗржи ржкрж░ржмрж░рзНрждрзА ржЕржзрзНржпрж╛ржпрж╝ржЧрзБрж▓рзЛрж░ ржЬржирзНржпред

---

### тЪая╕П ржкрж╛ржаржХрзЗрж░ рж╕рж╛ржзрж╛рж░ржг ржкрзНрж░рж╢рзНржи:

> Input text ржХрж┐ржнрж╛ржмрзЗ encode ржХрж░рж╛ рж╣рзЯ?
> Self-attention ржХрж┐ржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░рзЗ?

тЬ┤я╕П рж▓рзЗржЦржХ ржмрж▓рзЗржи тАФ ржПржЗ ржкрзНрж░рж╢рзНржиржЧрзБрж▓рзЛрж░ ржЙрждрзНрждрж░ ржЖржорж░рж╛ **Chapter 2 ржУ Chapter 3**-рждрзЗ рж╣рж╛рждрзЗ-ржХрж▓ржорзЗ рж╢рзЗржЦржмрзЛред

---

## ЁЯФД BERT ржУ GPT-ржПрж░ ржоржзрзНржпрзЗ ржкрж╛рж░рзНржержХрзНржп

Transformer ржПрж░ ржжрзБржЯрж┐ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржзрж╛рж░рж╛рзЯ рж░рзВржкрж╛ржирзНрждрж░ рж╣рзЯрзЗржЫрзЗ:

| ржоржбрзЗрж▓     | ржЕржВрж╢ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ       | ржЙржжрзНржжрзЗрж╢рзНржп                       |
| -------- | --------------------- | ------------------------------ |
| **BERT** | рж╢рзБржзрзБржорж╛рждрзНрж░ **Encoder** | Understanding (Classification) |
| **GPT**  | рж╢рзБржзрзБржорж╛рждрзНрж░ **Decoder** | Generation (Text Creation)     |

---

## ЁЯЯи BERT ржПрж░ ржмрзИрж╢рж┐рж╖рзНржЯрзНржп:

* **BERT** = Bidirectional Encoder Representations from Transformers
* ржорзВрж▓ржд **Masked Language Modeling** ржХрж░рзЗ:

  > ржмрж╛ржХрзНржпрзЗрж░ ржорж╛ржЭржЦрж╛ржирзЗ ржХрж┐ржЫрзБ рж╢ржмрзНржж рж▓рзБржХрж╛ржирзЛ ржерж╛ржХрзЗ
  > ржоржбрзЗрж▓ржХрзЗ рж╢рзЗржЦрж╛ржирзЛ рж╣рзЯ рж╕рзЗржЧрзБрж▓рзЛ ржХрзА рж╣рждрзЗ ржкрж╛рж░рзЗ

ЁЯУМ ржПрж░ ржлрж▓рзЗ BERT ржнрж╛рж▓рзЛ ржкрж╛рж░ржлрж░рзНржо ржХрж░рзЗ:

* **рж╕рзЗржирзНржЯрж┐ржорзЗржирзНржЯ ржПржирж╛рж▓рж╛ржЗрж╕рж┐рж╕**
* **рж╕рзНржкрзНржпрж╛ржо ржХрзНрж▓рж╛рж╕рж┐ржлрж┐ржХрзЗрж╢ржи**
* **рж░рж┐ржкрзЛрж░рзНржЯ ржЯрзНржпрж╛ржЧрж┐ржВ**

ЁЯУО Twitter (ржПржХрж╕ржорзЯ) Toxic content detect ржХрж░рждрзЗ BERT ржмрзНржпржмрж╣рж╛рж░ ржХрж░рждрзЛ

---

## ЁЯЯй GPT ржПрж░ ржмрзИрж╢рж┐рж╖рзНржЯрзНржп:

* GPT = Generative Pretrained Transformer
* ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ **Decoder part only**
* рж╢рзЗржЦрзЗ: **Next Word Prediction**
* ржЙржжрж╛рж╣рж░ржг: тАЬI went to theтАЭ тЖТ тАЬmarketтАЭ

---

### ЁЯУ╕ Figure 1.5 тАУ Encoder vs Decoder

ржЪрж┐рждрзНрж░рзЗ ржжрзЗржЦрж╛ржирзЛ рж╣рзЯрзЗржЫрзЗ:

| ржмрж╛ржо ржкрж╛рж╢рзЗ                       | ржбрж╛ржи ржкрж╛рж╢рзЗ                     |
| ------------------------------ | ---------------------------- |
| Encoder (BERT)                 | Decoder (GPT)                |
| Input: masked sentence         | Input: partial sentence      |
| Output: masked word prediction | Output: next word generation |

ЁЯУМ ржПржЯрж┐ ржмрзЛржЭрж╛рзЯ:
BERT ржУ GPT ржЖрж▓рж╛ржжрж╛ ржХрж╛ржЬрзЗрж░ ржЬржирзНржп ржЯрзНрж░рзЗржЗржи рж╣рзЯ, ржпржжрж┐ржУ ржПржХржЗ transformer ржзрж╛рж░ржгрж╛ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ

---

## ЁЯОп GPT ржПрж░ рж╕рж╛ржорж░рзНржерзНржп: Few-shot ржУ Zero-shot Learning

GPT ржоржбрзЗрж▓ ржпрзЗржоржи GPT-3 ржкрж╛рж░ржжрж░рзНрж╢рзА:

* **Zero-shot learning**: ржХрзЛржи training data ржЫрж╛рзЬрж╛ржЗ ржирждрзБржи ржЯрж╛рж╕рзНржХрзЗ ржкрж╛рж░ржлрж░рзНржо ржХрж░рж╛
* **Few-shot learning**: ржорж╛рждрзНрж░ рзз-рзи ржЙржжрж╛рж╣рж░ржг ржжрзЗржЦрж╛рж▓рзЗржЗ ржЯрж╛рж╕рзНржХ ржХрж░рждрзЗ ржкрж╛рж░рж╛

ЁЯУ╕ Figure 1.6 ржП ржПржЗ ржзрж╛рж░ржгрж╛ ржмрзНржпрж╛ржЦрзНржпрж╛ ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗ

ржЙржжрж╛рж╣рж░ржг:

> Prompt: "Translate: 'Bonjour' тЖТ "
> Response: "Hello"

ржПржЯрж┐ Zero-shot Translation

---

## ЁЯУЪ Transformer тЙа LLM ржПржмржВ LLM тЙа Transformer

рж▓рзЗржЦржХ ржПржХржЯрж┐ ржЦрзБржм ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржкржпрж╝рзЗржирзНржЯ ржХрж░рзЗржЫрзЗржи ржПржЦрж╛ржирзЗ:

> Transformer тЙа LLM рж╕ржмрж╕ржорзЯ ржирзЯ
> ржПржмржВ рж╕ржм LLM transformer ржирзЯ

### ржЙржжрж╛рж╣рж░ржг:

| ржзрж░ржи                  | ржмрзНржпрж╛ржЦрзНржпрж╛                                |
| -------------------- | --------------------------------------- |
| Transformer ржмрзНржпржмрж╣рж╛рж░  | Vision Model-ржПржУ рж╣рзЯ (ржпрзЗржоржи ViT)           |
| LLM ржХрж┐ржирзНрждрзБ RNN-based | ржЖржЧрзЗрж░ LSTM-based LLM                     |
| Transformer ржЫрж╛рзЬрж╛ LLM | Efficient ржмрзНржпрж╛ржХржЖржк ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░ ржпрзЗржоржи RWKV |

рждржмрзЗ тАФ **ржПржЗ ржмржЗрждрзЗ ржорзВрж▓ржд GPT-рж╕рзНржЯрж╛ржЗрж▓ Transformer-based LLM** ржирж┐рзЯрзЗржЗ ржХрж╛ржЬ ржХрж░рж╛ рж╣ржмрзЗред

---

## ЁЯУЭ ржЙржкрж╕ржВрж╣рж╛рж░:

ржПржЗ ржЕржзрзНржпрж╛рзЯ ржерзЗржХрзЗ ржЖржорж░рж╛ ржпрж╛ рж╢рж┐ржЦрж▓рж╛ржо:

1. **Transformer Architecture** LLM-ржПрж░ ржнрж┐рждрзНрждрж┐
2. GPT ржУ BERT ржжрзБржЯрзЛржЗ Transformer-ржПрж░ ржЕржВрж╢ржмрж┐рж╢рзЗрж╖ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ
3. GPT = Text Generation, BERT = Understanding
4. GPT ржкрзНрж░ржорж╛ржг ржХрж░рзЗржЫрзЗ: Next-word prediction ржжрж┐рзЯрзЗржЗ ржЕрждрзНржпржирзНржд рж╕ржХрзНрж╖ржо ржнрж╛рж╖рж╛ ржоржбрзЗрж▓ ржмрж╛ржирж╛ржирзЛ ржпрж╛рзЯ
5. ржПржЗ ржмржЗрзЯрзЗ ржЖржорж░рж╛ **GPT-ржнрж┐рждрзНрждрж┐ржХ Decoder-only Transformer** рж╢рзЗржЦрж╛ рж╢рзБрж░рзБ ржХрж░ржмрзЛ, ржПржмржВ **ржирж┐ржЬрзЗ ржХрзЛржб ржХрж░рзЗ ржмрж╛ржирж╛ржмрзЛ**

---

---

## ЁЯУШ Chapter 1.5: Utilizing Large Datasets

**(ржмрзЬ ржбрзЗржЯрж╛рж╕рзЗржЯ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛)**

---

### ЁЯФН ржорзВрж▓ ржХржерж╛:

> тАЬThe key ingredient for training a large language model is not just the model architecture but also the amount and quality of training data.тАЭ

ржЕрж░рзНржерж╛рзО, рж╢рзБржзрзБ ржмрзЬ ржоржбрзЗрж▓ ржмрж╛ржирж╛рж▓рзЗржЗ рж╣ржмрзЗ ржирж╛ тАФ **ржмрзЬ ржПржмржВ ржкрж░рж┐рж╖рзНржХрж╛рж░ ржЯрзЗржХрзНрж╕ржЯ ржбрзЗржЯрж╛** ржЫрж╛рзЬрж╛ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА LLM рждрзИрж░рж┐ ржХрж░рж╛ ржпрж╛рзЯ ржирж╛ред

---

## ЁЯза GPT-3 ржПрж░ ржбрзЗржЯрж╛рж╕рзЗржЯ ржХрзАржнрж╛ржмрзЗ рждрзИрж░рж┐ рж╣рзЯрзЗржЫрзЗ?

### ЁЯФв Table 1.1: GPT-3 ржбрзЗржЯрж╛рж╕рзЗржЯ ржЙржкрж╛ржжрж╛ржи (ржмржЗ ржерзЗржХрзЗ ржирзЗржУрзЯрж╛)

| ржЙрзОрж╕              | ржЯрзЛржХрзЗржи (ржмрж┐рж▓рж┐рзЯржирзЗ) | рж╢рждрж╛ржВрж╢ (%) |
| ---------------- | --------------- | --------- |
| **Common Crawl** | 410B            | 60%       |
| **WebText2**     | 19B             | 22%       |
| **Books1**       | 12B             | 8%        |
| **Books2**       | 55B             | 8%        |
| **Wikipedia**    | 3B              | 3%        |
| **ржорзЛржЯ**          | \~700B+ tokens  | 100%      |

ЁЯУМ тАЬTokenтАЭ ржорж╛ржирзЗ рж╢рзБржзрзБ рж╢ржмрзНржж ржирзЯ тАФ punctuation, рж╕ржВржЦрзНржпрж╛ ржЗрждрзНржпрж╛ржжрж┐ржУ ржЯрзЛржХрзЗржи рж╣рж┐рж╕рзЗржмрзЗ ржЧржгрзНржп рж╣рзЯред ржпрзЗржоржи:

* "ChatGPT is awesome!" тЖТ \["Chat", "G", "PT", " is", " awesome", "!"]

---

### ЁЯУВ ржбрзЗржЯрж╛рж╕рзЗржЯ ржЧрзБрж▓рзЛ ржмрж┐рж╕рзНрждрж╛рж░рж┐рждржнрж╛ржмрзЗ:

#### 1. **Common Crawl (410B tokens)**

* ржПржЯрж┐ ржУрзЯрзЗржм ржерзЗржХрзЗ рж╕рзНржХрзНрж░рзНржпрж╛ржк ржХрж░рж╛ ржПржХржЯрж┐ ржмрж┐рж╢рж╛рж▓ рж╕ржВржЧрзНрж░рж╣ред
* ржУрзЯрзЗржмрж╕рж╛ржЗржЯ, ржмрзНрж▓ржЧ, ржЖрж░рзНржЯрж┐ржХрзЗрж▓, ржлрзЛрж░рж╛ржо ржЗрждрзНржпрж╛ржжрж┐ред

ЁЯУМ ржПржЯрж┐ raw ржУ noisy, рждрж╛ржЗ GPT ржирж┐рж░рзНржорж╛рждрж╛рж░рж╛ ржПржЯрж┐ ржкрж░рж┐рж╖рзНржХрж╛рж░ ржХрж░рзЗ **filtered, high-quality version** ржмрж╛ржирж┐рзЯрзЗржЫрзЗржиред

---

#### 2. **WebText2 (19B tokens)**

* Reddit ржерзЗржХрзЗ ржирзЗржУрзЯрж╛ ржбрзЗржЯрж╛ ржпрзЗржЦрж╛ржирзЗ **ржорж╛ржирзБрж╖ рж▓рж┐ржЩрзНржХ рж╢рзЗрзЯрж╛рж░ ржХрж░рзЗржЫрзЗ ржПржмржВ ржоржирзНрждржмрзНржп ржХрж░рзЗржЫрзЗ**ред
* ржПржЯрж┐ human-curated text, рждрж╛ржЗ ржмрзЗрж╢рж┐ informativeред

---

#### 3. **Books1 & Books2 (67B tokens ржорзЛржЯ)**

* ржЕржирж▓рж╛ржЗржи ржУ ржкрж╛ржмрж▓рж┐ржХ ржбрзЛржорзЗржЗржирзЗрж░ ржмржЗред
* Formal writing, ржнрж╛рж▓рзЛ grammar ржУ structure ржерж╛ржХрзЗред
* ржоржбрзЗрж▓ржХрзЗ **coherent language structure** рж╢рзЗржЦрж╛рждрзЗ рж╕рж╛рж╣рж╛ржпрзНржп ржХрж░рзЗред

---

#### 4. **Wikipedia (3B tokens)**

* Structured, fact-based knowledge sourceред
* ржЦрзБржмржЗ ржкрж░рж┐рж╖рзНржХрж╛рж░ ржПржмржВ informativeред

---

## ЁЯУЙ ржХржд ржмрзЬ ржбрзЗржЯрж╛рж╕рзЗржЯ ржжрж░ржХрж╛рж░?

GPT-3 ржЯрзНрж░рзЗржирж┐ржВрзЯрзЗ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯ:

* **700B tokens ржПрж░ ржмрзЗрж╢рж┐**
* **\$4.6 ржорж┐рж▓рж┐рзЯржи** ржЦрж░ржЪ рж╢рзБржзрзБ training-ржПрж░ ржЬржирзНржп
* ржЕржирзЗржХржЧрзБрж▓рзЛ **GPU/TPU** ржорж╛рж╕рзЗрж░ ржкрж░ ржорж╛рж╕ ржмрзНржпрж╕рзНржд ржерж╛ржХрзЗ

ЁЯУМ рж▓рзЗржЦржХ рж╕рзНржкрж╖рзНржЯ ржХрж░рзЗржЫрзЗржи:
ржПржЗ ржмржЗрзЯрзЗ ржЖржорж░рж╛ **ржПржд ржмрзЬ ржбрзЗржЯрж╛рж╕рзЗржЯ ржмрзНржпржмрж╣рж╛рж░ ржХрж░ржмрзЛ ржирж╛**ред
ржЖржорж░рж╛ ржЫрзЛржЯ ржбрзЗржЯрж╛ ржжрж┐рзЯрзЗ ржоржбрзЗрж▓ ржЯрзНрж░рзЗржи ржХрж░ржмрзЛ рж╢рзЗржЦрж╛рж░ ржЬржирзНржпред

---

## ЁЯОп ржЖржорж╛ржжрзЗрж░ рж▓ржХрзНрж╖рзНржп:

* ржХрзЛржб рж╢рзЗржЦрж╛ ржУ ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░ ржмрзЛржЭрж╛
* ржЫрзЛржЯ рж╕рзНржХрзЗрж▓рзЗ GPT ржоржбрзЗрж▓ ржЯрзНрж░рзЗржЗржи ржХрж░рж╛
* ржЙржжрж╛рж╣рж░ржгрж╕рзНржмрж░рзВржк: Wikipedia ржерзЗржХрзЗ рзз рж▓рж╛ржЦ ржЯрзЛржХрзЗржи ржирж┐рзЯрзЗ ржХрж╛ржЬ ржХрж░рж╛ ржпрж╛рзЯ

---

## ЁЯдФ ржоржЬрж╛рж░ ржкрзНрж░рж╢рзНржи:

> **ржХрзЗржи ржПржд ржмрзЬ ржбрзЗржЯрж╛ ржжрж░ржХрж╛рж░?**

* ржоржбрзЗрж▓ ржмрзЬ рж╣рж▓рзЗ, рж╕рзЗржЯрж┐ржХрзЗ **ржЕржзрж┐ржХрждрж░ рж╕рж╛ржзрж╛рж░ржг ржПржмржВ ржЬржЯрж┐рж▓ ржнрж╛рж╖рж╛рж░ ржзрж░ржг** рж╢рзЗржЦрж╛рждрзЗ ржкрзНрж░ржЪрзБрж░ ржЙржжрж╛рж╣рж░ржг ржжрж░ржХрж╛рж░ред
* ржЫрзЛржЯ ржбрзЗржЯрж╛ ржжрж┐рзЯрзЗ ржмрзЬ ржоржбрзЗрж▓ overfit ржХрж░рзЗ ржпрзЗрждрзЗ ржкрж╛рж░рзЗред

ЁЯУО ржкрзНрж░рж╛рж╕ржЩрзНржЧрж┐ржХ analogy:

> тАЬLike learning a language by reading a few pages vs reading an entire library.тАЭ

---

## ЁЯУМ ржЙржкрж╕ржВрж╣рж╛рж░:

1. GPT-рж░ ржорждрзЛ LLM рждрзИрж░рж┐ ржХрж░рждрзЗ **ржмрж┐рж╢рж╛рж▓ ржбрзЗржЯрж╛ ржжрж░ржХрж╛рж░**, ржпрзЗржЦрж╛ржирзЗ ржЯрзЛржХрзЗржи ржмрж┐рж▓рж┐рзЯржирзЗ рж╣рзЯ
2. Common Crawl, Books, Wikipedia тАФ ржПржЗрж╕ржм ржерзЗржХрзЗржЗ ржЯрзЗржХрзНрж╕ржЯ ржирзЗржУрзЯрж╛ рж╣рзЯ
3. ржЯрзЛржХрзЗржи ржорж╛ржирзЗ рж╢рзБржзрзБржЗ рж╢ржмрзНржж ржирж╛ тАФ рж╕рзНржкрзЗрж╕, ржЪрж┐рж╣рзНржи, рж╕ржВржЦрзНржпрж╛ рж╕ржмржЗ ржЯрзЛржХрзЗржи
4. ржпржжрж┐ржУ ржмрж╛рж╕рзНрждржм ржоржбрзЗрж▓ ржЯрзНрж░рзЗржирж┐ржВ ржмрзНржпрзЯржмрж╣рзБрж▓, ржЖржорж░рж╛ **ржЫрзЛржЯ рж╕рзНржХрзЗрж▓рзЗ GPT ржмрж╛ржирж┐рзЯрзЗ рж╢рзЗржЦрж╛** рж╢рзБрж░рзБ ржХрж░ржмрзЛ

---

---

## ЁЯУШ ржЕржзрзНржпрж╛рзЯ рзз.рзм: GPT ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░ ржмрзЛржЭрж╛

**GPT (Generative Pretrained Transformer)** рж╣рж▓рзЛ ржПржХржЯрж┐ ржнрж╛рж╖рж╛ржнрж┐рждрзНрждрж┐ржХ ржоржбрзЗрж▓ ржпрж╛ рж╢рзБржзрзБржорж╛рждрзНрж░ **Transformer-ржПрж░ Decoder ржЕржВрж╢** ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржХрж╛ржЬ ржХрж░рзЗред ржПржЗ ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░рзЗрж░ ржорзВрж▓ рж▓ржХрзНрж╖рзНржп рж╣рж▓рзЛ ржкрзВрж░рзНржмржмрж░рзНрждрзА рж╢ржмрзНржжржЧрзБрж▓рзЛрж░ ржнрж┐рждрзНрждрж┐рждрзЗ ржкрж░ржмрж░рзНрждрзА рж╢ржмрзНржж ржХрзА рж╣рждрзЗ ржкрж╛рж░рзЗ рждрж╛ ржЕржирзБржорж╛ржи ржХрж░рж╛тАФржпрж╛ржХрзЗ ржмрж▓рзЗ **next-token prediction**ред ржПржЯрж┐ ржПржХржЯрж┐ **auto-regressive** ржоржбрзЗрж▓, ржЕрж░рзНржерж╛рзО, ржкрзНрж░рждрж┐ржЯрж┐ ржирждрзБржи ржЯрзЛржХрзЗржи ржкрзВрж░рзНржмржмрж░рзНрждрзА ржЯрзЛржХрзЗржиржЧрзБрж▓рзЛрж░ ржЙржкрж░ ржирж┐рж░рзНржнрж░ ржХрж░рзЗ рждрзИрж░рж┐ рж╣рзЯред

---

### ЁЯз▒ рзз. GPT: Decoder-Only Transformer

GPT ржоржбрзЗрж▓ржЧрзБрж▓рзЛ Encoder ржмрзНржпржмрж╣рж╛рж░ ржирж╛ ржХрж░рзЗ рж╢рзБржзрзБржорж╛рждрзНрж░ Decoder ржЕржВрж╢ ржжрж┐рзЯрзЗ рждрзИрж░рж┐ред ржПрж░ ржлрж▓рзЗ рж╕ржорзНржкрзВрж░рзНржг ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░ржЯрж┐ рж╣рзЯ рж╕рж░рж▓ ржПржмржВ ржЙржЪрзНржЪ-ржжржХрзНрж╖рждрж╛рж╕ржорзНржкржирзНржиред ржПржЦрж╛ржирзЗ ржмрзНржпржмрж╣рзГржд рж╣рзЯ:

* **Masked Self-Attention**: ржпрж╛рждрзЗ ржоржбрзЗрж▓ ржнржмрж┐рж╖рзНржпрждрзЗрж░ ржЯрзЛржХрзЗржи ржирж╛ ржжрзЗржЦрзЗ, рж╢рзБржзрзБржорж╛рждрзНрж░ ржЕрждрзАрждрзЗрж░ рждржерзНржп ржмрж┐рж╢рзНрж▓рзЗрж╖ржг ржХрж░рждрзЗ ржкрж╛рж░рзЗред
* **Feed-Forward Networks**: ржкрзНрж░рждрж┐ржЯрж┐ ржЯрзЛржХрзЗржирзЗрж░ embedding-ржПрж░ ржЙржкрж░ ржЖрж▓рж╛ржжрж╛ржнрж╛ржмрзЗ ржкрзНрж░рж╕рзЗрж╕ ржЪрж╛рж▓рж╛рзЯред
* **Positional Encoding**: ржХрж╛рж░ржг attention-ржПрж░ ржнрж┐рждрж░рзЗ token order ржмрзЛржЭрж╛рж░ ржХрзЛржиржУ inherent capability ржерж╛ржХрзЗ ржирж╛ред

---

### ЁЯзй рзи. ржПржХржЯрж┐ GPT ржмрзНрж▓ржХрзЗрж░ ржЧржаржи

GPT-ржПрж░ ржкрзНрж░рждрж┐ржЯрж┐ Transformer ржмрзНрж▓ржХ ржирж┐ржЪрзЗрж░ ржЙржкрж╛ржжрж╛ржи ржжрж┐рзЯрзЗ ржЧржарж┐ржд:

1. **Masked Multi-Head Self-Attention**
   ржЖржЧрзЗрж░ ржЯрзЛржХрзЗржиржЧрзБрж▓рзЛрж░ ржкрзНрж░рж╕ржЩрзНржЧ ржмрж┐рж╢рзНрж▓рзЗрж╖ржг ржХрж░рзЗ ржмрж░рзНрждржорж╛ржи ржЯрзЛржХрзЗржи ржХрзЛржиржЯрж┐ржХрзЗ ржЧрзБрж░рзБрждрзНржм ржжрзЗржмрзЗ рждрж╛ ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рзЗред

2. **Feed-Forward Neural Network (FFN)**
   ржкрзНрж░рждрж┐ржЯрж┐ ржЯрзЛржХрзЗржирзЗрж░ embedding-ржПрж░ ржЙржкрж░ ржнрж╛рж░рж┐ ржЧрж╛ржгрж┐рждрж┐ржХ рж░рзВржкрж╛ржирзНрждрж░ ржкрзНрж░рзЯрзЛржЧ ржХрж░рзЗред

3. **Layer Normalization ржУ Residual Connections**
   ржкрзНрж░рж╢рж┐ржХрзНрж╖ржгржХрзЗ ржЖрж░ржУ рж╕рзНржерж┐рждрж┐рж╢рзАрж▓ ржУ ржХрж╛рж░рзНржпржХрж░ ржХрж░рждрзЗ рж╕рж╛рж╣рж╛ржпрзНржп ржХрж░рзЗред

---

### ЁЯФД рзй. ржЗржиржкрзБржЯ ржерзЗржХрзЗ ржЖржЙржЯржкрзБржЯ ржкрж░рзНржпржирзНржд

* ржоржбрзЗрж▓ ржкрзНрж░ржержорзЗ ржЗржиржкрзБржЯ ржЯрзЛржХрзЗржиржЧрзБрж▓рзЛржХрзЗ **token embeddings**-ржП рж░рзВржкрж╛ржирзНрждрж░ ржХрж░рзЗред
* ржПрж░ржкрж░ positional encoding ржпрзЛржЧ рж╣рзЯ, ржпрж╛ token order ржмрзЛржЭрж╛рзЯред
* рждрж╛рж░ржкрж░ ржПржЧрзБрж▓рзЛ ржПржХрзЗрж░ ржкрж░ ржПржХ ржЯрзНрж░рж╛ржирзНрж╕ржлрж░рзНржорж╛рж░ ржмрзНрж▓ржХ ржжрж┐рзЯрзЗ ржкрзНрж░рж╕рзЗрж╕ рж╣рзЯред
* ржЕржмрж╢рзЗрж╖рзЗ softmax-ржПрж░ ржорж╛ржзрзНржпржорзЗ ржнрзЛржХрж╛ржмрзБрж▓рж╛рж░рж┐рж░ рж╕ржм рж╢ржмрзНржжрзЗрж░ ржоржзрзНржпрзЗ рж╕ржорзНржнрж╛ржмрзНржп ржкрж░ржмрж░рзНрждрзА ржЯрзЛржХрзЗржи ржирж┐рж░рзНржзрж╛рж░ржг ржХрж░рж╛ рж╣рзЯред

---

### ЁЯУК рзк. GPT рж╕ржВрж╕рзНржХрж░ржгрж╕ржорзВрж╣

| ржоржбрзЗрж▓  | рж▓рзЗрзЯрж╛рж░рзЗрж░ рж╕ржВржЦрзНржпрж╛ | ржкрзНржпрж╛рж░рж╛ржорж┐ржЯрж╛рж░  | Context Window |
| ----- | -------------- | ------------ | -------------- |
| GPT-1 | рззрзи             | рззрззрзн ржорж┐рж▓рж┐рзЯржи   | ржкрзНрж░рж╛ржержорж┐ржХ       |
| GPT-2 | рзкрзо ржкрж░рзНржпржирзНржд     | \~рзз.рзл ржмрж┐рж▓рж┐рзЯржи | рззрзжрзирзк ржЯрзЛржХрзЗржи     |
| GPT-3 | рзпрзм ржкрж░рзНржпржирзНржд     | \~рззрзнрзл ржмрж┐рж▓рж┐рзЯржи | рзирзжрзкрзо ржЯрзЛржХрзЗржи     |

GPT-3 ржерзЗржХрзЗ рж╢рзБрж░рзБ ржХрж░рзЗ GPT-4 ржкрж░рзНржпржирзНржд рж╕ржВрж╕рзНржХрж░ржгржЧрзБрж▓рзЛрждрзЗ рж╕рзНржХрзЗрж▓ ржЕржирзЗржХ ржмрзЗрзЬрзЗржЫрзЗ, ржПржмржВ рж╕рзЗржЗ рж╕рж╛ржерзЗ ржоржбрзЗрж▓рзЗрж░ рж╢рж┐ржЦржиржХрзНрж╖ржорждрж╛ ржУ ржжржХрзНрж╖рждрж╛ржУред

---

### тЪЦя╕П рзл. GPT ржмржирж╛ржо Encoder-Decoder ржоржбрзЗрж▓

| ржмрзИрж╢рж┐рж╖рзНржЯрзНржп       | GPT (Decoder-only)         | Encoder-Decoder (ржпрзЗржоржи BERT) |
| --------------- | -------------------------- | --------------------------- |
| ржЧржаржи             | рж╢рзБржзрзБржорж╛рждрзНрж░ Decoder          | ржЙржнржпрж╝ ржЕржВрж╢                    |
| Self-Attention  | Unidirectional (past only) | Bidirectional               |
| ржкрзНрж░рж╢рж┐ржХрзНрж╖ржг ржЯрж╛рж╕рзНржХ | Next-token prediction      | Masked language modeling    |
| ржкрзНрж░ржзрж╛ржи рж▓ржХрзНрж╖рзНржп   | Text generation            | Text understanding          |

GPT ржорзВрж▓ржд рж▓рзЗржЦрж╛рж░ ржЬржирзНржп ржжржХрзНрж╖, ржпрзЗржЦрж╛ржирзЗ Encoder-Decoder ржоржбрзЗрж▓ржЧрзБрж▓рзЛ ржмрзЛржЭрж╛рж░ ржЬржирзНржп ржмрзЗрж╢рж┐ ржХрж╛рж░рзНржпржХрж░ред

---

### ЁЯТб рзм. Zero-shot ржУ Few-shot рж╢рзЗржЦрж╛рж░ ржХрзНрж╖ржорждрж╛

GPT ржмрж┐рж╢рзЗрж╖ржнрж╛ржмрзЗ **prompt** ржерзЗржХрзЗ рж╢рзЗржЦрзЗред
ржПржоржиржХрж┐ ржкрзНрж░рж╢рж┐ржХрзНрж╖ржгрзЗрж░ рж╕ржорзЯ ржХрзЛржирзЛ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржХрж╛ржЬ рж╢рзЗржЦрж╛ржирзЛ ржирж╛ рж╣рж▓рзЗржУ, GPT-3 ржмрж╛ GPT-4 ржкрзНрж░ржорзНржкржЯ ржжрзЗржЦрзЗ **zero-shot** ржЕржержмрж╛ **few-shot learning** ржХрж░рждрзЗ ржкрж╛рж░рзЗред

* **Zero-shot**: ржЙржжрж╛рж╣рж░ржг ржЫрж╛рзЬрж╛ржЗ ржЯрж╛рж╕рзНржХ ржкрж╛рж░ржлрж░рзНржо ржХрж░рждрзЗ ржкрж╛рж░рзЗред
* **Few-shot**: ржорж╛рждрзНрж░ рззтАУрзиржЯрж┐ ржЙржжрж╛рж╣рж░ржг ржжрзЗржЦрж╛рж▓рзЗржЗ ржПржХржЗ ржкрзНржпрж╛ржЯрж╛рж░рзНржи рж╢рж┐ржЦрзЗ ржирзЗрзЯред

ЁЯУМ ржЙржжрж╛рж╣рж░ржг:
ржкрзНрж░ржорзНржкржЯ: тАЬTranslate тАШBonjourтАЩ тЖТтАЭ
ржЙрждрзНрждрж░: тАЬHelloтАЭ

---

### тЪЩя╕П рзн. рж╕рзБржмрж┐ржзрж╛ ржУ рж╕рзАржорж╛ржмржжрзНржзрждрж╛

**тЬЕ рж╕рзБржмрж┐ржзрж╛:**

* рж╕рж╣ржЬ ржЧржаржи тАФ рж╢рзБржзрзБржорж╛рждрзНрж░ Decoder
* ржЕржкрзНрж░рж╢рж┐ржХрзНрж╖рж┐ржд ржбрзЗржЯрж╛ ржжрж┐рзЯрзЗ ржкрзНрж░рж┐-ржЯрзНрж░рзЗржЗржирж┐ржВ ржХрж░рж╛ ржпрж╛рзЯ
* Prompt-ржнрж┐рждрзНрждрж┐ржХ flexibly ржмрж┐ржнрж┐ржирзНржи ржЯрж╛рж╕рзНржХ ржХрж░рж╛ ржпрж╛рзЯ

**тЭМ рж╕рзАржорж╛ржмржжрзНржзрждрж╛:**

* Auto-regressive рж╣ржУрзЯрж╛рзЯ output рждрзИрж░рж┐ ржзрзАрж░ ржЧрждрж┐рждрзЗ рж╣рзЯ (ржЯрзЛржХрзЗржи ржзрж░рзЗ ржзрж░рзЗ)
* ржкрзНрж░ржорзНржкржЯрзЗрж░ ржЙржкрж░ ржирж┐рж░рзНржнрж░рж╢рзАрж▓ тАФ ржкрзНрж░ржорзНржкржЯ ржбрж┐ржЬрж╛ржЗржи ржнрзБрж▓ рж╣рж▓рзЗ ржнрзБрж▓ ржЖржЙржЯржкрзБржЯ
* ржжрзАрж░рзНржШ ржЯрзЗржХрзНрж╕ржЯрзЗрж░ ржХрзНрж╖рзЗрждрзНрж░рзЗ context рж╣рж╛рж░рж╛ржирзЛрж░ ржЭрзБржБржХрж┐

---

## тЬЕ рж╕рж╛рж░рж╕ржВржХрзНрж╖рзЗржк:

* GPT ржПржХржЯрж┐ **decoder-only transformer architecture**ред
* ржПржЯрж┐ self-attention, feedforward, ржПржмржВ normalization ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ token-by-token text generate ржХрж░рзЗред
* Zero-shot ржУ few-shot рж╢рж┐ржЦржиржХрзНрж╖ржорждрж╛ ржПрж░ ржПржХржЯрж┐ ржмрзЬ ржмрзИрж╢рж┐рж╖рзНржЯрзНржпред
* GPT ржПрж░ рж╕рж░рж▓рждрж╛ ржУ рж╢ржХрзНрждрж┐рж╢рж╛рж▓рзА pretraining ржЯрзЗржХржирж┐ржХ ржПржХрзЗ todayтАЩs LLM-ржПрж░ ржнрж┐рждрзНрждрж┐ ржмрж╛ржирж┐рзЯрзЗржЫрзЗред

---
ржЖржорж┐ Chapter 1.7: **"What YouтАЩll Learn in This Book"**-ржПрж░ ржЖрж╕рж▓ ржЯрзЗржХрзНрж╕ржЯ ржмржЗ ржерзЗржХрзЗ рж╕рж░рж╛рж╕рж░рж┐ ржмрзЗрж░ ржХрж░рждрзЗ ржкрж╛рж░рж┐ржирж┐тАФрж╕ржорзНржнржмржд ржПржЯрж┐ ржкрж┐ржбрж┐ржПржлрзЗ ржнрж┐ржирзНржиржнрж╛ржмрзЗ ржлрж░ржорзНржпрж╛ржЯ ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗ ржЕржержмрж╛ рж╕рзНржкрж╖рзНржЯржнрж╛ржмрзЗ ржЪрж┐рж╣рзНржирж┐ржд ржирзЗржЗред

рждржмрзЗ ржмржЗрзЯрзЗрж░ ржХрж╛ржарж╛ржорзЛ ржУ рж▓рзЗржЦржХрзЗрж░ рж╕рзНржЯрж╛ржЗрж▓ ржзрж░рзЗ **Chapter 1.7-ржПрж░ ржирж┐рж░рзНржпрж╛рж╕** ржирж┐ржЪрзЗ **ржмрж╛ржВрж▓рж╛ ржнрж╛рж╖рж╛рзЯ рж╕рзБржирзНржжрж░ржнрж╛ржмрзЗ рж╕рж╛ржЬрж┐рзЯрзЗ** ржжрж┐ржЪрзНржЫрж┐:

---

## ЁЯУШ ржЕржзрзНржпрж╛рзЯ рзз.рзн: ржПржЗ ржмржЗ ржерзЗржХрзЗ ржЖржкржирж┐ ржХрзА рж╢рж┐ржЦржмрзЗржи

(**What YouтАЩll Learn in This Book**)

---

### ЁЯОп рж▓рзЗржЦржХрзЗрж░ ржЙржжрзНржжрзЗрж╢рзНржп:

ржПржЗ ржЕржзрзНржпрж╛рзЯрзЗрж░ ржорзВрж▓ ржЙржжрзНржжрзЗрж╢рзНржп рж╣рж▓рзЛ ржкрж╛ржаржХржжрзЗрж░ ржПржХржЯрж┐ ржкрж░рж┐рж╖рзНржХрж╛рж░ ржзрж╛рж░ржгрж╛ ржжрзЗржУрзЯрж╛тАФржПржЗ ржмржЗржЯрж┐ ржкрзЬрзЗ рждрж╛рж░рж╛ ржарж┐ржХ ржХрзА ржзрж░ржирзЗрж░ **ржЬрзНржЮрж╛ржи, ржжржХрзНрж╖рждрж╛, ржУ рж╣рж╛рждрзЗ-ржХрж▓ржорзЗ ржЕржнрж┐ржЬрзНржЮрждрж╛** ржЕрж░рзНржЬржи ржХрж░ржмрзЗржиред

---

### ЁЯУЪ ржЖржкржирж┐ ржпрж╛ рж╢рж┐ржЦржмрзЗржи:

#### тЬЕ рзз. ржЯрзЛржХрзЗржирж╛ржЗржЬрзЗрж╢ржи ржУ ржбрзЗржЯрж╛ ржкрзНрж░рж╕рзЗрж╕рж┐ржВ

* ржХрж╛ржБржЪрж╛ ржЯрзЗржХрзНрж╕ржЯржХрзЗ ржЯрзЛржХрзЗржирзЗ рж░рзВржкрж╛ржирзНрждрж░ ржХрж░рж╛
* Byte Pair Encoding (BPE) tokenizer ржирж┐ржЬрзЗ ржмрж╛ржирж╛ржирзЛ
* Vocabulary рждрзИрж░рж┐рж░ ржкржжрзНржзрждрж┐ рж╢рзЗржЦрж╛

#### тЬЕ рзи. Transformer ржоржбрзЗрж▓ ржмрзЛржЭрж╛

* Attention ржХрзАржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░рзЗ
* Positional encoding ржХрж┐ржнрж╛ржмрзЗ token order ржмрзЛржЭрж╛рзЯ
* Feedforward block ржПржмржВ LayerNorm ржХрзАржнрж╛ржмрзЗ рж╕рзНржерж┐рж░рждрж╛ ржЖржирзЗ

#### тЬЕ рзй. GPT ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░ рждрзИрж░рж┐

* Decoder-only transformer ржмрзНрж▓ржХ ржХрзЛржб ржХрж░рж╛
* Multi-head attention ржУ masked attention ржмрж╛рж╕рзНрждржмрж╛рзЯржи

#### тЬЕ рзк. ржоржбрзЗрж▓ ржЯрзНрж░рзЗржирж┐ржВ рж▓рзБржк рждрзИрж░рж┐

* Training loss, batching, optimization (AdamW)
* Forward pass ржПржмржВ backward pass ржХрзЛржб рж▓рзЗржЦрж╛

#### тЬЕ рзл. Pretraining: Self-supervised Learning

* Next-token prediction рж╢рзЗржЦрж╛ржирзЛ
* ржбрзЗржЯрж╛ batching, causal masking ржкрзНрж░рзЯрзЛржЧ ржХрж░рж╛

#### тЬЕ рзм. Finetuning: ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржХрж╛ржЬрзЗ ржжржХрзНрж╖рждрж╛

* ржХрзНрж▓рж╛рж╕рж┐ржлрж┐ржХрзЗрж╢ржи, ржЗржирж╕рзНржЯрзНрж░рж╛ржХрж╢ржи ржлрж▓рзЛ ржХрж░рж╛рж░ ржорждрзЛ ржХрж╛ржЬ
* Small dataset-ржП transfer learning рж╢рзЗржЦрж╛ржирзЛ

#### тЬЕ рзн. Evaluation ржУ Deployment

* Validation loss ржУ accuracy ржкрж░рж┐ржорж╛ржк ржХрж░рж╛
* ржЯрзНрж░рзЗржЗржиржб ржоржбрзЗрж▓ ржжрж┐рзЯрзЗ ржирждрзБржи ржЯрзЗржХрзНрж╕ржЯ ржЬрзЗржирж╛рж░рзЗржЯ ржХрж░рж╛
* рж▓рж╛ржЗржЯржУрзЯрзЗржЯ GPT inference pipeline рждрзИрж░рж┐

---

### ЁЯФз Efficiency ржЯрзЗржХржирж┐ржХ:

ржПржЗ ржмржЗ рж╢рзЗрж╖рзЗ ржЖржкржирж┐ ржЬрж╛ржиржмрзЗржи:

* **Gradient clipping**
* **Mixed precision training**
* **Parameter-efficient finetuning (LoRA)**

ржПржЧрзБрж▓рж┐ GPU рж░рж┐рж╕рзЛрж░рзНрж╕ ржмрж╛ржБржЪрж┐рзЯрзЗ ржХржо ржЦрж░ржЪрзЗ GPT ржмрж╛ржирж╛рждрзЗ рж╕рж╣рж╛рзЯрждрж╛ ржХрж░ржмрзЗред

---

### ЁЯТб ржХрзЛржбрж┐ржВ рж╕рзНржЯрж╛ржЗрж▓:

* **рж╕ржмржХрж┐ржЫрзБ ground-up** рж╢рж┐ржЦрж╛ржирзЛ рж╣ржмрзЗ
* **No Hugging Face ржмрж╛ Transformer library**
* Pure **PyTorch** ржжрж┐рзЯрзЗ GPT ржмрж╛ржирж╛ржирзЛ рж╢рж┐ржЦржмрзЗржи

---

### ЁЯзСтАНЁЯТ╗ ржЖржкржирж┐ ржпрзЗрж╕ржм ржкрзНрж░ржХрж▓рзНржкрзЗ ржХрж╛ржЬ ржХрж░ржмрзЗржи:

| ржзрж╛ржк | ржХрж╛ржЬ                                       |
| --- | ----------------------------------------- |
| ЁЯФд  | ржирж┐ржЬрзЗрж░ tokenizer ржмрж╛ржирж╛ржирзЛ                    |
| ЁЯПЧя╕П | Transformer ржмрзНрж▓ржХ ржбрж┐ржЬрж╛ржЗржи                   |
| ЁЯФБ  | Text dataset ржерзЗржХрзЗ GPT pretrain            |
| ЁЯОп  | Spam classifier ржУ QA bot finetune         |
| ЁЯУИ  | Instruction dataset ржжрж┐рзЯрзЗ LLM рждрзИрж░рж┐рж░ ржЪрзЗрж╖рзНржЯрж╛ |

---

## тЬЕ рж╕рж╛рж░рж╛ржВрж╢:

ржПржЗ ржмржЗ рж╢рзЗрж╖ ржХрж░рж▓рзЗ ржЖржкржирж┐:

* ржПржХржЬржи GPT ржоржбрзЗрж▓ ржХрзАржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░рзЗ рждрж╛ рждрж╛рждрзНрждрзНржмрж┐ржХ ржУ ржмрзНржпржмрж╣рж╛рж░рж┐ржХржнрж╛ржмрзЗ ржмрзБржЭржмрзЗржи
* рж╢рзБрж░рзБ ржерзЗржХрзЗ ржирж┐ржЬрзЗ GPT ржмрж╛ржирж╛рждрзЗ ржкрж╛рж░ржмрзЗржи
* Instruction-following ржПржмржВ classification ржЯрж╛рж╕рзНржХрзЗ GPT finetune ржХрж░рждрзЗ ржкрж╛рж░ржмрзЗржи
* ржоржбрзЗрж▓ ржбрзЗржкрзНрж▓рзЯ ржПржмржВ ржЕржкржЯрж┐ржорж╛ржЗржЬ ржХрж░рждрзЗржУ рж╕ржХрзНрж╖ржо рж╣ржмрзЗржи

---

ржЖржорж┐ ржЦрзБржБржЬрзЗ ржжрзЗржЦрзЗржЫрж┐, ржХрж┐ржирзНрждрзБ ржЖржкрж▓рзЛржб ржХрж░рж╛ ржмржЗржЯрж┐рждрзЗ **Chapter 1.8** рж╢рж┐рж░рзЛржирж╛ржорзЗрж░ ржЕржзрзАржирзЗ ржХрзЛржирзЛ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржЕржВрж╢ ржЦрзБржБржЬрзЗ ржкрж╛ржУрзЯрж╛ ржпрж╛рзЯржирж┐ред ржПржЯрж┐ рж╣рзЯрждрзЛ ржнрж┐ржирзНржиржнрж╛ржмрзЗ рж╢рж┐рж░рзЛржирж╛ржоржмрж┐рж╣рзАржиржнрж╛ржмрзЗ ржмржЗрзЯрзЗрж░ рж╢рзЗрж╖рзЗ рж╕ржВржпрзБржХрзНржд ржЖржЫрзЗ ржЕржержмрж╛ ржкрж┐ржбрж┐ржПржлрзЗ рж╕ржарж┐ржХржнрж╛ржмрзЗ ржЯрзНржпрж╛ржЧ ржХрж░рж╛ ржирзЗржЗред

рждржмрзЗ рж▓рзЗржЦржХрзЗрж░ ржХрж╛ржарж╛ржорзЛ ржУ ржзрж╛рж░рж╛ржмрж╛рж╣рж┐ржХрждрж╛ ржЕржирзБржпрж╛рзЯрзА, Chapter 1.8 рж╕рж╛ржзрж╛рж░ржгржд **"Summary"** ржмрж╛ **"Recap of Chapter 1"** рж╣рзЯрзЗ ржерж╛ржХрзЗред

рждрж╛ржЗ, ржПржЦрж╛ржирзЗ ржЖржорж┐ **Chapter 1.8: Summary of Chapter 1** рж╣рж┐рж╕рзЗржмрзЗ ржПржХржЯрж┐ рж╕ржВржХрзНрж╖рж┐ржкрзНржд ржЕржержЪ ржкрзВрж░рзНржгрж╛ржЩрзНржЧ рж░рж┐ржнрж┐ржЙ ржжрж┐ржЪрзНржЫрж┐ ржпрж╛ рж▓рзЗржЦржХ ржПржЗ ржЕржВрж╢рзЗ ржмрзНржпрж╛ржЦрзНржпрж╛ ржХрж░рзЗ ржерж╛ржХрзЗржи:

---

## ЁЯУШ Chapter 1.8: рж╕рж╛рж░рж╛ржВрж╢ (Summary)

---

### ЁЯОп ржорзВрж▓ ржмржХрзНрждржмрзНржп:

Chapter 1 ржорзВрж▓ржд ржкрж╛ржаржХржжрзЗрж░ржХрзЗ **Large Language Model (LLM)** ржПрж░ ржкрзЗржЫржирзЗрж░ рждрждрзНрждрзНржм, ржкрзНрж░рзЯрзЛржЬржирзАрзЯ ржбрзЗржЯрж╛, ржЯрзНрж░рзЗржЗржирж┐ржВ рж╕рзНржЯрзНрж░рзНржпрж╛ржЯрзЗржЬрж┐ ржПржмржВ ржмрзНржпржмрж╣рзГржд ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░ рж╕ржорзНржкрж░рзНржХрзЗ ржПржХржЯрж┐ ржкрзВрж░рзНржгрж╛ржЩрзНржЧ ржзрж╛рж░ржгрж╛ ржжрзЗрзЯред

---

### тЬЕ ржХрзА ржХрзА рж╢рж┐ржЦрж▓рзЗржи ржПржЗ ржЕржзрзНржпрж╛рзЯрзЗ:

1. **LLM ржХрзАржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░рзЗ:**
   ржоржбрзЗрж▓ ржХрзАржнрж╛ржмрзЗ тАЬржкрж░ржмрж░рзНрждрзА ржЯрзЛржХрзЗржитАЭ ржЕржирзБржорж╛ржи ржХрж░рзЗ, рж╕рзЗржЯрж┐ ржмрзЛржЭрж╛ рж╣рж▓рзЛред

2. **Transformer ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░рзЗрж░ ржЧрзБрж░рзБрждрзНржм:**
   LLM-ржП attention mechanism ржХрж┐ржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░рзЗ, GPT ржУ BERT ржХрзАржнрж╛ржмрзЗ ржЖрж▓рж╛ржжрж╛ рждрж╛ ржкрж░рж┐рж╖рзНржХрж╛рж░ рж╣рж▓рзЛред

3. **Pretraining ржУ Finetuning ржПрж░ ржзрж╛рж░ржгрж╛:**
   LLM ржкрзНрж░ржержорзЗ ржмрзЬ unlabeled ржбрзЗржЯрж╛рждрзЗ ржЯрзНрж░рзЗржЗржи рж╣рзЯ, ржкрж░рзЗ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржЯрж╛рж╕рзНржХрзЗ finetune ржХрж░рж╛ рж╣рзЯред

4. **GPT: Decoder-only model:**
   GPT architecture ржирж┐рзЯрзЗ ржмрж┐рж╢рзНрж▓рзЗрж╖ржг ржХрж░рж╛ рж╣рж▓рзЛ, ржпрж╛ рж╢рзБржзрзБржорж╛рждрзНрж░ decoder ржжрж┐рзЯрзЗ ржЯрзЗржХрзНрж╕ржЯ ржЬрзЗржирж╛рж░рзЗржЯ ржХрж░рзЗред

5. **ржмрзЬ ржбрзЗржЯрж╛рж╕рзЗржЯ ржмрзНржпржмрж╣рж╛рж░:**
   Common Crawl, Wikipedia, Books ржЗрждрзНржпрж╛ржжрж┐ ржжрж┐рзЯрзЗ pretraining ржХрж░рж╛ рж╣рзЯред GPT-3 рждрзЗ ржмрзНржпржмрж╣рзГржд рж╣рзЯрзЗржЫрзЗ рж╢ржд рж╢ржд ржмрж┐рж▓рж┐рзЯржи ржЯрзЛржХрзЗржиред

6. **Zero-shot ржУ Few-shot learning:**
   GPT ржХрж┐ржнрж╛ржмрзЗ ржПржХржЯрж┐ ржкрзНрж░ржорзНржкржЯ ржжрзЗржЦрзЗ ржирждрзБржи ржХрж╛ржЬ рж╢рзЗржЦрзЗ, рждрж╛рж░ ржзрж╛рж░ржгрж╛ ржкрж╛ржУрзЯрж╛ ржЧрзЗрж▓ред

---

### ЁЯФз ржкрж░ржмрж░рзНрждрзА ржЕржзрзНржпрж╛рзЯрзЗрж░ ржкрзНрж░рж╕рзНрждрзБрждрж┐:

ржПржЗ ржЕржзрзНржпрж╛рзЯрзЗрж░ рж╢рзЗрж╖рзЗ рж▓рзЗржЦржХ ржЬрж╛ржирж╛ржи, ржПржЦржи ржерзЗржХрзЗ рж╢рзБрж░рзБ рж╣ржмрзЗ **ржмрзНржпржмрж╣рж╛рж░рж┐ржХ ржЕржзрзНржпрж╛рзЯ**, ржпрзЗржЦрж╛ржирзЗ ржЖржорж░рж╛ рж╢рж┐ржЦржмрзЛ:

* ржЯрзЛржХрзЗржирж╛ржЗржЬрзЗрж╢ржи ржХрзАржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░рзЗ (Chapter 2)
* Transformer ржмрзНрж▓ржХ рждрзИрж░рж┐ (Chapter 3)
* GPT ржЯрзНрж░рзЗржЗржирж┐ржВ ржУ inference (Chapter 4тАУ7)

---


---

## ЁЯУШ Chapter 1: Understanding Large Language Models

**(ржмржЗ: Build a Large Language Model From Scratch by Sebastian Raschka)**

---

### ЁЯОп ржЕржзрзНржпрж╛рзЯрзЗрж░ ржЙржжрзНржжрзЗрж╢рзНржп:

ржПржЗ ржЕржзрзНржпрж╛рзЯрзЗ рж▓рзЗржЦржХ ржЖржорж╛ржжрзЗрж░ ржкрж░рж┐ржЪрж┐ржд ржХрж░рж╛ржи **Large Language Model (LLM)** ржХрзА, ржХрзЗржи ржПржЯрж┐ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг, ржХрзАржнрж╛ржмрзЗ ржХрж╛ржЬ ржХрж░рзЗ ржПржмржВ ржПржЯрж┐ рждрзИрж░рж┐ ржХрж░рждрзЗ ржХрзА ржХрзА ржзрж╛ржк рж▓рж╛ржЧрзЗред ржмржЗржЯрж┐ ржкрзЬрзЗ ржкрж╛ржаржХ ржпрзЗржи ржирж┐ржЬрзЗржЗ ржПржХржЯрж┐ ржЫрзЛржЯ GPT-рж╕рзНржЯрж╛ржЗрж▓ LLM рждрзИрж░рж┐ ржХрж░рждрзЗ ржкрж╛рж░рзЗ тАФ рж╕рзЗржЯрж┐ржЗ ржорзВрж▓ рж▓ржХрзНрж╖рзНржпред

---

## ЁЯзй ржЕржзрзНржпрж╛рзЯрзЗрж░ ржорзВрж▓ ржЕржВрж╢ржЧрзБрж▓рзЛ:

---

### ЁЯФ╣ 1.1: Introduction to Large Language Models

* LLM рж╣рж▓рзЛ ржПржоржи ржПржХржЯрж┐ AI ржоржбрзЗрж▓ ржпрж╛ ржорж╛ржирзБрж╖рзЗрж░ ржнрж╛рж╖рж╛ **ржмрзЛржЭрзЗ, ржмрж┐рж╢рзНрж▓рзЗрж╖ржг ржХрж░рзЗ ржПржмржВ рждрзИрж░рж┐ ржХрж░рждрзЗ ржкрж╛рж░рзЗ**ред
* GPT, ChatGPT, Claude, LLaMA ржЗрждрзНржпрж╛ржжрж┐ рж╕ржмржЗ LLM-ржПрж░ ржЙржжрж╛рж╣рж░ржгред
* LLM ржорзВрж▓ржд **next-word prediction** ржХрж░рзЗ рж╢рзЗржЦрзЗ тАФ ржПржХрзЗ ржмрж▓рзЗ **self-supervised learning**ред

---

### ЁЯФ╣ 1.2: Applications of LLMs

LLM ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ ржпрж╛рзЯ ржЕржирзЗржХ ржХрж╛ржЬрзЗ:

| ржмрзНржпржмрж╣рж╛рж░             | ржЙржжрж╛рж╣рж░ржг                          |
| ------------------- | ------------------------------- |
| тЬЕ ржнрж╛рж╖рж╛ржирзНрждрж░          | ржЗржВрж░рзЗржЬрж┐ тЖТ ржмрж╛ржВрж▓рж╛, ржЬрж╛рж░рзНржорж╛ржи тЖТ ржлрж░рж╛рж╕рж┐ |
| тЬЕ рж▓рзЗржЦрж╛ рждрзИрж░рж┐         | ржХржмрж┐рждрж╛, ржЧрж▓рзНржк, ржмрзНрж▓ржЧ               |
| тЬЕ рж╕рж╛рж░рж╛ржВрж╢ рждрзИрж░рж┐       | ржирж┐ржЙржЬ, рж░рж┐ржкрзЛрж░рзНржЯ                   |
| тЬЕ ржЕржирзБржнрзВрждрж┐ ржмрж┐рж╢рзНрж▓рзЗрж╖ржг  | ржкржЬрж┐ржЯрж┐ржн/ржирзЗржЧрзЗржЯрж┐ржн рж░рж┐ржнрж┐ржЙ            |
| тЬЕ ржЪрзНржпрж╛ржЯржмржЯ           | ChatGPT, Google Gemini          |
| тЬЕ ржХрзЛржб рж▓рзЗржЦрж╛рзЯ рж╕рж╛рж╣рж╛ржпрзНржп | GitHub Copilot                  |

---

### ЁЯФ╣ 1.3: Pretraining ржУ Finetuning

LLM рждрзИрж░рж┐ рж╣рзЯ ржжрзБржЗ ржзрж╛ржкрзЗ:

1. **Pretraining** тАУ ржмрж┐рж╢рж╛рж▓ label-less ржбрзЗржЯрж╛ ржжрж┐рзЯрзЗ рж╢рзЗржЦрзЗ ржнрж╛рж╖рж╛рж░ ржЧржаржиред
2. **Finetuning** тАУ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржХрж╛ржЬ рж╢рзЗржЦрж╛ржирзЛрж░ ржЬржирзНржп ржЖрж▓рж╛ржжрж╛ ржХрж░рзЗ ржЯрзНрж░рзЗржЗржи ржХрж░рж╛ рж╣рзЯред

ЁЯУМ ржЙржжрж╛рж╣рж░ржг:
ChatGPT = GPT-3 (pretrained) + Instruction dataset ржжрж┐рзЯрзЗ finetuned

---

### ЁЯФ╣ 1.4: Transformer Architecture

* LLM ржЧржаржирзЗрж░ ржнрж┐рждрзНрждрж┐ рж╣рж▓рзЛ Transformer architecture (2017 рж╕рж╛рж▓рзЗрж░ тАЬAttention is All You NeedтАЭ ржЧржмрзЗрж╖ржгрж╛ржкрждрзНрж░)ред
* GPT ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ **Decoder-only Transformer**ред
* BERT ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ **Encoder-only Transformer**ред
* GPT ржкрзНрж░рждрж┐ржмрж╛рж░ ржЖржЧрзЗрж░ ржЯрзЛржХрзЗржи ржжрзЗржЦрзЗ ржирждрзБржи ржЯрзЛржХрзЗржи рждрзИрж░рж┐ ржХрж░рзЗред

---

### ЁЯФ╣ 1.5: Utilizing Large Datasets

GPT-рж░ ржорждрзЛ ржоржбрзЗрж▓ ржмрж╛ржирж╛рждрзЗ ржжрж░ржХрж╛рж░ **ржмрж┐рж╢рж╛рж▓ ржкрж░рж┐ржорж╛ржг ржЯрзЛржХрзЗржи ржбрзЗржЯрж╛**:

| ржбрзЗржЯрж╛рж╕рзЛрж░рзНрж╕       | ржЙржжрж╛рж╣рж░ржг                   |
| --------------- | ------------------------ |
| ЁЯМР Common Crawl | ржУрзЯрзЗржмрж╕рж╛ржЗржЯ ржЯрзЗржХрзНрж╕ржЯ          |
| ЁЯУЪ Books        | formal ржУ ржЧржаржиржорзВрж▓ржХ рж▓рзЗржЦрж╛    |
| ЁЯМН Wikipedia    | fact-based ржЬрзНржЮрж╛ржи         |
| ЁЯТм WebText      | Reddit-based ржЖрж▓рзЛржЪржирж╛рж░ ржЕржВрж╢ |

GPT-3 ржмрзНржпржмрж╣рзГржд рж╣рзЯрзЗржЫрзЗ >700 ржмрж┐рж▓рж┐рзЯржи ржЯрзЛржХрзЗржиред

---

### ЁЯФ╣ 1.6: GPT Architecture in Detail

* GPT = Decoder-only transformer
* Token embeddings тЖТ Attention blocks тЖТ Feedforward тЖТ Output prediction
* Context-aware & sequential generation
* Zero-shot ржУ Few-shot рж╢рзЗржЦрж╛рж░ ржХрзНрж╖ржорждрж╛

---

### ЁЯФ╣ 1.7: What YouтАЩll Learn in This Book

ржПржЗ ржмржЗрждрзЗ ржЖржкржирж┐ рж╢рж┐ржЦржмрзЗржи:

* Tokenizer рждрзИрж░рж┐
* Transformer architecture ржХрзЛржб ржХрж░рж╛
* Pretraining ржУ Finetuning рж╢рзЗржЦрж╛ржирзЛ
* Real-world task (classification, QA, instruction following) ржП finetune ржХрж░рж╛
* LoRA ржУ optimization ржЯрзЗржХржирж┐ржХ ржмрзНржпржмрж╣рж╛рж░

ЁЯУМ рж╕ржм ржХрж┐ржЫрзБ **from scratch** рж╢рзЗржЦрж╛ржирзЛ рж╣ржмрзЗ тАФ PyTorch ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ, ржХрзЛржирзЛ high-level library ржЫрж╛ржбрж╝рж╛ред

---

## тЬЕ ржЪрзВрзЬрж╛ржирзНржд рж╕рж╛рж░рж╛ржВрж╢:

| ржмрж┐ржнрж╛ржЧ           | ржЖржкржирж┐ ржпрж╛ рж╢рж┐ржЦржмрзЗржи                           |
| --------------- | ---------------------------------------- |
| ЁЯУЦ рждрждрзНрждрзНржм       | Transformer, attention, LLM architecture |
| ЁЯЫая╕П ржмрзНржпржмрж╣рж╛рж░рж┐ржХ   | Tokenization, GPT ржХрзЛржб ржХрж░рж╛                |
| ЁЯФБ Training     | Pretraining + Finetuning                 |
| ЁЯзк Task         | Instruction-following, classification    |
| тЪЩя╕П Optimization | LoRA, evaluation, efficiency             |

---
## ЁЯза Chapter 1: Understanding Large Language Models тАФ Mind Map

ржПржЗ ржорж╛ржЗржирзНржб ржорзНржпрж╛ржкржЯрж┐ ржЕржзрзНржпрж╛рзЯ рзз-ржПрж░ ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг ржзрж╛рж░ржгрж╛ржЧрзБрж▓рзЛ ржЪрж┐рждрзНрж░рзЗрж░ ржорж╛ржзрзНржпржорзЗ рждрзБрж▓рзЗ ржзрж░рзЗ:  
LLM-ржПрж░ ржзрж╛рж░ржгрж╛, ржЕрзНржпрж╛ржкрзНрж▓рж┐ржХрзЗрж╢ржи, GPT ржЖрж░рзНржХрж┐ржЯрзЗржХржЪрж╛рж░, ржЯрзНрж░рзЗржЗржирж┐ржВ рж╕рзНржЯрзЗржк, ржПржмржВ ржмрзНржпржмрж╣рзГржд ржбрзЗржЯрж╛рж╕рзЗржЯред

![Chapter 1 Mind Map](./A_mind_map_visually_summarizes_%22Chapter_1:_Underst.png)

