
---

## üîß What Is an Activation Function?

An **activation function** is a mathematical function used in neural networks to decide whether a neuron should be activated (i.e., contribute to the output). It applies to the **weighted sum of inputs + bias**, and its primary role is to introduce **non-linearity** into the model.

---

## üîç Why Are Activation Functions Important?

| Purpose             | Explanation                                                                                 |
| ------------------- | ------------------------------------------------------------------------------------------- |
| **Non-Linearity**   | Without it, a neural network acts like a simple linear model, unable to learn complex data. |
| **Signal Control**  | Helps determine if a neuron should activate or not.                                         |
| **Learn Patterns**  | Enables deep networks to learn from complicated datasets using layers and non-linearities.  |
| **Backpropagation** | Their derivatives are used during backpropagation to adjust weights and minimize loss.      |

---

## üìö Types of Activation Functions

### 1. **Sigmoid (Logistic Function)**

**Formula**:

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

**Properties**:

* Output range: (0, 1)
* Used for binary classification
* Problem: vanishing gradient for large/small inputs

### 2. **Tanh (Hyperbolic Tangent)**

**Formula**:

$$
\sigma(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

**Properties**:

* Output range: (‚àí1, 1)
* Zero-centered (better for optimization)
* Still can suffer from vanishing gradients

### 3. **ReLU (Rectified Linear Unit)**

**Formula**:

$$
\sigma(x) = \max(0, x)
$$

**Properties**:

* Most widely used in deep learning
* Output: 0 for x < 0, and x for x ‚â• 0
* Solves vanishing gradient to some extent
* Problem: ‚Äúdying ReLU‚Äù (neurons stuck at 0)

### 4. **Softmax**

**Formula**:

$$
\sigma(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
$$

**Properties**:

* Used in output layer for multi-class classification
* Outputs a **probability distribution** (sums to 1)

---

## ‚úÖ Summary Table

| Function | Output Range  | Common Use Case            | Notes                                    |
| -------- | ------------- | -------------------------- | ---------------------------------------- |
| Sigmoid  | (0, 1)        | Binary classification      | Can squash gradients                     |
| Tanh     | (‚àí1, 1)       | Hidden layers (earlier NN) | Zero-centered, better than sigmoid       |
| ReLU     | \[0, ‚àû)       | Most hidden layers         | Fast and effective; watch for dead units |
| Softmax  | (0, 1), sum=1 | Output layer (multi-class) | Gives probabilities for each class       |

---


---

## 1Ô∏è‚É£ ReLU (Rectified Linear Unit)

**Formula:**

$$
f(x) = \max(0, x)
$$

### üì¶ Used in:

* Hidden layers of CNNs and feedforward networks
* Very common in deep networks due to simplicity

### ‚úÖ Advantages:

* Fast & easy to compute
* Avoids vanishing gradient (for x > 0)
* Sparse activation ‚Üí improves efficiency

### ‚ö†Ô∏è Disadvantages:

* ‚ÄúDying ReLU‚Äù problem: if input is ‚â§ 0, gradient becomes zero and neuron may stop learning

---

## 2Ô∏è‚É£ Leaky ReLU

**Formula:**

$$
f(x) = 
\begin{cases}
x & x \ge 0 \\
\epsilon x & x < 0
\end{cases}
\quad (\epsilon \text{ is a small value like 0.01})
$$

### üì¶ Used in:

* Hidden layers to fix dying ReLU

### ‚úÖ Advantages:

* Allows small gradient for negative inputs
* Fixes dead neuron issue

### ‚ö†Ô∏è Disadvantages:

* Still not entirely zero-centered
* Value of Œµ must be chosen carefully

---

## 3Ô∏è‚É£ Parametric ReLU (PReLU)

**Like Leaky ReLU, but Œµ is learnable during training**

### üì¶ Used in:

* Deeper architectures where learning flexibility helps (e.g., ResNet variants)

### ‚úÖ Advantages:

* More flexible
* Learns optimal leakiness

### ‚ö†Ô∏è Disadvantages:

* Risk of overfitting with small datasets

---

## 4Ô∏è‚É£ ELU (Exponential Linear Unit)

**Formula:**

$$
f(x) = 
\begin{cases}
x & x \ge 0 \\
\alpha (e^x - 1) & x < 0
\end{cases}
$$

### üì¶ Used in:

* Deep networks (especially where faster convergence is desired)

### ‚úÖ Advantages:

* Smooth gradient for all x
* Avoids dead neuron problem
* Negative values help mean activations closer to zero (faster training)

### ‚ö†Ô∏è Disadvantages:

* More computationally expensive than ReLU

---

## 5Ô∏è‚É£ Swish

**Formula:**

$$
f(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}
$$

### üì¶ Used in:

* Advanced models (e.g., EfficientNet)

### ‚úÖ Advantages:

* Smooth function, good for gradient flow
* Outperforms ReLU in deeper models

### ‚ö†Ô∏è Disadvantages:

* Slower to compute
* Newer ‚Üí less universally supported

---

## 6Ô∏è‚É£ Sigmoid (Logistic Function)

**Formula:**

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

### üì¶ Used in:

* **Output layer** for **binary classification**

### ‚úÖ Advantages:

* Output in range (0, 1), useful for probabilities

### ‚ö†Ô∏è Disadvantages:

* Vanishing gradient
* Not zero-centered
* Expensive computation

---

## 7Ô∏è‚É£ Tanh (Hyperbolic Tangent)

**Formula:**

$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

### üì¶ Used in:

* Hidden layers (especially in RNNs, GRU, LSTM)

### ‚úÖ Advantages:

* Zero-centered
* Better than sigmoid for hidden layers

### ‚ö†Ô∏è Disadvantages:

* Still has vanishing gradient
* More expensive than ReLU

---

## 8Ô∏è‚É£ Softmax

**Formula (for class $i$):**

$$
\sigma(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$

### üì¶ Used in:

* **Output layer** for **multi-class classification**

### ‚úÖ Advantages:

* Outputs probabilities for each class
* Normalized (sum = 1)

### ‚ö†Ô∏è Disadvantages:

* Not good for regression
* Sensitive to large input values (can saturate)

---

### üß† Quick Review Table

| Activation | Use Layer | Use Case                   | Advantage         | Disadvantage    |
| ---------- | --------- | -------------------------- | ----------------- | --------------- |
| ReLU       | Hidden    | CNNs, FFNs                 | Fast, Sparse      | Dying ReLU      |
| Leaky ReLU | Hidden    | CNNs, RNNs                 | Fixes dying ReLU  | Œµ manual        |
| PReLU      | Hidden    | Deep learning              | Learnable Œµ       | Overfit         |
| ELU        | Hidden    | Deep nets                  | Smooth & fast     | Costly          |
| Swish      | Hidden    | Deep nets (new)            | Smooth & flexible | Expensive       |
| Sigmoid    | Output    | Binary classification      | Probabilities     | Vanishing grad  |
| Tanh       | Hidden    | RNNs                       | Centered          | Still vanishing |
| Softmax    | Output    | Multi-class classification | Class probs       | Can saturate    |

---


---

### 1. **ReLU (Rectified Linear Unit)**

* **Formula**: `f(x) = max(0, x)`
* **Use in Layer**: Hidden layers (most commonly)
* **Use Case**: General deep learning models ‚Äî fast and simple.
* **Advantages**:

  * Fast to compute
  * Avoids vanishing gradient (mostly)
* **Disadvantages**:

  * "Dying ReLU" problem: neurons can become inactive
* **Code**:

  ```python
  def relu(x):
      return np.maximum(0, x)
  ```

---

### 2. **Leaky ReLU**

* **Formula**: `f(x) = x if x > 0 else Œ±x`, commonly Œ± = 0.01
* **Use in Layer**: Hidden layers
* **Use Case**: When ReLU causes dead neurons
* **Advantages**:

  * Fixes dying ReLU by allowing small gradient for x < 0
* **Disadvantages**:

  * Slightly more complex, Œ± must be tuned
* **Code**:

  ```python
  def leaky_relu(x, alpha=0.01):
      return np.where(x > 0, x, x * alpha)
  ```

---

### 3. **ELU (Exponential Linear Unit)**

* **Formula**:

  ```
  f(x) = x if x ‚â• 0 else Œ± * (exp(x) - 1)
  ```
* **Use in Layer**: Hidden layers
* **Use Case**: Deep networks, when smooth negative values help
* **Advantages**:

  * Non-zero gradient for x < 0
  * Helps with vanishing gradients
* **Disadvantages**:

  * Computationally slower (uses `exp`)
* **Code**:

  ```python
  def elu(x, alpha=1.0):
      return np.where(x > 0, x, alpha * (np.exp(x) - 1))
  ```

---

### 4. **Sigmoid**

* **Formula**: `œÉ(x) = 1 / (1 + exp(-x))`
* **Use in Layer**: Output layer for **binary classification**
* **Advantages**:

  * Maps input to (0, 1), like probability
* **Disadvantages**:

  * Vanishing gradient
  * Not zero-centered
  * Slower to compute
* **Code**:

  ```python
  def sigmoid(x):
      return 1 / (1 + np.exp(-x))
  ```

---

### 5. **Softmax**

* **Formula**:

  $$
  \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
  $$
* **Use in Layer**: Output layer for **multiclass classification**
* **Advantages**:

  * Outputs probabilities for multiple classes
* **Disadvantages**:

  * Sensitive to outliers (exponentiation)
* **Code**:

  ```python
  def softmax(x):
      e_x = np.exp(x - np.max(x))  # for numerical stability
      return e_x / e_x.sum(axis=0)
  ```

---

### 6. **Swish**

* **Formula**: `f(x) = x * sigmoid(x)`
* **Use in Layer**: Hidden layers
* **Use Case**: Advanced models (e.g., EfficientNet, deeper CNNs)
* **Advantages**:

  * Can outperform ReLU
  * Smooth and non-monotonic
* **Disadvantages**:

  * Slower due to sigmoid
* **Code**:

  ```python
  def swish(x):
      return x * sigmoid(x)
  ```

---

### 7. **Tanh**

* **Formula**:

  $$
  \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
  $$
* **Use in Layer**: Hidden layers (especially in RNNs, LSTM, GRU)
* **Advantages**:

  * Zero-centered
  * Maps to (-1, 1)
* **Disadvantages**:

  * Still has vanishing gradient
* **Code**:

  ```python
  def tanh(x):
      return np.tanh(x)
  ```

---

### 8. **Linear**

* **Formula**: `f(x) = x`
* **Use in Layer**: Output layer for regression problems
* **Advantages**:

  * Best for regression (keeps full range)
* **Disadvantages**:

  * No non-linearity ‚Äî not useful in hidden layers
* **Code**:

  ```python
  def linear(x):
      return x
  ```

---



