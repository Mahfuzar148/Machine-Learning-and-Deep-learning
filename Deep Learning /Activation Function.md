
---

## üîß What Is an Activation Function?

An **activation function** is a mathematical function used in neural networks to decide whether a neuron should be activated (i.e., contribute to the output). It applies to the **weighted sum of inputs + bias**, and its primary role is to introduce **non-linearity** into the model.

---

## üîç Why Are Activation Functions Important?

| Purpose             | Explanation                                                                                 |
| ------------------- | ------------------------------------------------------------------------------------------- |
| **Non-Linearity**   | Without it, a neural network acts like a simple linear model, unable to learn complex data. |
| **Signal Control**  | Helps determine if a neuron should activate or not.                                         |
| **Learn Patterns**  | Enables deep networks to learn from complicated datasets using layers and non-linearities.  |
| **Backpropagation** | Their derivatives are used during backpropagation to adjust weights and minimize loss.      |

---

## üìö Types of Activation Functions

### 1. **Sigmoid (Logistic Function)**

**Formula**:

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

**Properties**:

* Output range: (0, 1)
* Used for binary classification
* Problem: vanishing gradient for large/small inputs

### 2. **Tanh (Hyperbolic Tangent)**

**Formula**:

$$
\sigma(x) = \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

**Properties**:

* Output range: (‚àí1, 1)
* Zero-centered (better for optimization)
* Still can suffer from vanishing gradients

### 3. **ReLU (Rectified Linear Unit)**

**Formula**:

$$
\sigma(x) = \max(0, x)
$$

**Properties**:

* Most widely used in deep learning
* Output: 0 for x < 0, and x for x ‚â• 0
* Solves vanishing gradient to some extent
* Problem: ‚Äúdying ReLU‚Äù (neurons stuck at 0)

### 4. **Softmax**

**Formula**:

$$
\sigma(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
$$

**Properties**:

* Used in output layer for multi-class classification
* Outputs a **probability distribution** (sums to 1)

---

## ‚úÖ Summary Table

| Function | Output Range  | Common Use Case            | Notes                                    |
| -------- | ------------- | -------------------------- | ---------------------------------------- |
| Sigmoid  | (0, 1)        | Binary classification      | Can squash gradients                     |
| Tanh     | (‚àí1, 1)       | Hidden layers (earlier NN) | Zero-centered, better than sigmoid       |
| ReLU     | \[0, ‚àû)       | Most hidden layers         | Fast and effective; watch for dead units |
| Softmax  | (0, 1), sum=1 | Output layer (multi-class) | Gives probabilities for each class       |

---


---

## 1Ô∏è‚É£ ReLU (Rectified Linear Unit)

**Formula:**

$$
f(x) = \max(0, x)
$$

### üì¶ Used in:

* Hidden layers of CNNs and feedforward networks
* Very common in deep networks due to simplicity

### ‚úÖ Advantages:

* Fast & easy to compute
* Avoids vanishing gradient (for x > 0)
* Sparse activation ‚Üí improves efficiency

### ‚ö†Ô∏è Disadvantages:

* ‚ÄúDying ReLU‚Äù problem: if input is ‚â§ 0, gradient becomes zero and neuron may stop learning

---

## 2Ô∏è‚É£ Leaky ReLU

**Formula:**

$$
f(x) = 
\begin{cases}
x & x \ge 0 \\
\epsilon x & x < 0
\end{cases}
\quad (\epsilon \text{ is a small value like 0.01})
$$

### üì¶ Used in:

* Hidden layers to fix dying ReLU

### ‚úÖ Advantages:

* Allows small gradient for negative inputs
* Fixes dead neuron issue

### ‚ö†Ô∏è Disadvantages:

* Still not entirely zero-centered
* Value of Œµ must be chosen carefully

---

## 3Ô∏è‚É£ Parametric ReLU (PReLU)

**Like Leaky ReLU, but Œµ is learnable during training**

### üì¶ Used in:

* Deeper architectures where learning flexibility helps (e.g., ResNet variants)

### ‚úÖ Advantages:

* More flexible
* Learns optimal leakiness

### ‚ö†Ô∏è Disadvantages:

* Risk of overfitting with small datasets

---

## 4Ô∏è‚É£ ELU (Exponential Linear Unit)

**Formula:**

$$
f(x) = 
\begin{cases}
x & x \ge 0 \\
\alpha (e^x - 1) & x < 0
\end{cases}
$$

### üì¶ Used in:

* Deep networks (especially where faster convergence is desired)

### ‚úÖ Advantages:

* Smooth gradient for all x
* Avoids dead neuron problem
* Negative values help mean activations closer to zero (faster training)

### ‚ö†Ô∏è Disadvantages:

* More computationally expensive than ReLU

---

## 5Ô∏è‚É£ Swish

**Formula:**

$$
f(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}
$$

### üì¶ Used in:

* Advanced models (e.g., EfficientNet)

### ‚úÖ Advantages:

* Smooth function, good for gradient flow
* Outperforms ReLU in deeper models

### ‚ö†Ô∏è Disadvantages:

* Slower to compute
* Newer ‚Üí less universally supported

---

## 6Ô∏è‚É£ Sigmoid (Logistic Function)

**Formula:**

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

### üì¶ Used in:

* **Output layer** for **binary classification**

### ‚úÖ Advantages:

* Output in range (0, 1), useful for probabilities

### ‚ö†Ô∏è Disadvantages:

* Vanishing gradient
* Not zero-centered
* Expensive computation

---

## 7Ô∏è‚É£ Tanh (Hyperbolic Tangent)

**Formula:**

$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

### üì¶ Used in:

* Hidden layers (especially in RNNs, GRU, LSTM)

### ‚úÖ Advantages:

* Zero-centered
* Better than sigmoid for hidden layers

### ‚ö†Ô∏è Disadvantages:

* Still has vanishing gradient
* More expensive than ReLU

---

## 8Ô∏è‚É£ Softmax

**Formula (for class $i$):**

$$
\sigma(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
$$

### üì¶ Used in:

* **Output layer** for **multi-class classification**

### ‚úÖ Advantages:

* Outputs probabilities for each class
* Normalized (sum = 1)

### ‚ö†Ô∏è Disadvantages:

* Not good for regression
* Sensitive to large input values (can saturate)

---

### üß† Quick Review Table

| Activation | Use Layer | Use Case                   | Advantage         | Disadvantage    |
| ---------- | --------- | -------------------------- | ----------------- | --------------- |
| ReLU       | Hidden    | CNNs, FFNs                 | Fast, Sparse      | Dying ReLU      |
| Leaky ReLU | Hidden    | CNNs, RNNs                 | Fixes dying ReLU  | Œµ manual        |
| PReLU      | Hidden    | Deep learning              | Learnable Œµ       | Overfit         |
| ELU        | Hidden    | Deep nets                  | Smooth & fast     | Costly          |
| Swish      | Hidden    | Deep nets (new)            | Smooth & flexible | Expensive       |
| Sigmoid    | Output    | Binary classification      | Probabilities     | Vanishing grad  |
| Tanh       | Hidden    | RNNs                       | Centered          | Still vanishing |
| Softmax    | Output    | Multi-class classification | Class probs       | Can saturate    |

---



