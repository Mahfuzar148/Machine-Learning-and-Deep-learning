
---


# ðŸ§  Deep Learning: Neural Network Training  
**(Forward Pass + Backward Pass with Calculations)**

---

## ðŸ“š Table of Contents

1. [ðŸ”¹ Introduction](#1-introduction)  
2. [ðŸ”¹ Neural Network Architecture](#2-neural-network-architecture)  
3. [ðŸ”¹ Forward Propagation](#3-forward-propagation)  
4. [ðŸ”¹ Loss Function](#4-loss-function)  
5. [ðŸ”¹ Backward Propagation](#5-backward-propagation)  
6. [ðŸ”¹ Parameter Update (Gradient Descent)](#6-gradient-descent)  
7. [ðŸ”¹ Numerical Example](#7-numerical-example)  
8. [ðŸ”¹ Summary](#8-summary)

---

## ðŸ”¹ 1. Introduction

A **neural network** learns by minimizing error between predictions and actual outputs using:

- Forward propagation  
- Loss calculation  
- Backpropagation

---

## ðŸ”¹ 2. Neural Network Architecture

We'll use a **simple feedforward neural network**:

- Input Layer â†’ 2 features  
- 1 Hidden Layer â†’ 2 neurons  
- Output Layer â†’ 1 neuron

### Structure:

```

x1     x2
\|      |
\|     \[w11, w12]      (Hidden Layer)
\    /
\[y1, y2] ---> Activation ---> Output Layer

\[w2]   â†’   z

```

---

## ðŸ”¹ 3. Forward Propagation

### Input:

Let:

```

x = \[x1, x2]^T
W1 = \[\[w11, w12],
\[w21, w22]]
b1 = \[b1, b2]^T

```

### Hidden Layer:

```

z1 = W1 \* x + b1
a1 = Ïƒ(z1)        (e.g., sigmoid)

```

### Output Layer:

```

W2 = \[w31, w32], b2 = b3
z2 = W2 \* a1 + b2
Å·  = Ïƒ(z2)         (final output)

```

---

## ðŸ”¹ 4. Loss Function

For binary classification, use **binary cross-entropy**:

```

L(Å·, y) = -\[y \* log(Å·) + (1 - y) \* log(1 - Å·)]

```

---

## ðŸ”¹ 5. Backward Propagation

Using the **chain rule** to compute gradients.

### Output Layer:

```

dL/dÅ· = (Å· - y) / \[Å·(1 - Å·)]
dÅ·/dz2 = Å·(1 - Å·)
â†’ dL/dz2 = Å· - y

dL/dW2 = (Å· - y) \* a1
dL/db2 = Å· - y

```

### Hidden Layer:

Let Î´2 = Å· - y

```

Î´1 = (Î´2 \* W2) \* a1 \* (1 - a1)
dL/dW1 = Î´1 \* x^T
dL/db1 = Î´1

```

---

## ðŸ”¹ 6. Gradient Descent

Using learning rate Î·:

```

W = W - Î· \* dL/dW
b = b - Î· \* dL/db

```

---

## ðŸ”¹ 7. Numerical Example

### Given:

```

x = \[1, 0]^T
W1 = \[\[0.1, 0.2], \[0.3, 0.4]]
b1 = \[0.1, 0.1]^T
W2 = \[0.2, 0.3]
b2 = 0.1
y = 1

```

### Forward Pass:

```

z1 = W1 \* x + b1 = \[0.2, 0.4]
a1 = Ïƒ(z1) â‰ˆ \[0.55, 0.60]
z2 = W2 \* a1 + b2 = 0.39
Å· = Ïƒ(0.39) â‰ˆ 0.596

```

### Loss:

```

L = -\[log(0.596)] â‰ˆ 0.517

```

### Backward Pass:

```

Î´2 = Å· - y = -0.404
dW2 = Î´2 \* a1 = \[-0.222, -0.243]

Î´1 = Î´2 \* W2 \* a1 \* (1 - a1) â‰ˆ \[-0.020, -0.029]
dW1 = Î´1 \* x^T = \[\[-0.020, 0], \[-0.029, 0]]

```

---

## ðŸ”¹ 8. Summary

| Step              | Description                          |
|-------------------|--------------------------------------|
| Forward Pass      | Calculate output from input          |
| Loss              | Compare prediction with target       |
| Backward Pass     | Compute gradients via chain rule     |
| Update Parameters | Adjust weights using gradient descent|

---
```

---


