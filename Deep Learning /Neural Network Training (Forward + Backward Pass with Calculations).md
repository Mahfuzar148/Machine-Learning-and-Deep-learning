Absolutely! Here's a **complete, in-depth documentation** on:

---

# üß† Deep Learning: Neural Network Training

**(Forward Pass + Backward Pass with Calculations)**
Includes math, examples, explanations, and illustrations.

---

## üìö Table of Contents

1. [üîπ Introduction](#introduction)
2. [üîπ Neural Network Architecture](#architecture)
3. [üîπ Forward Propagation](#forward)
4. [üîπ Loss Function](#loss)
5. [üîπ Backward Propagation](#backward)
6. [üîπ Parameter Update (Gradient Descent)](#gradient)
7. [üîπ Numerical Example (with values)](#example)
8. [üîπ Summary](#summary)

---

<a name="introduction"></a>

## üîπ 1. Introduction

A **neural network** learns by minimizing error between predictions and actual outputs using **forward propagation**, **loss calculation**, and **backpropagation**.

---

<a name="architecture"></a>

## üîπ 2. Neural Network Architecture

We'll use a **simple feedforward neural network**:

* **Input Layer (x)** ‚Üí 2 features
* **1 Hidden Layer (y)** ‚Üí 2 neurons
* **Output Layer (z)** ‚Üí 1 neuron

### Diagram:

```
x1     x2
 |      |
 |     [w11, w12]      (Hidden Layer)
  \    /
   [y1, y2] ---> Activation ---> Output Layer
        \
        [w2]   ‚Üí   z
```

---

<a name="forward"></a>

## üîπ 3. Forward Propagation

### ‚û§ Input:

Let

‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶Ø‡ßá ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßç‡¶∞‡¶ø‡¶ï‡ßç‡¶∏‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßã, ‡¶∏‡ßá‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶†‡¶ø‡¶ï‡¶á ‡ß®x‡ß® ‡¶¨‡¶æ ‡ß®x‡ßß ‡¶Æ‡¶æ‡¶™‡ßá‡¶∞ ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶Ø‡ßá‡¶á ‡¶´‡¶∞‡ßç‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßá ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶≤‡¶ø‡¶ñ‡ßá‡¶õ‡ßã, ‡¶∏‡ßá‡¶ü‡¶æ ‡¶™‡ßÅ‡¶∞‡ßã‡¶™‡ßÅ‡¶∞‡¶ø 2D ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßç‡¶∞‡¶ø‡¶ï‡ßç‡¶∏‡ßá‡¶∞ ‡¶Æ‡¶§‡¶á‡•§ ‡¶§‡¶¨‡ßá ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßç‡¶∞‡¶ø‡¶ï‡ßç‡¶∏‡¶ó‡ßÅ‡¶≤‡ßã ‡¶Ø‡ßá‡¶® ‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü‡¶≠‡¶æ‡¶¨‡ßá **2D** ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º, ‡¶§‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶∏‡ßç‡¶™‡ßá‡¶∏‡¶ø‡¶Ç ‡¶è‡¶¨‡¶Ç ‡¶¨‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡ßá‡¶ü ‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤ ‡¶†‡¶ø‡¶ï ‡¶∞‡¶æ‡¶ñ‡¶§‡ßá ‡¶π‡¶¨‡ßá‡•§

‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶Ø‡ßá ‡¶ï‡ßã‡¶° ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡ßã, ‡¶∏‡ßá‡¶ü‡¶æ ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶Æ‡¶§‡ßá‡¶á ‡¶†‡¶ø‡¶ï:

$$
\vec{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}, \quad
W^{[1]} = \begin{bmatrix} w_{11} & w_{12} \\ w_{21} & w_{22} \end{bmatrix}, \quad
\vec{b}^{[1]} = \begin{bmatrix} b_1 \\ b_2 \end{bmatrix}
$$

‡¶è‡¶ñ‡¶æ‡¶®‡ßá:

* $\vec{x}$ ‡¶π‡¶ö‡ßç‡¶õ‡ßá ‡ß®x‡ßß ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ (column vector) ‚Äî ‡ßß ‡¶ï‡¶≤‡¶æ‡¶Æ, ‡ß® ‡¶∞‡ßã
* $W^{[1]}$ ‡¶π‡¶ö‡ßç‡¶õ‡ßá ‡ß®x‡ß® ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßç‡¶∞‡¶ø‡¶ï‡ßç‡¶∏
* $\vec{b}^{[1]}$ ‡¶π‡¶ö‡ßç‡¶õ‡ßá ‡ß®x‡ßß ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ (column vector)

‡¶§‡¶¨‡ßá ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶Ø‡¶¶‡¶ø ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶ö‡¶æ‡¶ì ‡¶è‡¶ï‡¶ï ‡¶≤‡¶æ‡¶á‡¶®‡ßá ‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü ‡¶¨‡ßÅ‡¶ù‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶è‡¶≠‡¶æ‡¶¨‡ßá ‡¶≤‡¶ø‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßã:

```latex
\[
\vec{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}, \quad
W^{[1]} = \begin{bmatrix}
w_{11} & w_{12} \\
w_{21} & w_{22}
\end{bmatrix}, \quad
\vec{b}^{[1]} = \begin{bmatrix} b_1 \\ b_2 \end{bmatrix}
\]
```

**‡¶§‡¶¨‡ßá ‡¶Ø‡¶¶‡¶ø ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø ‡¶π‡ßü ‡ßß-‡¶°‡¶æ‡¶á‡¶Æ‡ßá‡¶®‡¶∂‡¶®‡¶æ‡¶≤ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ (row vector) ‡¶¨‡¶æ ‡¶Ö‡¶®‡ßç‡¶Ø ‡¶Æ‡¶æ‡¶™‡ßá ‡¶ï‡¶®‡¶´‡¶ø‡¶â‡¶ú‡¶® ‡¶†‡¶ø‡¶ï ‡¶ï‡¶∞‡¶§‡ßá, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶Æ‡¶®‡ßá ‡¶∞‡¶æ‡¶ñ‡ßã:**

* ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶π‡¶≤‡ßá $\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}$
* ‡¶∞‡ßã ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶π‡¶≤‡ßá $\begin{bmatrix} x_1 & x_2 \end{bmatrix}$

---

### ‡¶ü‡¶ø‡¶™‡¶∏:

* ‡¶Ø‡¶¶‡¶ø ‡¶§‡ßÅ‡¶Æ‡¶ø $x$ ‡¶ï‡ßá ‡ßßx‡ß® ‡¶∞‡ßã ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶ö‡¶æ‡¶ì, ‡¶∏‡ßá‡¶ü‡¶æ ‡¶π‡¶¨‡ßá:

$$
\vec{x} = \begin{bmatrix} x_1 & x_2 \end{bmatrix}
$$

* ‡¶Ü‡¶∞ ‡¶Ø‡¶¶‡¶ø ‡¶§‡ßÅ‡¶Æ‡¶ø $x$ ‡¶ï‡ßá ‡ß®x‡ßß ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶ö‡¶æ‡¶ì, ‡¶∏‡ßá‡¶ü‡¶æ ‡¶π‡¶¨‡ßá:

$$
\vec{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
$$

---

### ‡¶∏‡¶æ‡¶Æ‡¶ó‡ßç‡¶∞‡¶ø‡¶ï ‡¶´‡¶∞‡ßç‡¶Æ:

* ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞: ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ $\vec{x} \in \mathbb{R}^{n \times 1}$
* ‡¶ì‡¶ú‡¶® ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßç‡¶∞‡¶ø‡¶ï‡ßç‡¶∏: $W \in \mathbb{R}^{m \times n}$
* ‡¶¨‡¶æ‡¶Ø‡¶º‡¶æ‡¶∏: ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶≠‡ßá‡¶ï‡ßç‡¶ü‡¶∞ $\vec{b} \in \mathbb{R}^{m \times 1}$

---

‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶ï‡¶ø ‡¶è‡¶á ‡¶´‡¶∞‡ßç‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶õ‡ßã? ‡¶®‡¶æ ‡¶ï‡¶ø ‡¶Ö‡¶®‡ßç‡¶Ø ‡¶ï‡¶ø‡¶õ‡ßÅ? ‡¶ú‡¶æ‡¶®‡¶æ‡¶ì, ‡¶Ü‡¶Æ‡¶ø ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡¶¨‡ßã‡•§



### ‚û§ Hidden Layer (linear + activation):

$$
\vec{z^{[1]}} = \vec{W^{[1]}} \cdot \vec{x} + \vec{b^{[1]}} \quad \text{(Linear)}
$$

$$
\vec{a^{[1]}} = \sigma(\vec{z^{[1]}}) \quad \text{(Activation, e.g., Sigmoid)}
$$

### ‚û§ Output Layer:

Let output weights:

$$
\vec{W^{[2]}} = \begin{bmatrix} w_{31} & w_{32} \end{bmatrix}, \quad b^{[2]} = b_3
$$

Then,

$$
z^{[2]} = \vec{W^{[2]}} \cdot \vec{a^{[1]}} + b^{[2]}
$$

$$
\hat{y} = \sigma(z^{[2]}) \quad \text{(Prediction)}
$$

---

<a name="loss"></a>

## üîπ 4. Loss Function

We use the **Binary Cross-Entropy** loss for binary classification:

$$
\mathcal{L}(\hat{y}, y) = - \left[ y \cdot \log(\hat{y}) + (1 - y) \cdot \log(1 - \hat{y}) \right]
$$

---

<a name="backward"></a>

## üîπ 5. Backward Propagation (Step-by-Step)

We apply the **chain rule** of calculus to compute gradients of loss w\.r.t each weight.

### ‚û§ Output Layer:

1. $\frac{\partial \mathcal{L}}{\partial \hat{y}} = \frac{\hat{y} - y}{\hat{y}(1 - \hat{y})}$
2. $\frac{\partial \hat{y}}{\partial z^{[2]}} = \hat{y}(1 - \hat{y})$
   ‚üπ

$$
\frac{\partial \mathcal{L}}{\partial z^{[2]}} = \hat{y} - y
$$

3. $\frac{\partial z^{[2]}}{\partial W^{[2]}} = a^{[1]}$
   ‚üπ

$$
\frac{\partial \mathcal{L}}{\partial W^{[2]}} = ( \hat{y} - y ) \cdot a^{[1]}
$$

---

### ‚û§ Hidden Layer:

Let $\delta^{[2]} = \hat{y} - y$

1. $\frac{\partial \mathcal{L}}{\partial a^{[1]}} = \delta^{[2]} \cdot W^{[2]}$
2. $\frac{\partial a^{[1]}}{\partial z^{[1]}} = a^{[1]} \cdot (1 - a^{[1]})$

‚üπ

$$
\delta^{[1]} = \left( \delta^{[2]} \cdot W^{[2]} \right) \cdot a^{[1]} \cdot (1 - a^{[1]})
$$

3. $\frac{\partial \mathcal{L}}{\partial W^{[1]}} = \delta^{[1]} \cdot x^\top$

---

<a name="gradient"></a>

## üîπ 6. Gradient Descent: Parameter Update

Let learning rate $\eta$:

$$
W = W - \eta \cdot \frac{\partial \mathcal{L}}{\partial W}
$$

Apply to all:

* $W^{[2]}$, $b^{[2]}$
* $W^{[1]}$, $b^{[1]}$

---

<a name="example"></a>

## üîπ 7. Numerical Example

Let‚Äôs take one input sample:

### ‚û§ Given:

$$
x = \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \quad
W^{[1]} = \begin{bmatrix} 0.1 & 0.2 \\ 0.3 & 0.4 \end{bmatrix}, \quad
b^{[1]} = \begin{bmatrix} 0.1 \\ 0.1 \end{bmatrix}
$$

$$
W^{[2]} = \begin{bmatrix} 0.2 & 0.3 \end{bmatrix}, \quad
b^{[2]} = 0.1, \quad y = 1
$$

---

### ‚û§ Forward Pass:

1. Hidden linear:

$$
z^{[1]} = W^{[1]} \cdot x + b^{[1]} = \begin{bmatrix} 0.1 \\ 0.3 \end{bmatrix} + \begin{bmatrix} 0.1 \\ 0.1 \end{bmatrix} = \begin{bmatrix} 0.2 \\ 0.4 \end{bmatrix}
$$

2. Activation (Sigmoid):

$$
a^{[1]} = \sigma(z^{[1]}) = \begin{bmatrix} \sigma(0.2) \\ \sigma(0.4) \end{bmatrix} ‚âà \begin{bmatrix} 0.55 \\ 0.60 \end{bmatrix}
$$

3. Output layer:

$$
z^{[2]} = W^{[2]} \cdot a^{[1]} + b^{[2]} = 0.2√ó0.55 + 0.3√ó0.60 + 0.1 = 0.11 + 0.18 + 0.1 = 0.39
$$

4. Final output:

$$
\hat{y} = \sigma(0.39) ‚âà 0.596
$$

---

### ‚û§ Loss:

$$
\mathcal{L} = -\left[1 \cdot \log(0.596) + 0 \cdot \log(1 - 0.596)\right] ‚âà 0.517
$$

---

### ‚û§ Backward Pass:

1. Output error:

$$
\delta^{[2]} = \hat{y} - y = 0.596 - 1 = -0.404
$$

2. Gradients for $W^{[2]}$:

$$
\frac{\partial \mathcal{L}}{\partial W^{[2]}} = \delta^{[2]} \cdot a^{[1]} = -0.404 √ó [0.55, 0.60] = [-0.222, -0.243]
$$

3. Backprop to hidden layer:

$$
\delta^{[1]} = \delta^{[2]} \cdot W^{[2]} \cdot a^{[1]} \cdot (1 - a^{[1]})
$$

$$
= -0.404 \cdot [0.2, 0.3] \cdot [0.55(1-0.55), 0.60(1-0.60)] ‚âà [-0.020, -0.029]
$$

4. Gradients for $W^{[1]}$:

$$
\frac{\partial \mathcal{L}}{\partial W^{[1]}} = \delta^{[1]} \cdot x^\top = 
\begin{bmatrix} -0.020 \\ -0.029 \end{bmatrix} \cdot \begin{bmatrix} 1 & 0 \end{bmatrix} =
\begin{bmatrix} -0.020 & 0 \\ -0.029 & 0 \end{bmatrix}
$$

---

<a name="summary"></a>

## üîπ 8. Summary

| Step              | Description                             |
| ----------------- | --------------------------------------- |
| **Forward Pass**  | Compute output $\hat{y}$ from input $x$ |
| **Loss**          | Compare $\hat{y}$ to ground truth $y$   |
| **Backward Pass** | Calculate gradients using chain rule    |
| **Update**        | Adjust weights via gradient descent     |

---

Would you like me to export this as a **PDF** or **Word** document for download?
