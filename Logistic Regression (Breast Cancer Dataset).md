
---

## ‚úÖ Full PyTorch Code for Logistic Regression (Breast Cancer Dataset)

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Load and prepare the data
x, y = load_breast_cancer(return_X_y=True)
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

# Standardize the features
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# Convert to PyTorch tensors
x_train = torch.tensor(x_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
x_test = torch.tensor(x_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)

# Define the logistic regression model
model = nn.Sequential(
    nn.Linear(x_train.shape[1], 1),
    nn.Sigmoid()
)

# Define the loss function and optimizer
loss_fn = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Train the model
losses = []
for epoch in range(100):
    y_pred = model(x_train)
    loss = loss_fn(y_pred, y_train)
    losses.append(loss.item())

    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

# Evaluate the model
with torch.no_grad():
    train_acc = (model(x_train).round() == y_train).float().mean()
    test_acc = (model(x_test).round() == y_test).float().mean()

print(f"Train Accuracy: {train_acc:.4f}")
print(f"Test Accuracy: {test_acc:.4f}")

# Plot the training loss
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Curve')
plt.grid(True)
plt.show()
```

---



---

## üî¢ ‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶ï‡ßã‡¶°‡ßá‡¶∞ ‡¶≤‡¶æ‡¶á‡¶®-‡¶¨‡¶æ‡¶á-‡¶≤‡¶æ‡¶á‡¶® ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ:

```python
import torch
```

* ‚úÖ `torch`: PyTorch ‡¶≤‡¶æ‡¶á‡¶¨‡ßç‡¶∞‡ßá‡¶∞‡¶ø, ‡¶Ø‡¶æ tensor operation, model design, training ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡ßü‡•§

---

```python
import torch.nn as nn
```

* ‚úÖ `torch.nn`: PyTorch-‡¶è‡¶∞ neural network module‡•§
* `nn` ‡¶π‡¶≤ alias ‚Äî ‡¶è‡¶ñ‡¶® ‡¶Ü‡¶Æ‡¶∞‡¶æ `nn.Linear`, `nn.Sigmoid` ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡•§

---

```python
import torch.optim as optim
```

* ‚úÖ `torch.optim`: Optimization tools (‡¶Ø‡ßá‡¶Æ‡¶® SGD, Adam ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø) ‡¶™‡¶æ‡¶ì‡ßü‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡ßü‡•§

---

```python
from sklearn.datasets import load_breast_cancer
```

* ‚úÖ `load_breast_cancer`: sklearn ‡¶•‡ßá‡¶ï‡ßá breast cancer dataset ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßá‡•§

---

```python
from sklearn.model_selection import train_test_split
```

* ‚úÖ `train_test_split`: ‡¶°‡ßá‡¶ü‡¶æ‡¶ï‡ßá ‡¶ü‡ßç‡¶∞‡ßá‡¶® ‡¶ì ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶∏‡ßá‡¶ü‡ßá ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡•§

---

```python
from sklearn.preprocessing import StandardScaler
```

* ‚úÖ `StandardScaler`: ‡¶∏‡¶¨ feature-‡¶ï‡ßá standardize (mean=0, std=1) ‡¶ï‡¶∞‡ßá‡•§

---

```python
import matplotlib.pyplot as plt
```

* ‚úÖ `matplotlib.pyplot`: ‡¶°‡ßá‡¶ü‡¶æ visualization ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø plotting tool‡•§

---

```python
x, y = load_breast_cancer(return_X_y=True)
```

* `x`: Features (30‡¶ü‡¶æ)
* `y`: Labels (0 ‡¶¨‡¶æ 1 ‚Äî malignant ‡¶¨‡¶æ benign)

---

```python
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)
```

* ‡¶°‡ßá‡¶ü‡¶æ‡¶ï‡ßá ‡ßÆ‡ß¶% train ‡¶ì ‡ß®‡ß¶% test ‡¶∏‡ßá‡¶ü‡ßá ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡ßá‡•§
* `random_state=0`: repeatable result ‡¶™‡ßá‡¶§‡ßá‡•§

---

```python
scaler = StandardScaler()
```

* Scaler object ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶≤‡ßã‡•§

---

```python
x_train = scaler.fit_transform(x_train)
```

* ‚úÖ `fit_transform`: train ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ mean ‡¶ì std ‡¶¶‡¶ø‡ßü‡ßá scale ‡¶ï‡¶∞‡ßá‡•§

---

```python
x_test = scaler.transform(x_test)
```

* ‚úÖ `transform`: ‡¶è‡¶ï‡¶á scaling rule test set ‡¶è apply ‡¶ï‡¶∞‡ßá‡•§

---

```python
x_train = torch.tensor(x_train, dtype=torch.float32)
```

* Numpy array ‚Üí PyTorch Tensor (float32: standard datatype deep learning ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø)

---

```python
y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
```

* `.view(-1, 1)` ‚Üí ‡¶è‡¶ï‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶ø‡¶ï vector ‡¶ï‡ßá 2D (column) tensor ‡¶¨‡¶æ‡¶®‡¶æ‡ßü‡•§
* Binary classification ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø `[n_samples, 1]` shape ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞‡•§

---

```python
x_test = torch.tensor(x_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)
```

* ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø‡¶ì ‡¶è‡¶ï‡¶á ‡¶∞‡¶ï‡¶Æ ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞‡•§

---

```python
model = nn.Sequential(
    nn.Linear(x_train.shape[1], 1),
    nn.Sigmoid()
)
```

* ‚úÖ `nn.Sequential`: ‡¶è‡¶ï‡¶ü‡¶ø simple stack of layers‡•§
* `nn.Linear`: input ‚Üí output (‡¶è‡¶ñ‡¶æ‡¶®‡ßá 30 input ‚Üí 1 output)
* `nn.Sigmoid`: output ‡¶ï‡ßá 0-1 probability ‡¶§‡ßá ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡ßá‡•§

---

```python
loss_fn = nn.BCELoss()
```

* ‚úÖ `BCELoss`: Binary Cross Entropy Loss ‚Äî classification error ‡¶™‡¶∞‡¶ø‡¶Æ‡¶æ‡¶™ ‡¶ï‡¶∞‡ßá‡•§

---

```python
optimizer = optim.SGD(model.parameters(), lr=0.01)
```

* ‚úÖ `SGD`: Stochastic Gradient Descent optimizer‡•§
* `model.parameters()`: model ‡¶è‡¶∞ weight & bias‡•§
* `lr=0.01`: learning rate ‚Üí ‡¶ï‡¶§‡¶ü‡¶æ ‡¶ß‡¶æ‡¶™‡ßá weight ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶π‡¶¨‡ßá‡•§

---

```python
losses = []
```

* ‚úÖ Loss values store ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ñ‡¶æ‡¶≤‡¶ø ‡¶≤‡¶ø‡¶∏‡ßç‡¶ü‡•§

---

```python
for epoch in range(100):
```

* ‡ßß‡ß¶‡ß¶ ‡¶¨‡¶æ‡¶∞ model ‡¶ï‡ßá train ‡¶ï‡¶∞‡¶æ‡¶®‡ßã ‡¶π‡¶¨‡ßá (epochs = full passes over data)

---

```python
    y_pred = model(x_train)
```

* Model input ‡¶®‡ßá‡ßü ‡¶è‡¶¨‡¶Ç output (prediction) ‡¶ï‡¶∞‡ßá‡•§

---

```python
    loss = loss_fn(y_pred, y_train)
```

* Prediction ‡¶Ü‡¶∞ true label ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá error ‡¶Æ‡¶æ‡¶™‡¶æ ‡¶π‡ßü‡•§

---

```python
    losses.append(loss.item())
```

* `.item()` ‚Üí PyTorch tensor ‡¶•‡ßá‡¶ï‡ßá scalar ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶®‡ßá‡ßü‡•§
* `losses` ‡¶≤‡¶ø‡¶∏‡ßç‡¶ü‡ßá ‡¶∏‡ßá‡¶á ‡¶Æ‡¶æ‡¶® ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßá‡•§

---

```python
    loss.backward()
```

* Gradient calculate ‡¶ï‡¶∞‡ßá (backpropagation)‡•§

---

```python
    optimizer.step()
```

* Weight ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡ßá (gradient descent step)‡•§

---

```python
    optimizer.zero_grad()
```

* ‡¶Ü‡¶ó‡ßá‡¶∞ gradient clear ‡¶ï‡¶∞‡ßá, ‡¶Ø‡¶æ‡¶§‡ßá accumulate ‡¶®‡¶æ ‡¶π‡ßü‡•§

---

```python
with torch.no_grad():
```

* ‡¶è‡¶á block ‡¶è‡¶∞ ‡¶≠‡ßá‡¶§‡¶∞‡ßá gradient tracking ‡¶¨‡¶®‡ßç‡¶ß ‡¶•‡¶æ‡¶ï‡ßá ‚Üí faster evaluation‡•§

---

```python
    train_acc = (model(x_train).round() == y_train).float().mean()
```

* Model prediction ‡¶ï‡ßá 0/1 ‡¶§‡ßá ‡¶∞‡¶æ‡¶â‡¶®‡ßç‡¶° ‡¶ï‡¶∞‡ßá‡•§
* ‡¶∏‡¶†‡¶ø‡¶ï prediction ‡¶è‡¶∞ ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ‡¶∞ ‡¶ó‡ßú ‡¶π‡¶ø‡¶∏‡ßá‡¶¨ ‡¶ï‡¶∞‡ßá ‚Üí accuracy‡•§

---

```python
    test_acc = (model(x_test).round() == y_test).float().mean()
```

* Test dataset ‡¶è‡¶∞ accuracy ‡¶π‡¶ø‡¶∏‡¶æ‡¶¨ ‡¶ï‡¶∞‡ßá‡•§

---

```python
print(f"Train Accuracy: {train_acc:.4f}")
print(f"Test Accuracy: {test_acc:.4f}")
```

* Accuracy print ‡¶ï‡¶∞‡ßá ‡ß™ ‡¶¶‡¶∂‡¶Æ‡¶ø‡¶ï ‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§‡•§

---

```python
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.grid(True)
plt.show()
```

* Loss curve plot ‡¶ï‡¶∞‡ßá ‚Üí ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶ï‡ßá‡¶Æ‡¶® ‡¶ï‡¶∞‡ßá loss ‡¶ï‡¶Æ‡ßá‡¶õ‡ßá‡•§

---

## üìò ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ PyTorch ‡¶ï‡¶ø‡¶ì‡¶Ø‡¶º‡¶æ‡¶∞‡ßç‡¶° ‡¶ì ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ:

| ‡¶ï‡¶ø‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶°               | ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ                                    |
| ---------------------- | ------------------------------------------- |
| `torch.tensor()`       | NumPy ‚Üí PyTorch ‡¶ü‡ßá‡¶®‡ßç‡¶∏‡¶∞ ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡ßá         |
| `nn.Linear`            | ‡¶è‡¶ï‡¶ü‡¶ø ‡¶≤‡¶ø‡¶®‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶≤‡ßá‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá (Wx + b)      |
| `nn.Sigmoid()`         | Output ‡¶ï‡ßá 0-1 ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶∏‡ßç‡¶ï‡ßá‡¶≤ ‡¶ï‡¶∞‡ßá            |
| `nn.BCELoss()`         | Binary classification ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø loss function |
| `optimizer.step()`     | Weights ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡ßá                           |
| `loss.backward()`      | Gradient ‡¶ó‡ßÅ‡¶≤‡ßã calculate ‡¶ï‡¶∞‡ßá                 |
| `zero_grad()`          | ‡¶Ü‡¶ó‡ßá‡¶∞ gradient ‡¶Æ‡ßÅ‡¶õ‡ßá ‡¶´‡ßá‡¶≤‡ßá                     |
| `with torch.no_grad()` | Evaluation ‡¶è‡¶∞ ‡¶∏‡¶Æ‡ßü gradient ‡¶π‡¶ø‡¶∏‡¶æ‡¶¨ ‡¶®‡¶æ ‡¶ï‡¶∞‡ßá     |
| `.round()`             | Probabilities ‚Üí 0 ‡¶¨‡¶æ 1 (threshold 0.5)      |

---

