
---

## тЬЕ Full Code: PyTorch Linear Regression Example

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sklearn import datasets
import matplotlib.pyplot as plt

# 1. Generate synthetic regression dataset
x_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=10, random_state=1)

# 2. Convert numpy arrays to PyTorch tensors
x = torch.from_numpy(x_numpy.astype(np.float32))
y = torch.from_numpy(y_numpy.astype(np.float32))
y = y.view(y.shape[0], 1)  # reshape to (100, 1)

# 3. Model definition
model = nn.Linear(in_features=1, out_features=1)

# 4. Define Loss Function and Optimizer
loss_fn = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 5. Training Loop
num_epochs = 1000
for epoch in range(num_epochs):
    # Forward pass
    y_pred = model(x)

    # Compute loss
    loss = loss_fn(y_pred, y)

    # Backward pass
    loss.backward()

    # Update weights
    optimizer.step()

    # Zero gradients
    optimizer.zero_grad()

    # Print every 100 epochs
    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# 6. Visualize the regression line
predicted = model(x).detach().numpy()
plt.plot(x_numpy, y_numpy, 'ro', label='Original data')
plt.plot(x_numpy, predicted, label='Fitted line')
plt.legend()
plt.show()
```

---

## ЁЯза Step-by-Step Explanation in Bengali

---

### ЁЯФв **1. ржбрзЗржЯрж╛рж╕рзЗржЯ рждрзИрж░рж┐ ржХрж░рж╛**

```python
x_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=10, random_state=1)
```

тЬЕ ржПржЯрж╛ `scikit-learn` рж▓рж╛ржЗржмрзНрж░рзЗрж░рж┐рж░ `make_regression()` ржлрж╛ржВрж╢ржи, ржпрж╛ ржХрзГрждрзНрж░рж┐ржо ржбрзЗржЯрж╛ рждрзИрж░рж┐ ржХрж░рзЗред

* `n_samples=100`: рззрзжрзжржЯрж┐ ржбрзЗржЯрж╛ ржкрзЯрзЗржирзНржЯ рждрзИрж░рж┐ ржХрж░рзЗред
* `n_features=1`: ржкрзНрж░рждрж┐ ржбрзЗржЯрж╛ ржкрзЯрзЗржирзНржЯрзЗ рззржЯрж┐ ржлрж┐ржЪрж╛рж░ ржерж╛ржХржмрзЗред
* `noise=10`: рж░тАНрзНржпрж╛ржирзНржбржо рж╢ржмрзНржж ржпрзЛржЧ ржХрж░рзЗ ржмрж╛рж╕рзНрждржмрзЗрж░ ржорждрзЛ ржХрж░рзЗ рждрзЛрж▓рзЗред

---

### ЁЯФБ **2. PyTorch ржЯрзЗржирзНрж╕рж░рзЗ рж░рзВржкрж╛ржирзНрждрж░**

```python
x = torch.from_numpy(x_numpy.astype(np.float32))
y = torch.from_numpy(y_numpy.astype(np.float32))
y = y.view(y.shape[0], 1)
```

тЬЕ NumPy array тЖТ PyTorch ржЯрзЗржирзНрж╕рж░рзЗ рж░рзВржкрж╛ржирзНрждрж░ ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗред

ЁЯФД `y.view(...)` ржжрж┐рзЯрзЗ `y` ржПрж░ рж╢рзЗржк ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗ `[100, 1]`, ржпрзЗржи ржоржбрзЗрж▓рзЗрж░ ржЖржЙржЯржкрзБржЯрзЗрж░ рж╕ржЩрзНржЧрзЗ ржорзЗрж▓рзЗред

---

### ЁЯПЧя╕П **3. ржоржбрзЗрж▓ ржбрж┐ржлрж╛ржЗржи ржХрж░рж╛**

```python
model = nn.Linear(in_features=1, out_features=1)
```

тЬЕ `nn.Linear()` ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржПржХржЯрж┐ рж╕рзЛржЬрж╛ рж▓рж┐ржирж┐рзЯрж╛рж░ рж░рж┐ржЧрзНрж░рзЗрж╢ржи ржоржбрзЗрж▓ рждрзИрж░рж┐ ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗ:
$\hat{y} = w \cdot x + b$

ржПржЦрж╛ржирзЗ:

* `in_features=1`: ржЗржиржкрзБржЯ ржПржХржорж╛рждрзНрж░рж╛ред
* `out_features=1`: ржЖржЙржЯржкрзБржЯ ржПржХржорж╛рждрзНрж░рж╛ред

---

### ЁЯзо **4. рж▓рж╕ ржлрж╛ржВрж╢ржи ржПржмржВ ржЕржкржЯрж┐ржорж╛ржЗржЬрж╛рж░**

```python
loss_fn = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)
```

* **Loss Function:**

  ```python
  loss_fn = nn.MSELoss()
  ```

  ЁЯУП Mean Squared Error ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗред
  $\text{Loss} = \frac{1}{n} \sum (y - \hat{y})^2$
  ржПржЯрж┐ predicted ржУ actual ржорж╛ржирзЗрж░ ржкрж╛рж░рзНржержХрзНржп ржкрж░рж┐ржорж╛ржк ржХрж░рзЗред

* **Optimizer:**

  ```python
  optimizer = optim.SGD(model.parameters(), lr=0.01)
  ```

  ЁЯФБ Stochastic Gradient Descent (SGD) ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржУржЬржи ржЖржкржбрзЗржЯ ржХрж░рж╛ рж╣ржмрзЗред

---

### ЁЯФБ **5. Training Loop**

```python
for epoch in range(num_epochs):
    y_pred = model(x)
    loss = loss_fn(y_pred, y)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
```

тП▒я╕П **ржкрзНрж░рждрж┐ epoch-ржП ржпрж╛ рж╣ржЪрзНржЫрзЗ:**

1. **Forward Pass:** ржоржбрзЗрж▓ ржжрж┐рзЯрзЗ ржкрзНрж░рзЗржбрж┐ржХрж╢ржи ржХрж░рж╛ рж╣ржЪрзНржЫрзЗред
2. **Loss Calculation:** `MSELoss` ржжрж┐рзЯрзЗ error ржорж╛ржкрж╛ рж╣ржЪрзНржЫрзЗред
3. **Backward Pass:** `.backward()` ржжрж┐рзЯрзЗ ржЧрзНрж░рзНржпрж╛ржбрж┐рзЯрзЗржирзНржЯ рж╣рж┐рж╕рж╛ржм ржХрж░рж╛ рж╣ржЪрзНржЫрзЗред
4. **Weight Update:** `optimizer.step()` ржжрж┐рзЯрзЗ ржУржЬржи ржЖржкржбрзЗржЯ рж╣ржЪрзНржЫрзЗред
5. **Gradient Reset:** `optimizer.zero_grad()` ржжрж┐рзЯрзЗ ржкрзБрж░ржирзЛ ржЧрзНрж░рзНржпрж╛ржбрж┐рзЯрзЗржирзНржЯ ржорзБржЫрзЗ ржлрзЗрж▓рж╛ рж╣ржЪрзНржЫрзЗред

ЁЯУд ржкрзНрж░рждрж┐ рззрзжрзж epoch ржкрж░ржкрж░ ржкрзНрж░рж┐ржирзНржЯ:

```python
if (epoch + 1) % 100 == 0:
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
```

---

### ЁЯУК **6. ржлрж╛ржЗржирж╛рж▓ ржлрж▓рж╛ржлрж▓ ржнрж┐ржЬрзНржпрзБрзЯрж╛рж▓рж╛ржЗржЬ ржХрж░рж╛**

```python
predicted = model(x).detach().numpy()
plt.plot(x_numpy, y_numpy, 'ro', label='Original data')
plt.plot(x_numpy, predicted, label='Fitted line')
plt.legend()
plt.show()
```

* ржоржбрзЗрж▓рзЗрж░ ржкрзНрж░рзЗржбрж┐ржХрж╢ржи `.detach().numpy()` ржХрж░рзЗ NumPy array ржмрж╛ржирж╛ржирзЛ рж╣рзЯрзЗржЫрзЗред
* `matplotlib` ржжрж┐рзЯрзЗ ржЖрж╕рж▓ ржбрзЗржЯрж╛ ржУ ржкрзНрж░рзЗржбрж┐ржХрзНржЯ ржХрж░рж╛ рж░рзЗржЦрж╛ ржкрзНрж▓ржЯ ржХрж░рж╛ рж╣рзЯрзЗржЫрзЗред

---

## тЬЕ ржПржЗ ржХрзЛржб ржжрж┐рзЯрзЗ ржЖржкржирж┐ ржХрзА рж╢рж┐ржЦрж▓рзЗржи?

| ржмрж┐рж╖рзЯ                | ржмрж░рзНржгржирж╛                           |
| ------------------- | -------------------------------- |
| Dataset Preparation | `make_regression` ржжрж┐рзЯрзЗ ржбрзЗржЯрж╛ рждрзИрж░рж┐ |
| Model Building      | `nn.Linear` ржжрж┐рзЯрзЗ рж░рж┐ржЧрзНрж░рзЗрж╢ржи ржоржбрзЗрж▓   |
| Loss Function       | `nn.MSELoss` ржжрж┐рзЯрзЗ ржнрзБрж▓ рж╣рж┐рж╕рж╛ржм      |
| Optimizer           | `SGD` ржжрж┐рзЯрзЗ ржУржЬржи ржЖржкржбрзЗржЯ             |
| Training            | Manual forward + backward pass   |
| Visualization       | ржоржбрзЗрж▓рзЗрж░ рж╢рзЗржЦрж╛ рж░рзЗржЦрж╛ ржжрзЗржЦрж╛ржирзЛ рж╣рзЯрзЗржЫрзЗ    |

---


