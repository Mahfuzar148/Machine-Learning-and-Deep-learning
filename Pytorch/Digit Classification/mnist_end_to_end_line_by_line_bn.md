‡¶¨‡ßÅ‡¶ù‡ßá‡¶õ‡¶ø üôÇ ‚Äî ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶Ø‡ßá‡¶Æ‡¶® **training loop**‚Äì‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶≤‡¶æ‡¶á‡¶®‡ßá‚Äì‡¶≤‡¶æ‡¶á‡¶®‡ßá, ‡¶ñ‡ßÅ‡¶Å‡¶ü‡¶ø‡¶®‡¶æ‡¶ü‡¶ø ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶ö‡ßá‡ßü‡ßá‡¶õ‡¶ø‡¶≤‡ßá, **‡¶è‡¶ï‡¶á ‡¶∏‡ßç‡¶ü‡¶æ‡¶á‡¶≤‡ßá ‡¶™‡ßÅ‡¶∞‡ßã ‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡¶ø‡¶™‡ßç‡¶ü‡ßá‡¶∞ ‡¶∏‡¶¨ ‡¶¨‡ßç‡¶≤‡¶ï** (‡ßß‚Äì‡ßÆ) ‡¶∏‡¶æ‡¶ú‡¶ø‡ßü‡ßá ‡¶¶‡¶ø‡¶≤‡¶æ‡¶Æ‡•§
‡¶è‡¶ü‡¶æ ‡¶ï‡¶™‡¶ø ‡¶ï‡¶∞‡ßá GitHub-‡¶è ‡¶∞‡¶æ‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßã‡•§

> **GitHub ‡¶´‡¶æ‡¶á‡¶≤ ‡¶®‡¶æ‡¶Æ (‡¶∏‡¶æ‡¶ú‡ßá‡¶∏‡ßç‡¶ü‡ßá‡¶°):** `docs/mnist_end_to_end_line_by_line_bn.md`

---

# 1) Imports ‚Äî ‡¶≤‡¶æ‡¶á‡¶®‡ßá‚Äì‡¶≤‡¶æ‡¶á‡¶®‡ßá

```
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
```

* `matplotlib.pyplot as plt` ‚Üí ‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü/‡¶á‡¶Æ‡ßá‡¶ú ‡¶™‡ßç‡¶≤‡¶ü ‡¶ï‡¶∞‡¶§‡ßá‡•§
* `torch` ‚Üí ‡¶ü‡ßá‡¶®‡¶∏‡¶∞, ‡¶Ö‡¶ü‡ßã-‡¶ó‡ßç‡¶∞‡¶æ‡¶°, ‡¶°‡¶ø‡¶≠‡¶æ‡¶á‡¶∏ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßá‡¶ú‡¶Æ‡ßá‡¶®‡ßç‡¶ü‡•§
* `torch.nn as nn` ‚Üí ‡¶≤‡ßá‡ßü‡¶æ‡¶∞/‡¶Æ‡¶°‡ßá‡¶≤/‡¶≤‡¶∏ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡•§
* `torch.optim as optim` ‚Üí Adam/SGD ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø ‡¶Ö‡¶™‡ßç‡¶ü‡¶ø‡¶Æ‡¶æ‡¶á‡¶ú‡¶æ‡¶∞‡•§
* `DataLoader` ‚Üí ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ö‡ßá ‡¶°‡ßá‡¶ü‡¶æ ‡¶≤‡ßã‡¶°‡•§
* `datasets`, `transforms` ‚Üí ‡¶∞‡ßá‡¶°‡¶ø‡¶Æ‡ßá‡¶° ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü + ‡¶™‡ßç‡¶∞‡¶ø-‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç‡•§

---

# 2) Data Preprocessing ‚Äî ‡¶≤‡¶æ‡¶á‡¶®‡ßá‚Äì‡¶≤‡¶æ‡¶á‡¶®‡ßá

```
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])
```

* `Compose([...])` ‚Üí ‡¶ï‡ßü‡ßá‡¶ï‡¶ü‡¶æ ‡¶ü‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶∏‡¶´‡¶∞‡ßç‡¶Æ ‡¶ï‡ßç‡¶∞‡¶Æ‡¶æ‡¶®‡ßÅ‡¶∏‡¶æ‡¶∞‡ßá ‡¶ö‡¶æ‡¶≤‡¶æ‡ßü‡•§
* `ToTensor()` ‚Üí PIL‚ÜíTensor, ‡¶™‡¶ø‡¶ï‡ßç‡¶∏‡ßá‡¶≤ `[0,1]` ‡¶∏‡ßç‡¶ï‡ßá‡¶≤‡ßá‡•§
* `Normalize(mean,std)` ‚Üí `(x-mean)/std`; **MNIST mean=0.1307, std=0.3081**‡•§

```
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)
```

* `root='./data'` ‚Üí ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶°‡ßá‡¶ü‡¶æ ‡¶•‡¶æ‡¶ï‡¶¨‡ßá‡•§
* `train=True/False` ‚Üí ‡¶ü‡ßç‡¶∞‡ßá‡¶®/‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶∏‡ßç‡¶™‡ßç‡¶≤‡¶ø‡¶ü‡•§
* `download=True` ‚Üí ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶®‡¶æ‡¶Æ‡¶æ‡ßü‡•§
* `transform` ‚Üí ‡¶â‡¶™‡¶∞‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶ø-‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™‡ßç‡¶≤‡¶æ‡¶á‡•§

```
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader  = DataLoader(test_dataset,  batch_size=1000, shuffle=False)
```

* `batch_size=64` (train) ‚Üí ‡¶∏‡ßç‡¶•‡¶ø‡¶∞ ‡¶ó‡ßç‡¶∞‡ßá‡¶°‡¶ø‡ßü‡ßá‡¶®‡ßç‡¶ü + ‡¶≠‡¶æ‡¶≤‡ßã ‡¶∏‡ßç‡¶™‡¶ø‡¶°‡•§
* `shuffle=True` (train) ‚Üí ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡¶æ‡¶≤‡¶æ‡¶á‡¶ú‡ßá‡¶∂‡¶® ‡¶≠‡¶æ‡¶≤‡ßã‡•§
* `batch_size=1000` (test) ‚Üí ‡¶á‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ‡ßü‡ßá‡¶∂‡¶® ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§; `shuffle=False` ‡¶∞‡¶æ‡¶ñ‡ßá‡•§

> **‡¶ü‡¶ø‡¶â‡¶®‡¶ø‡¶Ç ‡¶ü‡¶ø‡¶™‡¶∏:** ‡¶¨‡ßú GPU ‡¶π‡¶≤‡ßá train `batch_size` ‡¶¨‡¶æ‡ßú‡¶æ‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßã; Windows/Notebook ‡¶è `num_workers=0/2`, Linux/Colab ‡¶è `num_workers=2‚Äì8`, CUDA ‡¶π‡¶≤‡ßá `pin_memory=True`‡•§

---

# 3) Model ‚Äî ‡¶≤‡¶æ‡¶á‡¶®‡ßá‚Äì‡¶≤‡¶æ‡¶á‡¶®‡ßá

```
class DigitClassifier(nn.Module):
    def __init__(self):
        super(DigitClassifier, self).__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(28*28, 128)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)  # 10 classes for digits 0‚Äì9
```

* `nn.Module` ‡¶∏‡¶æ‡¶¨‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ ‚Üí ‡¶ï‡¶æ‡¶∏‡ßç‡¶ü‡¶Æ ‡¶Æ‡¶°‡ßá‡¶≤‡•§
* `Flatten()` ‚Üí `[N,1,28,28]` ‚Üí `[N,784]`‡•§
* `Linear(784,128)` ‚Üí ‡¶™‡ßç‡¶∞‡¶•‡¶Æ FC; 28√ó28=784 **‡¶π‡¶æ‡¶∞‡ßç‡¶° ‡¶∞‡ßÅ‡¶≤: Flatten ‡¶ï‡¶∞‡¶æ ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ = `in_features`**‡•§
* `ReLU()` ‚Üí ‡¶®‡¶®-‡¶≤‡¶ø‡¶®‡¶ø‡ßü‡¶æ‡¶∞‡¶ø‡¶ü‡¶ø; ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§/‡¶∏‡ßç‡¶ü‡ßá‡¶¨‡¶≤‡•§
* `Linear(128,64)`, `Linear(64,10)` ‚Üí ‡¶ß‡¶æ‡¶™‡ßá ‡¶ß‡¶æ‡¶™‡ßá ‡¶´‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶ï‡¶Æ‡ßç‡¶™‡ßç‡¶∞‡ßá‡¶∏ ‚Üí ‡ßß‡ß¶ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ logits‡•§

```
    def forward(self, x):
        x = self.flatten(x)        # [N,784]
        x = self.relu(self.fc1(x)) # [N,128]
        x = self.relu(self.fc2(x)) # [N,64]
        x = self.fc3(x)            # [N,10] (logits)
        return x  # CrossEntropyLoss ‡¶®‡¶ø‡¶ú‡ßá‡¶á softmax ‡¶®‡ßá‡¶¨‡ßá
```

* **logits** ‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶®; ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ Softmax ‡¶¶‡ßá‡¶¨‡ßá ‡¶®‡¶æ (CrossEntropyLoss ‡¶®‡¶ø‡¶ú‡ßá‡¶∞‡¶æ ‡¶®‡ßá‡ßü)‡•§

> **‡¶≠‡ßç‡¶Ø‡¶æ‡¶∞‡¶ø‡ßü‡ßá‡¶∂‡¶®:** ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶∏‡¶æ‡¶á‡¶ú ‡¶¨‡¶¶‡¶≤‡¶æ‡¶≤‡ßá `784` ‡¶¨‡¶¶‡¶≤‡¶æ‡¶§‡ßá ‡¶π‡¶¨‡ßá, ‡¶®‡¶á‡¶≤‡ßá `LazyLinear`/CNN+GlobalPool ‡¶®‡¶æ‡¶ì‡•§

---

# 4) Device Setup ‚Äî ‡¶≤‡¶æ‡¶á‡¶®‡ßá‚Äì‡¶≤‡¶æ‡¶á‡¶®‡ßá

```
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

* GPU ‡¶•‡¶æ‡¶ï‡¶≤‡ßá `"cuda"`, ‡¶®‡¶á‡¶≤‡ßá `"cpu"`‡•§
* **Rule:** model, data‚Äî‡¶è‡¶ï‡¶á device‚Äì‡¶è ‡¶•‡¶æ‡¶ï‡¶§‡ßá ‡¶π‡¶¨‡ßá (‡¶®‡¶æ ‡¶π‡¶≤‡ßá RuntimeError)‡•§

---

# 5) Model, Loss, Optimizer ‚Äî ‡¶≤‡¶æ‡¶á‡¶®‡ßá‚Äì‡¶≤‡¶æ‡¶á‡¶®‡ßá

```
model = DigitClassifier().to(device)
```

* ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶°‡¶ø‡¶≠‡¶æ‡¶á‡¶∏‡ßá (GPU/CPU) ‡¶™‡¶æ‡¶†‡¶æ‡¶≤‡¶æ‡¶Æ‡•§

```
criterion = nn.CrossEntropyLoss()
```

* ‡¶Æ‡¶æ‡¶≤‡ßç‡¶ü‡¶ø-‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶∂‡¶®; **input=logits `[N,C]`**, **target=int `[N]`**‡•§
* ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ softmax **‡¶¶‡ßá‡¶¨‡ßá ‡¶®‡¶æ**‡•§

```
optimizer = optim.Adam(model.parameters(), lr=0.001)
```

* Adam: ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§ ‡¶ï‡¶®‡¶≠‡¶æ‡¶∞‡ßç‡¶ú; `lr=1e-3` ‡¶≠‡¶æ‡¶≤‡ßã ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡ßç‡¶ü‡•§
* ‡¶¨‡ßú `lr` ‚Üí ‡¶Ö‡¶∏‡ßç‡¶•‡¶ø‡¶∞/NaN; ‡¶õ‡ßã‡¶ü `lr` ‚Üí ‡¶∏‡ßç‡¶≤‡ßã‡•§

> **‡¶™‡ßç‡¶∞‡ßã ‡¶ü‡¶ø‡¶™‡¶∏:** `AdamW` + `weight_decay=1e-4` ‡¶Ü‡¶ß‡ßÅ‡¶®‡¶ø‡¶ï ‡¶¨‡ßá‡¶∏‡ßç‡¶ü-‡¶™‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡¶ü‡¶ø‡¶∏; LR scheduler (Cosine/ReduceLROnPlateau) ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶≤‡ßá ‡¶ü‡¶ø‡¶â‡¶®‡¶ø‡¶Ç ‡¶∏‡¶π‡¶ú ‡¶π‡ßü‡•§

---

# 6) Training Function ‚Äî ‡¶≤‡¶æ‡¶á‡¶®‡ßá‚Äì‡¶≤‡¶æ‡¶á‡¶®‡ßá

```
def train(model, device, train_loader, optimizer, criterion, epochs=5):
    model.train()
```

* **train mode**: Dropout on, BN ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ö-‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶ü‡¶∏ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü‡•§

```
    for epoch in range(epochs):
        running_loss = 0.0
```

* ‡¶á‡¶™‡¶ï ‡¶≤‡ßÅ‡¶™ + ‡¶≤‡¶∏ ‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü‡¶æ‡¶∞‡•§

```
        for batch_idx, (data, targets) in enumerate(train_loader):
            data, targets = data.to(device), targets.to(device)
```

* ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ö‡ßá ‡¶°‡ßá‡¶ü‡¶æ/‡¶≤‡ßá‡¶¨‡ßá‡¶≤; device‚Äì‡¶è ‡¶™‡¶æ‡¶†‡¶æ‡¶≤‡¶æ‡¶Æ‡•§ (‡¶Æ‡¶ø‡¶∏‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ö=RuntimeError)

```
            optimizer.zero_grad()
            outputs = model(data)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
```

* **‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡ß© ‡¶ß‡¶æ‡¶™**: zero\_grad ‚Üí backward ‚Üí step
* `outputs` logits `[N,10]`; `targets` int `[N]`‡•§

```
            running_loss += loss.item()
            if batch_idx % 100 == 0:
                print(f"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}")
```

* ‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡ßß‡ß¶‡ß¶ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ö‡ßá ‡¶™‡ßç‡¶∞‡¶ó‡ßç‡¶∞‡ßá‡¶∏ ‡¶≤‡¶ó (‡¶°‡ßá‡¶ü‡¶æ ‡¶õ‡ßã‡¶ü ‡¶π‡¶≤‡ßá 20/50 ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡ßü)‡•§

```
        avg_loss = running_loss / len(train_loader)
        print(f"Epoch {epoch+1} finished. Average Loss: {avg_loss:.4f}")
```

* ‡¶á‡¶™‡¶ï‡ßá‡¶∞ ‡¶ó‡ßú ‡¶ü‡ßç‡¶∞‡ßá‡¶® ‡¶≤‡¶∏‚Äî‡¶ï‡¶Æ‡¶§‡ßá ‡¶•‡¶æ‡¶ï‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶∏‡¶Ç‡¶ï‡ßá‡¶§‡•§

> **‡¶∞‡¶ø‡¶ï‡¶Æ‡ßá‡¶®‡ßç‡¶°‡ßá‡¶° ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶°-‡¶Ö‡¶®‡¶∏:** AMP (mixed precision), gradient clipping, LR scheduler, validation loop, early stopping‡•§

---

# 7) Testing Function ‚Äî ‡¶≤‡¶æ‡¶á‡¶®‡ßá‚Äì‡¶≤‡¶æ‡¶á‡¶®‡ßá

```
def test(model, device, test_loader):
    model.eval()
    correct = 0
    total = 0
```

* eval mode: Dropout off, BN ‡¶∏‡ßç‡¶•‡¶ø‡¶∞‡•§
* accuracy ‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü‡¶æ‡¶∞‡•§

```
    with torch.no_grad():
        for data, targets in test_loader:
            data, targets = data.to(device), targets.to(device)
            outputs = model(data)
            _, predicted = torch.max(outputs, 1)
            total += targets.size(0)
            correct += (predicted == targets).sum().item()
```

* `no_grad` ‚Üí ‡¶ó‡ßç‡¶∞‡ßá‡¶°‡¶ø‡ßü‡ßá‡¶®‡ßç‡¶ü ‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤‡¶ï‡ßÅ‡¶≤‡ßá‡¶∂‡¶® ‡¶¨‡¶®‡ßç‡¶ß (‡¶Æ‡ßá‡¶Æ‡¶∞‡¶ø/‡¶∏‡¶Æ‡ßü ‡¶∏‡¶æ‡¶∂‡ßç‡¶∞‡ßü)‡•§
* `argmax(dim=1)` ‚Üí predicted class‡•§
* `total/correct` ‡¶Ü‡¶™‡¶°‡ßá‡¶ü‡•§

```
    accuracy = 100 * correct / total
    print(f"Test Accuracy: {accuracy:.2f}%")
    return accuracy
```

* % ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ï‡ßÅ‡¶∞‡ßá‡¶∏‡¶ø ‡¶∞‡¶ø‡¶ü‡¶æ‡¶∞‡ßç‡¶®/‡¶™‡ßç‡¶∞‡¶ø‡¶®‡ßç‡¶ü‡•§

---

# 8) Prediction Visualization ‚Äî ‡¶≤‡¶æ‡¶á‡¶®‡ßá‚Äì‡¶≤‡¶æ‡¶á‡¶®‡ßá

```
def visualize_predictions(model, device, test_loader, n=6):
    model.eval()
    data_iter = iter(test_loader)
    images, labels = next(data_iter)
    images, labels = images.to(device), labels.to(device)
```

* ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ö; device‚Äì‡¶è ‡¶™‡¶æ‡¶†‡¶æ‡¶®‡ßã‡•§
* `n` ‡¶õ‡¶¨‡¶ø‡¶∞ ‡¶≠‡¶ø‡¶ú‡ßÅ‡ßü‡¶æ‡¶≤ (‡¶¨‡ßç‡¶Ø‡¶æ‡¶ö‡ßá `n`‚Äì‡¶è‡¶∞ ‡¶ï‡¶Æ ‡¶π‡¶≤‡ßá ‡¶Ü‡¶ó‡ßá `n=min(n, images.size(0))` ‡¶ï‡¶∞‡ßã)‡•§

```
    outputs = model(images)
    _, preds = torch.max(outputs, 1)
```

* logits‚Üípreds (‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ ‡¶á‡¶®‡¶°‡ßá‡¶ï‡ßç‡¶∏)‡•§

```
    images = images.cpu()
    labels = labels.cpu()
    preds = preds.cpu()
```

* matplotlib CPU ‡¶ü‡ßá‡¶®‡¶∏‡¶∞ ‡¶≤‡¶æ‡¶ó‡ßá, ‡¶§‡¶æ‡¶á CPU-‡¶§‡ßá ‡¶Ü‡¶®‡¶æ‡•§

```
    plt.figure(figsize=(12, 4))
    for i in range(n):
        plt.subplot(1, n, i+1)
        plt.imshow(images[i].squeeze(), cmap='gray')
        plt.title(f"True: {labels[i]}\nPred: {preds[i]}")
        plt.axis('off')
    plt.tight_layout()
    plt.show()
```

* 1√ón ‡¶ó‡ßç‡¶∞‡¶ø‡¶°‡ßá ‡¶õ‡¶¨‡¶ø; `squeeze()` ‚Üí \[1,28,28]‚Üí\[28,28]‡•§
* `cmap='gray'` ‚Üí ‡¶ó‡ßç‡¶∞‡ßá‡¶∏‡ßç‡¶ï‡ßá‡¶≤; RGB ‡¶π‡¶≤‡ßá ‡¶¨‡¶æ‡¶¶ ‡¶¶‡¶æ‡¶ì‡•§
* ‡¶∂‡¶ø‡¶∞‡ßã‡¶®‡¶æ‡¶Æ‡ßá ‡¶∏‡¶§‡ßç‡¶Ø ‡¶≤‡ßá‡¶¨‡ßá‡¶≤ ‡¶ì ‡¶™‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶ï‡¶∂‡¶®‡•§

---

# 9) End-to-End ‡¶∞‡¶æ‡¶®‚Äî‡¶≤‡¶æ‡¶á‡¶®‡ßá‚Äì‡¶≤‡¶æ‡¶á‡¶®‡ßá

```
train(model, device, train_loader, optimizer, criterion, epochs=5)
```

* ‡ß´ ‡¶á‡¶™‡¶ï ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç; **epochs ‡¶¨‡¶æ‡ßú‡¶æ‡¶≤‡ßá** ‡¶¨‡ßá‡¶∂‡¶ø ‡¶∂‡ßá‡¶ñ‡ßá (‡¶ì‡¶≠‡¶æ‡¶∞‡¶´‡¶ø‡¶ü‡¶ø‡¶Ç ‡¶π‡¶≤‡ßá ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡¶ø‡¶°‡ßá‡¶∂‡¶®/‡¶Ü‡¶∞‡ßç‡¶≤‡¶ø-‡¶∏‡ßç‡¶ü‡¶™‡¶ø‡¶Ç ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞)‡•§

```
test(model, device, test_loader)
```

* ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶∏‡ßá‡¶ü‡ßá ‡¶´‡¶æ‡¶á‡¶®‡¶æ‡¶≤ ‡¶è‡¶ï‡¶ø‡¶â‡¶∞‡ßá‡¶∏‡¶ø‡•§

```
visualize_predictions(model, device, test_loader)
```

* ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶∏‡ßç‡¶Ø‡¶æ‡¶Æ‡ßç‡¶™‡¶≤‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶ï‡¶∂‡¶® vs ‡¶∏‡¶§‡ßç‡¶Ø ‡¶≤‡ßá‡¶¨‡ßá‡¶≤ ‡¶¶‡ßá‡¶ñ‡¶æ‡ßü (‡¶°‡¶ø‡¶¨‡¶æ‡¶ó/‡¶¨‡ßã‡¶ù‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶£)‡•§

---    


---

## 1) Imports (‡¶ï‡ßÄ ‡¶ï‡ßá‡¶®)

1. `matplotlib.pyplot as plt` ‚Äî ‡¶™‡ßç‡¶≤‡¶ü/‡¶≠‡¶ø‡¶ú‡ßÅ‡ßü‡¶æ‡¶≤‡¶æ‡¶á‡¶ú‡ßá‡¶∂‡¶®
2. `torch` ‚Äî ‡¶ü‡ßá‡¶®‡¶∏‡¶∞, ‡¶Ö‡¶ü‡ßã-‡¶ó‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶°, ‡¶°‡¶ø‡¶≠‡¶æ‡¶á‡¶∏
3. `torch.nn as nn` ‚Äî ‡¶≤‡ßá‡ßü‡¶æ‡¶∞/‡¶Æ‡¶°‡ßá‡¶≤/‡¶≤‡¶∏ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏
4. `torch.optim as optim` ‚Äî ‡¶Ö‡¶™‡ßç‡¶ü‡¶ø‡¶Æ‡¶æ‡¶á‡¶ú‡¶æ‡¶∞ (Adam/SGD/‚Ä¶)
5. `DataLoader` ‚Äî ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ö‡ßá ‡¶°‡ßá‡¶ü‡¶æ ‡¶≤‡ßã‡¶°
6. `datasets, transforms` ‚Äî ‡¶∞‡ßá‡¶°‡¶ø‡¶Æ‡ßá‡¶° ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü ‡¶ì ‡¶™‡ßç‡¶∞‡¶ø-‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç

---

## 2) Data Processing

1. **Transforms (MNIST)**

   * `ToTensor()` ‚Üí ‡¶á‡¶Æ‡ßá‡¶ú ‚Üí ‡¶ü‡ßá‡¶®‡¶∏‡¶∞ \[0,1]
   * `Normalize((0.1307,), (0.3081,))` ‚Üí ‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡¶æ‡¶∞‡ßç‡¶°‡¶æ‡¶á‡¶ú
2. **Dataset**

   * `datasets.MNIST(..., train=True/False, download=True, transform=transform)`
3. **DataLoader**

   * Train: `batch_size=64`, `shuffle=True`
   * Test: `batch_size=1000`, `shuffle=False`
   * (‡¶ê‡¶ö‡ßç‡¶õ‡¶ø‡¶ï) `num_workers` (Linux 2‚Äì8), `pin_memory=True` (CUDA)

---

## 3) Model Architecture (MLP for MNIST)

1. ‡¶á‡¶®‡¶™‡ßÅ‡¶ü: \[N, 1, 28, 28] ‚Üí `Flatten()` ‚Üí \[N, **784**]
2. ‡¶≤‡ßá‡ßü‡¶æ‡¶∞: `Linear(784,128)` ‚Üí `ReLU` ‚Üí `Linear(128,64)` ‚Üí `ReLU` ‚Üí `Linear(64,10)`
3. ‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü: **logits** \[N,10] (Softmax ‡¶≤‡¶∏ ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡¶á ‡¶π‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡ßá‡¶≤ ‡¶ï‡¶∞‡ßá)

---

## 4) Device Setup

1. `device = torch.device("cuda" if torch.cuda.is_available() else "cpu")`
2. **‡¶∞‡ßÅ‡¶≤:** model ‡¶è‡¶¨‡¶Ç data‚Äî**‡¶è‡¶ï‡¶á device** ‡¶è ‡¶•‡¶æ‡¶ï‡¶§‡ßá ‡¶π‡¶¨‡ßá

---

## 5) Model, Loss, Optimizer

1. `model = DigitClassifier().to(device)`
2. `criterion = nn.CrossEntropyLoss()` ‚Äî ‡¶Æ‡¶æ‡¶≤‡ßç‡¶ü‡¶ø-‡¶ï‡ßç‡¶≤‡¶æ‡¶∏; logits ‡¶á‡¶®‡¶™‡ßÅ‡¶ü
3. `optimizer = optim.Adam(model.parameters(), lr=1e-3)`

   * (‡¶™‡ßç‡¶∞‡ßã) AdamW + `weight_decay=1e-4` ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞‡ßç‡¶Ø

---

## 6) Training Loop ‚Äî ‡¶ß‡¶æ‡¶™‡ßá ‡¶ß‡¶æ‡¶™‡ßá (‡¶∏‡¶¨‡¶∏‡¶Æ‡ßü ‡¶è‡¶≠‡¶æ‡¶¨‡ßá‡¶á ‡¶Ø‡¶æ‡¶¨‡ßá)

1. **Mode set:** `model.train()`
2. **Batch loop:** `for data, targets in train_loader:`
3. **Device move:** `data, targets = data.to(device), targets.to(device)`
4. **Zero grads:** `optimizer.zero_grad()`
5. **Forward:** `outputs = model(data)`
6. **Loss:** `loss = criterion(outputs, targets)`
7. **Backward:** `loss.backward()`
8. **(Optional) Clip:** `torch.nn.utils.clip_grad_norm_(...)`
9. **Update:** `optimizer.step()`
10. **Log:** ‡¶™‡ßç‡¶∞‡¶§‡¶ø `log_interval` ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ö‡ßá ‡¶≤‡¶∏ ‡¶™‡ßç‡¶∞‡¶ø‡¶®‡ßç‡¶ü
11. **Epoch end:** `avg_train_loss = running_loss / len(train_loader)`
12. **(Recommended) Validation:** eval+no\_grad, val loss/acc
13. **(Optional) Scheduler:** `scheduler.step(val_loss)` / `scheduler.step()`
14. \*\*(Optional) Early stopping / Checkpoint save\`

---

## 7) Evaluation (Validation/Test) Loop ‚Äî ‡¶ß‡¶æ‡¶™‡ßá ‡¶ß‡¶æ‡¶™‡ßá

1. **Mode set:** `model.eval()`
2. **No grad:** `with torch.no_grad():`
3. **Batch loop:** `for data, targets in data_loader:`
4. **Device move:** `data, targets = data.to(device), targets.to(device)`
5. **Forward:** `outputs = model(data)`
6. **Loss (optional but good):** `loss = criterion(outputs, targets)` ‚Üí `total_loss += loss.item()`
7. **Predictions:** `preds = outputs.argmax(dim=1)`
8. **Metrics:** `correct += (preds == targets).sum().item()`; `total += targets.size(0)`
9. **Aggregate:** `avg_loss = total_loss / len(data_loader)`; `accuracy = correct / total`
10. **Return/Log:** `avg_loss, accuracy`

---

## 8) Prediction Visualization ‚Äî ‡¶ß‡¶æ‡¶™‡ßá ‡¶ß‡¶æ‡¶™‡ßá

1. **Mode set:** `model.eval()`
2. **Get a batch:** `images, labels = next(iter(test_loader))`
3. **Device move:** `images, labels = images.to(device), labels.to(device)`
4. **Forward:** `outputs = model(images)`
5. **Predictions:** `preds = outputs.argmax(dim=1)`
6. **CPU for plotting:** `images, labels, preds = images.cpu(), labels.cpu(), preds.cpu()`
7. **Figure:** `plt.figure(figsize=(2*n, 3))`
8. **Loop 0..n-1:** `subplot ‚Üí imshow(images[i].squeeze(), cmap='gray') ‚Üí title(True/Pred) ‚Üí axis off`
9. **Neat layout:** `plt.tight_layout(); plt.show()`

   * (‡¶ü‡¶ø‡¶™) `n = min(n, images.size(0))` ‡¶¶‡¶ø‡ßü‡ßá ‡¶Ü‡¶â‡¶ü-‡¶Ö‡¶´-‡¶∞‡ßá‡¶û‡ßç‡¶ú ‡¶è‡ßú‡¶æ‡¶ì
   * RGB ‡¶π‡¶≤‡ßá `permute(1,2,0)`, `cmap` ‡¶¨‡¶æ‡¶¶

---

## 9) End-to-End ‡¶∞‡¶æ‡¶® ‡¶Ö‡¶∞‡ßç‡¶°‡¶æ‡¶∞

1. **Transforms ‡¶∏‡ßá‡¶ü** (train/test)
2. **Dataset ‡¶≤‡ßã‡¶°** (`download=True`)
3. **DataLoader** (train: shuffle=True; test: shuffle=False)
4. **Device ‡¶∏‡¶ø‡¶≤‡ßá‡¶ï‡ßç‡¶ü**
5. **Model ‚Üí device**, **Loss**, **Optimizer**
6. **Train (N epochs)** ‚Üí avg train loss
7. **Test/Validation** ‚Üí avg loss, accuracy
8. **Visualization** ‚Üí ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶™‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶ï‡¶∂‡¶® ‡¶™‡ßç‡¶≤‡¶ü
9. **(Optional)** Scheduler / Early stopping / Checkpoint

---

## 10) ‡¶ü‡¶ø‡¶â‡¶®‡¶ø‡¶Ç ‡¶ö‡¶ø‡¶ü‡¶∂‡¶ø‡¶ü

* **batch\_size** ‚Üë ‚Üí ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§/‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶¨‡¶≤, RAM ‚Üë; ‚Üì ‚Üí ‡¶ß‡ßÄ‡¶∞, ‡¶ï‡¶ñ‡¶®‡ßã ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡¶æ‡¶≤‡¶æ‡¶á‡¶ú ‚Üë
* **lr** ‚Üë ‚Üí ‡¶´‡¶æ‡¶∏‡ßç‡¶ü/‡¶Ö‡¶∏‡ßç‡¶•‡¶ø‡¶∞; ‚Üì ‚Üí ‡¶∏‡ßç‡¶≤‡ßã/‡¶™‡ßç‡¶≤‡ßá‡¶ü‡ßã ‚Üí Scheduler ‡¶á‡¶â‡¶ú‡¶´‡ßÅ‡¶≤
* **epochs** ‚Üë ‚Üí ‡¶∂‡ßá‡¶ñ‡¶æ ‚Üë/‡¶ì‡¶≠‡¶æ‡¶∞‡¶´‡¶ø‡¶ü ‚Üë ‚Üí Early stopping ‡¶∏‡¶π‡¶æ‡ßü‡¶ï
* **optimizer** ‚Üí AdamW (‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡ßç‡¶ü), ‡¶¨‡ßú ‡¶≠‡¶ø‡¶∂‡¶®‡ßá SGD+momentum + cosine/step LR
* **regularization** ‚Üí weight decay, dropout, data augmentation
* **perf** ‚Üí Linux ‡¶è `num_workers` ‡¶¨‡¶æ‡ßú‡¶æ‡¶ì; CUDA ‡¶π‡¶≤‡ßá `pin_memory=True`; AMP ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßã

---

‡¶è‡¶ü‡¶æ‡¶á **‡¶∏‡¶ø‡¶∞‡¶ø‡ßü‡¶æ‡¶≤-‡¶ì‡ßü‡¶æ‡¶á‡¶ú ‡¶´‡ßÅ‡¶≤ ‡¶∏‡¶æ‡¶Æ‡¶æ‡¶∞‡¶ø**‚ÄîImports ‚Üí Data ‚Üí Model ‚Üí Device ‚Üí Loss/Opt ‚Üí Train ‚Üí Eval ‚Üí Viz ‚Üí Run ‚Üí Tuning‡•§


