
---

# üß† Understanding **Tensors** in PyTorch

---

## üìå What is a Tensor?

A **tensor** is a multi-dimensional array, similar to a NumPy array, but with additional capabilities optimized for deep learning. Tensors are the **basic building blocks** of PyTorch.

> üî∏ Think of tensors as generalizations of scalars, vectors, and matrices to **n-dimensions**.

| Tensor Type | PyTorch Shape                               | Description         |
| ----------- | ------------------------------------------- | ------------------- |
| Scalar      | `torch.tensor(5)` ‚Üí `[]`                    | Single number       |
| Vector      | `torch.tensor([1, 2, 3])` ‚Üí `[3]`           | 1D tensor           |
| Matrix      | `torch.tensor([[1, 2], [3, 4]])` ‚Üí `[2, 2]` | 2D tensor           |
| n-D Tensor  | Higher dimensions                           | 3D, 4D, ... tensors |

---

## üîß Creating Tensors

### 1. **From Python Lists**

```python
import torch

# 1D Tensor (vector)
x = torch.tensor([1, 2, 3])
# 2D Tensor (matrix)
y = torch.tensor([[1.0, 2.0], [3.0, 4.0]])
```

### 2. **With Initial Values**

```python
torch.zeros((2, 3))      # 2x3 tensor filled with zeros
torch.ones((2, 2))       # 2x2 tensor filled with ones
torch.full((2, 2), 7)    # tensor filled with 7s
torch.eye(3)             # identity matrix
```

### 3. **With Random Values**

```python
torch.rand(2, 3)         # Uniform distribution [0,1)
torch.randn(2, 3)        # Normal distribution (mean=0, std=1)
```

### 4. **From NumPy**

```python
import numpy as np
arr = np.array([[1, 2], [3, 4]])
tensor = torch.from_numpy(arr)
```

---

## üîÑ Tensor Operations

### ‚û§ Basic Math

```python
a = torch.tensor([1, 2])
b = torch.tensor([3, 4])
print(a + b)       # tensor([4, 6])
print(a * b)       # tensor([3, 8])
```

### ‚û§ Matrix Multiplication

```python
x = torch.tensor([[1, 2], [3, 4]])
y = torch.tensor([[5, 6], [7, 8]])
print(torch.mm(x, y))
```

### ‚û§ Reshaping

```python
x = torch.arange(9)       # tensor([0, 1, ..., 8])
x = x.reshape(3, 3)
```

### ‚û§ Indexing and Slicing

```python
x = torch.tensor([[1, 2, 3], [4, 5, 6]])
x[0]         # tensor([1, 2, 3])
x[:, 1]      # column index 1 ‚Üí tensor([2, 5])
```

---

## üíª Working with GPU

Move a tensor to GPU (if available):

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
x = torch.tensor([1.0, 2.0, 3.0], device=device)
```

---

## üîÅ Gradients Support (for ML)

Make a tensor require gradients (for autograd):

```python
x = torch.tensor([2.0], requires_grad=True)
y = x ** 2
y.backward()
print(x.grad)  # Should print 4.0
```

---

## üîÑ Tensors vs NumPy Arrays

| Feature       | `torch.Tensor`                | `numpy.ndarray` |
| ------------- | ----------------------------- | --------------- |
| GPU support   | ‚úÖ (with `.cuda()`)            | ‚ùå               |
| Auto-diff     | ‚úÖ (with `autograd`)           | ‚ùå               |
| Deep learning | ‚úÖ (PyTorch-native)            | ‚ùå               |
| Interoperable | ‚úÖ (`from_numpy()`, `numpy()`) | ‚úÖ               |

---

## üß™ Examples to Try Yourself

```python
# Element-wise operations
a = torch.tensor([2.0, 3.0])
b = torch.tensor([4.0, 5.0])
print(torch.pow(a, 2))         # tensor([4., 9.])
print(torch.sqrt(b))           # tensor([2.0, 2.2361])

# Broadcasting
a = torch.tensor([[1], [2]])
b = torch.tensor([3, 4])
print(a + b)                   # shape becomes [2,2] due to broadcasting
```

---

## üìö Summary

| Concept         | Example                    |
| --------------- | -------------------------- |
| Create          | `torch.tensor([1,2])`      |
| Shape           | `x.shape`                  |
| Device transfer | `x.to("cuda")`             |
| Gradients       | `requires_grad=True`       |
| Operations      | `+`, `*`, `.matmul()` etc. |

---

---

## üåü ‡ß®‡ß¶‡¶ü‡¶ø PyTorch Tensor ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ 

1. **‡¶´‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü‡¶∞‡¶ø creation** (zeros, ones, rand, full ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø)

   ```python
   torch.zeros(2,3); torch.ones(2,2); torch.rand(2,3); torch.full((2,2), 7)
   ```

   ([docs.pytorch.org][1])

2. **torch.empty() ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ì uninitialized memory ‡¶™‡ßç‡¶∞‡¶¶‡¶∞‡ßç‡¶∂‡¶®**

   ```python
   x = torch.empty(3,4); print(x)
   ```

   ([docs.pytorch.org][1])

3. **element-wise multiplication, sum ‡¶ì indexing**

   ```python
   a = torch.randn(2,3); b = torch.randn(2,3)
   print(a * b); print(a.sum()); print(a[1,2])
   ```

   ([Wikipedia][2])

4. **‡¶≤‡¶ø‡¶®‡¶∏‡ßç‡¶™‡ßá‡¶∏ ‡¶ì ‡¶ü‡ßç‡¶∞‡¶ø‡¶ó ‡¶∏‡¶æ‡¶á‡¶® ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® fitting**

   ```python
   x = torch.linspace(-math.pi, math.pi, 2000)
   y = torch.sin(x)
   y_pred = a + b*x + c*x**2 + d*x**3
   loss = (y_pred - y).pow(2).sum()
   ```

   ([docs.pytorch.org][3], [h-huang.github.io][4])

5. **‡¶ü‡ßá‡¶®‡¶∏‡¶∞ ‡¶°‡¶ø‡¶≠‡¶æ‡¶á‡¶∏ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ (CPU/GPU)**

   ```python
   device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
   x = torch.tensor([1.0,2.0,3.0], device=device)
   ```

   ([docs.pytorch.org][3], [Wikipedia][2])

6. **NumPy ‡¶•‡ßá‡¶ï‡ßá ‡¶ü‡ßá‡¶®‡¶∏‡¶∞ ‡¶ï‡¶®‡¶≠‡¶æ‡¶∞‡ßç‡¶∂‡¶®**

   ```python
   arr = np.array([[1,2],[3,4]])
   tensor = torch.from_numpy(arr)
   ```

   ([GitHub][5])

7. **reshape ‡¶ì transpose**

   ```python
   x = torch.arange(9).reshape(3,3)
   y = x.t()
   ```

   ([docs.pytorch.org][1])

8. **broadcasting ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£**

   ```python
   a = torch.tensor([[1],[2]]); b = torch.tensor([3,4])
   print(a + b)  # shape becomes [2,2]
   ```

   ([armanasq.github.io][6])

9. **logical reductions ‡¶ì axis-wise reductions**

   ```python
   t.sum(dim=0); t.mean(dim=1); t.min(), t.max()
   ```

   ([armanasq.github.io][6])

10. **one-hot encoding utility**

    ```python
    idx = [2,0,1]
    onehot = torch.nn.functional.one_hot(torch.tensor(idx), num_classes=3)
    ```

    ([GitHub][7])

11. **column normalization (mean‚Äëstd normalization)**

    ```python
    x = torch.rand(4,4)
    norm = (x - x.mean(dim=0)) / x.std(dim=0)
    ```

    ([GitHub][7])

12. **batched matrix multiplication**

    ```python
    x = torch.rand(10,3,4); y = torch.rand(10,4,5)
    out = torch.bmm(x,y)
    ```

    ([GitHub][7])

13. **slice indexing ‡¶ì modifying tensor**

    ```python
    t = torch.arange(16).reshape(4,4)
    t[:2, :2] = -1
    ```

    ([GitHub][7])

14. **reverse rows ‡¶ì shuffle columns**

    ```python
    rev = t.flip(0); shuffled = t[:, torch.randperm(t.size(1))]
    ```

    ([GitHub][7])

15. **scatter‚Äëgather ‡¶¨‡¶æ masked operations**

    ```python
    mask = (x > 0.5)
    x[mask] = 1.0
    ```

    ([armanasq.github.io][6])

16. **einsum ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ (advanced broadcasting)**

    ```python
    out = torch.einsum('ij,jk->ik', mat1, mat2)
    ```

    ([armanasq.github.io][6])

17. **detaching tensors from autograd**

    ```python
    y = x.detach()
    ```

    ([armanasq.github.io][6])

18. **requires\_grad ‡¶ì gradient calculation**

    ```python
    x = torch.tensor(2.0, requires_grad=True)
    y = x**3 + 2*x
    y.backward()
    print(x.grad)
    ```

    ([armanasq.github.io][6])

19. **tensor.to(dtype=torch.float16) mixed precision**

    ```python
    x = x.half()
    ```

    ([armanasq.github.io][6])

20. **saving ‡¶ì loading tensor**

    ```python
    torch.save(t, 'tensor.pt'); t2 = torch.load('tensor.pt')
    ```

    ([GitHub][8])

---

## üß† ‡¶™‡ßç‡¶∞‡¶§‡ßç‡¶Ø‡ßá‡¶ï ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£‡ßá‡¶∞ ‡¶Æ‡ßÇ‡¶≤ ‡¶≤‡¶ï‡ßç‡¶∑‡ßç‡¶Ø:

* **Tensor creation**, **indexing**, **reshaping**, **broadcasting**, **reductions**, **device handling**, **autograd**, ‡¶è‡¶¨‡¶Ç **utilities**‚Äî‡¶∏‡¶¨‡¶ó‡ßÅ‡¶≤‡ßã ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶ß‡¶æ‡¶∞‡¶£‡¶æ‡•§
* ‡¶ê‡¶§‡¶ø‡¶π‡¶æ‡¶∏‡¶ø‡¶ï ‡¶ì ‡¶∂‡¶ø‡¶ï‡ßç‡¶∑‡¶æ‡¶Æ‡ßÇ‡¶≤‡¶ï ‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø‡ßá ‡¶Ö‡¶´‡¶ø‡¶∏‡¶ø‡ßü‡¶æ‡¶≤ ‡¶ü‡¶ø‡¶â‡¶ü‡ßã‡¶∞‡¶ø‡ßü‡¶æ‡¶≤, Gist ‡¶è‡¶¨‡¶Ç GitHub ‡¶∞‡¶ø‡¶™‡ßã‡¶ú‡¶ø‡¶ü‡¶∞‡¶ø ‡¶•‡ßá‡¶ï‡ßá ‡¶®‡ßá‡ßü‡¶æ ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§

---

## ‚úÖ ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶∂‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶™‡¶∞‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶™‡¶¶‡¶ï‡ßç‡¶∑‡ßá‡¶™:

* ‡¶™‡ßç‡¶∞‡¶§‡ßç‡¶Ø‡ßá‡¶ï ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ ‡¶®‡¶ø‡¶ú‡ßá ‡¶ü‡¶æ‡¶á‡¶™ ‡¶ï‡¶∞‡ßá ‡¶ö‡¶æ‡¶≤‡¶æ‡¶® ‡¶ì ‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü ‡¶™‡¶∞‡ßç‡¶Ø‡¶¨‡ßá‡¶ï‡ßç‡¶∑‡¶£ ‡¶ï‡¶∞‡ßÅ‡¶®‡•§
* `.shape`, `.dtype`, `.device`, `.grad_fn`, `.requires_grad` ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ ‡¶ï‡¶∞‡ßá ‡¶¨‡ßÅ‡¶ù‡ßÅ‡¶®‡•§
* ‡¶õ‡ßã‡¶ü ‡¶õ‡ßã‡¶ü ‡¶™‡ßç‡¶∞‡¶ú‡ßá‡¶ï‡ßç‡¶ü ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®: polynomial fitting, matrix ops, broadcasting challenges ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø‡•§

---


[1]: https://docs.pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html?utm_source=chatgpt.com "Introduction to PyTorch Tensors ‚Äî PyTorch Tutorials 2.7.0+cu126 ..."
[2]: https://en.wikipedia.org/wiki/PyTorch?utm_source=chatgpt.com "PyTorch"
[3]: https://docs.pytorch.org/tutorials/beginner/pytorch_with_examples?utm_source=chatgpt.com "Learning PyTorch with Examples ‚Äî PyTorch Tutorials 2.7.0+cu126 ..."
[4]: https://h-huang.github.io/tutorials/beginner/pytorch_with_examples.html?utm_source=chatgpt.com "Learning PyTorch with Examples ‚Äî PyTorch Tutorials 1.8.1+cu102 ..."
[5]: https://github.com/ml-dev-world/pytorch-fundamentals?utm_source=chatgpt.com "ml-dev-world/pytorch-fundamentals - GitHub"
[6]: https://armanasq.github.io/Deep-Learning/PyTorch-Tensors/?utm_source=chatgpt.com "A Profound Journey into PyTorch Tensors: A Comprehensive Tutorial"
[7]: https://github.com/AnmolGulati6/PyTorch-101-Tensor-Operations-and-Utilities?utm_source=chatgpt.com "PyTorch-101-Tensor-Operations-and-Utilities - GitHub"
[8]: https://github.com/sedwna/PyTorch-Tensor-Basics/?utm_source=chatgpt.com "GitHub - sedwna/PyTorch-Tensor-Basics: This repository contains ..."

---

## ‡ßß. Polynomial Fitting to Sine Function

**‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø**: ‡¶è‡¶ï‡¶ü‡¶ø ‡ß©‡ßü‚Äë‡¶Ö‡¶∞‡ßç‡¶°‡¶æ‡¶∞ ‡¶™‡¶≤‡¶ø‡¶®‡ßã‡¶Æ‡¶ø‡ßü‡¶æ‡¶≤ ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® fit ‡¶ï‡¶∞‡¶æ ‡¶ï‡ßã‡¶®‡¶ì dataset‚Äë‡¶è (‡¶Ø‡ßá‡¶Æ‡¶® sine wave)
**‡¶ï‡ßã‡¶°**:

```python
x = torch.linspace(-math.pi, math.pi, 2000)
y = torch.sin(x)
a,b,c,d = torch.randn((),requires_grad=True), ...
for t in range(2000):
    y_pred = a + b*x + c*x**2 + d*x**3
    loss = (y_pred - y).pow(2).sum()
    loss.backward()
    # optimizer.step() style update
```

‡¶è‡¶ñ‡¶æ‡¶®‡ßá tensor arithmetic, `.pow()`, `.sum()`, gradient tracking ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡ßü‡ßá‡¶õ‡ßá ([docs.pytorch.org][1], [Python Mania][2])‡•§

---

## ‡ß®. Image Classification (MNIST ‡¶¨‡¶æ CIFAR‚Äë10)

**‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø**: ‡¶ü‡ßá‡¶®‡¶∏‡¶∞ ‡¶¶‡¶ø‡ßü‡ßá input data, CNN, loss, optimizer training ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶æ
**‡¶ï‡ßã‡¶°**:

```python
images = images.to(device)
labels = labels.to(device)
outputs = model(images)
loss = criterion(outputs, labels)
loss.backward()
optimizer.step()
```

Tensor batch operations, `.to(device)`, gradient update ‡¶á‡¶§‡ßç‡¶Ø‡¶æ‡¶¶‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§‡•§ ‡¶è‡¶á ‡¶ï‡¶æ‡¶†‡¶æ‡¶Æ‡ßã‡¶ü‡¶ø standard vision models‚Äë‡¶è ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡ßü ([myscale.com][3], [Placement Preparation][4])‡•§

---

## ‡ß©. Text Classification using LSTM

**‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø**: ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡¶ï‡ßá ‡¶ü‡ßã‡¶ï‡ßá‡¶® ‡¶ï‡¶∞‡ßá LSTM ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶ï sentiment classification ‡¶ï‡¶∞‡¶æ
**‡¶ï‡ßã‡¶°**:

```python
embedded = embedding(input_seq)
output, (h_n, c_n) = lstm(embedded)
logits = classifier(h_n[-1])
```

‡¶ü‡ßá‡¶®‡¶∏‡¶∞ shape manipulation, embeddings & RNN ‡¶´‡¶ø‡¶° ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü ([GitHub][5])‡•§

---

## ‡ß™. Transfer Learning with Pretrained ResNet

**‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø**: pretrained CNN model ‡¶´‡¶æ‡¶á‡¶®‚Äë‡¶ü‡¶ø‡¶â‡¶® ‡¶ï‡¶∞‡¶æ
**‡¶ï‡ßã‡¶°**:

```python
model = torchvision.models.resnet18(pretrained=True)
for param in model.parameters(): param.requires_grad = False
model.fc = nn.Linear(512, num_classes)
```

‡¶ü‡ßá‡¶®‡¶∏‡¶∞ forward/backward ‡¶¨‡¶æ‡¶∏‡¶ø operations ‡¶∏‡¶π structure modification ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü ([Placement Preparation][4], [ProjectPro][6])‡•§

---

## ‡ß´. DCGAN Image Generation

**‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø**: GAN‚Äëbased image generation, tensors ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá noise vector & generated images ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§
**‡¶ï‡ßã‡¶°**:

```python
noise = torch.randn(batch_size, nz, 1, 1, device=device)
fake = netG(noise)
```

‡¶ü‡ßá‡¶®‡¶∏‡¶∞ initialization, backward on generator loss, ‡¶ì discriminator loss‚Äì‡¶è gradient tracking ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü ([docs.pytorch.org][1])‡•§

---

## ‡ß¨. Plant Disease Detection (Image Segmentation)

**‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø**: Leaf image ‡¶•‡ßá‡¶ï‡ßá segmentation mask predict ‡¶ï‡¶∞‡¶æ U‚ÄëNet model ‡¶¶‡¶ø‡ßü‡ßá
**‡¶ï‡ßã‡¶°**:

```python
pred = model(input_image)
loss = criterion(pred, mask)
loss.backward()
```

‡¶ü‡ßá‡¶®‡¶∏‡¶∞ dimension handling, BCE/softmax loss ‡¶¨‡ßç‡¶Ø‡¶æ‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá segmentation output ‡¶è gradient tracking ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü ([Placement Preparation][4])‡•§

---

## ‡ß≠. Emotion Recognition from Video (Audio + Vision tensors)

**‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø**: ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì ‡¶¨‡¶æ audio ‡¶•‡ßá‡¶ï‡ßá emotion classify ‡¶ï‡¶∞‡¶æ‡¶®‡ßã
**‡¶ï‡ßã‡¶°**:

```python
audio_feat = torchaudio.transforms.MFCC()(waveform)
video_feat = vision_backbone(frames)
combined = torch.cat((audio_feat, video_feat), dim=1)
out = classifier(combined)
```

‡¶ü‡ßá‡¶®‡¶∏‡¶∞ concatenation, multi‚Äëmodal fusion ‡¶¶‡ßá‡¶ñ‡¶æ‡¶®‡ßã ‡¶π‡ßü ([Omdena][7])‡•§

---

## ‡ßÆ. Stock Price Prediction using LSTM Time Series

**‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø**: past price series predict ‡¶ï‡¶∞‡¶æ LSTM ‡¶¶‡¶ø‡ßü‡ßá
**‡¶ï‡ßã‡¶°**:

```python
out, _ = lstm(seq_batch)
pred = linear(out[:, -1, :])
```

Time‚Äëseries ‡¶ü‡ßá‡¶®‡¶∏‡¶∞ shapes, sequence batch processing, tensor gradients ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡ßü ([cognitiveclass.ai][8], [ProjectPro][6])‡•§

---

## ‡ßØ. Siamese Network for Image Similarity

**‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø**: Siamese twin network ‡¶¶‡¶ø‡ßü‡ßá two-image embeddings compare ‡¶ï‡¶∞‡¶æ
**‡¶ï‡ßã‡¶°**:

```python
out1 = net(img1)
out2 = net(img2)
dist = F.pairwise_distance(out1, out2)
loss = contrastive_loss(dist, label)
loss.backward()
```

Tensor distance, embedding generation, contrastive loss tracking ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡ßü ([myscale.com][3])‡•§

---

## üîü Image Segmentation with U‚ÄëNet on Medical Data

**‡¶â‡¶¶‡ßç‡¶¶‡ßá‡¶∂‡ßç‡¶Ø**: Retina ‡¶¨‡¶æ MRI image segmentation
**‡¶ï‡ßã‡¶°**:

```python
pred_mask = model(image_tensor)
loss = dice_loss(pred_mask, mask_tensor)
loss.backward()
optimizer.step()
```

Tensor operations, element-wise dice coefficient, training loop integration‚Äîall shown here ([cognitiveclass.ai][8], [ProjectPro][6])‡•§

---

### üß≠ ‡¶∏‡¶æ‡¶∞‡¶∏‡¶Ç‡¶ï‡ßç‡¶∑‡ßá‡¶™:

| ‡¶™‡ßç‡¶∞‡¶ú‡ßá‡¶ï‡ßç‡¶ü             | ‡¶Æ‡ßÇ‡¶≤ ‡¶ü‡ßá‡¶®‡¶∏‡¶∞ ‡¶ï‡¶®‡¶∏‡ßá‡¶™‡ßç‡¶ü                     |
| -------------------- | ------------------------------------- |
| Polynomial fitting   | tensor math, autograd                 |
| CNN Classifier       | batch tensors, loss, optimizer        |
| LSTM Text            | embeddings, seq‚Äëtensor                |
| Transfer Learning    | pretrained weights, requires\_grad    |
| GAN                  | noise tensor, generator/discriminator |
| Image Segmentation   | multi-dimensional input/output        |
| Multi-modal fusion   | concat, feature tensors               |
| Time Series LSTM     | sequence tensors, prediction          |
| Siamese Network      | embedding distance tensors            |
| Medical Segmentation | mask tensors, dice loss               |

---



[1]: https://docs.pytorch.org/tutorials/beginner/pytorch_with_examples.html?utm_source=chatgpt.com "Learning PyTorch with Examples"
[2]: https://pythonmania.org/how-to-use-pytorch-the-ultimate-guide-case-study-example/?utm_source=chatgpt.com "How to Use PyTorch: The Ultimate Guide + Case Study + Example"
[3]: https://myscale.com/blog/master-pytorch-example-projects-hands-on-guide-learning/?utm_source=chatgpt.com "Master PyTorch Example Projects: Your Hands-On Guide"
[4]: https://www.placementpreparation.io/blog/pytorch-project-ideas-for-beginners/?utm_source=chatgpt.com "10 Best PyTorch Project Ideas for Beginners [With Source Code]"
[5]: https://github.com/es-OmarHani/pytorch-projects?utm_source=chatgpt.com "PyTorch Projects and Tutorials - GitHub"
[6]: https://www.projectpro.io/projects/data-science-projects/pytorch?utm_source=chatgpt.com "5+ PyTorch Projects for Beginners with Source Code to Practice"
[7]: https://www.omdena.com/blog/best-pytorch-projects?utm_source=chatgpt.com "Best PyTorch Projects for Beginners in 2024 - Omdena"
[8]: https://cognitiveclass.ai/learn/pytorch-projects?utm_source=chatgpt.com "PyTorch Projects - cognitiveclass.ai"

# PyTorch Tensor Documentation with Examples

## Introduction to PyTorch Tensors

Tensors are the central data structure in PyTorch, similar to NumPy arrays but with additional GPU acceleration and automatic differentiation capabilities.

---

## Creating Tensors

```python
import torch
import math
```

### 1. Empty Tensor

```python
x = torch.empty(3, 4)
```

* Creates a 3x4 uninitialized tensor

### 2. Zeros, Ones, Random

```python
zeros = torch.zeros(2, 3)
ones = torch.ones(2, 3)
torch.manual_seed(1729)
random = torch.rand(2, 3)
```

### 3. Random Seed Reproducibility

```python
torch.manual_seed(1729)
random1 = torch.rand(2, 3)
random2 = torch.rand(2, 3)
torch.manual_seed(1729)
random3 = torch.rand(2, 3)
```

### 4. Tensor Like

```python
x = torch.empty(2, 2, 3)
empty_like_x = torch.empty_like(x)
zeros_like_x = torch.zeros_like(x)
ones_like_x = torch.ones_like(x)
rand_like_x = torch.rand_like(x)
```

### 5. From Python Collections

```python
some_constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]])
some_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19))
```

---

## Tensor Data Types

```python
a = torch.ones((2, 3), dtype=torch.int16)
b = torch.rand((2, 3), dtype=torch.float64) * 20.
c = b.to(torch.int32)
```

---

## Math & Logic Operations

### Scalar Arithmetic

```python
ones = torch.zeros(2, 2) + 1
twos = torch.ones(2, 2) * 2
threes = (torch.ones(2, 2) * 7 - 1) / 2
fours = twos ** 2
sqrt2s = twos ** 0.5
```

### Tensor-Tensor Arithmetic

```python
fives = ones + fours
dozens = threes * fours
```

### Broadcasting

```python
rand = torch.rand(2, 4)
doubled = rand * (torch.ones(1, 4) * 2)
```

---

## More Math Operations

```python
a = torch.rand(2, 4) * 2 - 1
print(torch.abs(a))
print(torch.ceil(a))
print(torch.floor(a))
print(torch.clamp(a, -0.5, 0.5))
```

### Trigonometry

```python
angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])
sines = torch.sin(angles)
inverses = torch.asin(sines)
```

### Bitwise

```python
b = torch.tensor([1, 5, 11])
c = torch.tensor([2, 7, 10])
print(torch.bitwise_xor(b, c))
```

### Comparisons and Reductions

```python
d = torch.tensor([[1., 2.], [3., 4.]])
e = torch.ones(1, 2)
print(torch.eq(d, e))
print(torch.max(d))
print(torch.mean(d))
print(torch.std(d))
```

---

## Vectors & Matrices

```python
v1 = torch.tensor([1., 0., 0.])
v2 = torch.tensor([0., 1., 0.])
m1 = torch.rand(2, 2)
m2 = torch.tensor([[3., 0.], [0., 3.]])
m3 = torch.linalg.matmul(m1, m2)
print(torch.linalg.svd(m3))
```

---

## In-place Operations

```python
a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])
print(torch.sin(a))

b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])
print(torch.sin_(b))
```

---

## Memory-Efficient Operations

```python
a = torch.rand(2, 2)
b = torch.rand(2, 2)
c = torch.zeros(2, 2)
torch.matmul(a, b, out=c)
```

---

## Copying and Cloning Tensors

```python
a = torch.ones(2, 2)
b = a.clone()
c = a.detach().clone()
```

---

## Moving to Accelerators (GPU)

```python
my_device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
x = torch.rand(2, 2, device=my_device)
y = x.to(my_device)
```

---

## Manipulating Tensor Shapes

### Unsqueeze / Squeeze

```python
a = torch.rand(3, 226, 226)
b = a.unsqueeze(0)
c = b.squeeze(0)
```

### Reshape

```python
output3d = torch.rand(6, 20, 20)
input1d = output3d.reshape(6 * 20 * 20)
```

---

## NumPy Interoperability

```python
import numpy as np
numpy_array = np.ones((2, 3))
torch_tensor = torch.from_numpy(numpy_array)

pytorch_rand = torch.rand(2, 3)
numpy_rand = pytorch_rand.numpy()
```

### Shared Memory

```python
numpy_array[1, 1] = 23
print(torch_tensor)

pytorch_rand[1, 1] = 17
print(numpy_rand)
```

---


