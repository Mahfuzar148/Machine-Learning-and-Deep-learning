
---

## ğŸ”¶ 1. **Importing PyTorch**

```python
import torch
```

* This imports the PyTorch library so you can use its functions.

---

## ğŸ”¶ 2. **Creating Tensors**

### ğŸ”¸ Random tensor

```python
x = torch.rand(3)
print(x)
```

* Creates a 1D tensor of 3 elements with **random values between 0 and 1**.

### ğŸ”¸ Empty tensor

```python
x1 = torch.empty(3)
```

* Creates an uninitialized 1D tensor of size 3. Contents are **random garbage values** (whatever is in memory).

```python
x2 = torch.empty(2, 3)
x3 = torch.empty(2, 3, 4)
```

* Same idea but with different shapes:

  * `x2`: 2 rows, 3 columns
  * `x3`: 2 blocks, 3 rows, 4 columns

---

## ğŸ”¶ 3. **Other Tensor Initializations**

```python
rand = torch.rand(2, 3, 4)
```

* 3D tensor with random values between 0 and 1

```python
zeros = torch.zeros(2, 3, 4)
```

* 3D tensor filled with **zeros**

```python
ones = torch.ones(2, 3, 4, dtype=torch.int)
```

* 3D tensor filled with **ones**, with **integer data type**

```python
print(ones.dtype)
```

* Prints data type: `torch.int32` or `torch.int64`, depending on system

### ğŸ”¸ Changing data types

```python
ones = torch.ones(2, 3, 4, dtype=torch.double)
print(ones.dtype)  # torch.float64
```

```python
ones = torch.ones(2, 3, 4, dtype=torch.float32)
print(ones.dtype)  # torch.float32
```

```python
print(ones.size())
```

* Prints the shape: `torch.Size([2, 3, 4])`

---

## ğŸ”¶ 4. **Basic Tensor Math**

### ğŸ”¸ Element-wise addition

```python
x = torch.rand(2, 2)
y = torch.rand(2, 2)
print(x + y)
print(torch.add(x, y))
```

* `x + y` and `torch.add(x, y)` do the same thing: **element-wise addition**

### ğŸ”¸ In-place addition

```python
y.add_(x)
print(y)
```

* Adds `x` to `y` **in-place**, modifying `y` directly (notice the `_`)

### ğŸ”¸ Subtraction

```python
print(x - y)
print(torch.sub(x, y))
```

### ğŸ”¸ Multiplication

```python
print(x * y)
print(torch.mul(x, y))
```

### ğŸ”¸ Division

```python
print(x / y)
print(torch.div(x, y))
```

---

## ğŸ”¶ 5. **In-Place Arithmetic**

```python
a = torch.rand(2, 3)
a1 = a  # `a1` and `a` point to the same memory
b = torch.rand(2, 3)
```

### ğŸ”¸ In-place subtraction

```python
a.sub_(b)
```

* `a` is updated: `a = a - b`

### ğŸ”¸ Reset `a` to original (still same as `a1`)

```python
a = a1
```

### ğŸ”¸ In-place multiplication

```python
a.mul_(b)
```

### ğŸ”¸ In-place division

```python
a.div_(b)
```

### ğŸ”¸ Safe division (returns new tensor)

```python
print(torch.div(a, b))
```

---

## ğŸ§  Summary of Important Concepts:

| Operation | Description                 | In-place Version |
| --------- | --------------------------- | ---------------- |
| `x + y`   | Element-wise addition       | `y.add_(x)`      |
| `x - y`   | Element-wise subtraction    | `x.sub_(y)`      |
| `x * y`   | Element-wise multiplication | `x.mul_(y)`      |
| `x / y`   | Element-wise division       | `x.div_(y)`      |


---

## ğŸ”¶ Code Overview: Tensor Indexing & Conversion

```python
import torch
x = torch.rand(3, 4)
print(x)
```

### âœ… Explanation:

* Creates a **2D tensor** with 3 rows and 4 columns filled with random values.
* Shape: `(3, 4)`

---

## ğŸ”¶ 1. **Accessing All Rows and Columns**

```python
print(x[:, :])
```

### âœ… Explanation:

* This is **full slicing**: `:` means **"take everything"**.
* `x[:, :]` = All rows, all columns (same as just `x`).

> ğŸ§  Similar to Python NumPy slicing: `matrix[row_slice, column_slice]`

---

## ğŸ”¶ 2. **Accessing a Specific Row**

```python
print(x[0, :])
```

### âœ… Explanation:

* First row (index 0), all columns.
* Shape: `(4,)` â†’ a 1D tensor with 4 values.

```python
print(x[1, :])
```

* Second row (index 1), all columns.

---

## ğŸ”¶ 3. **Accessing Specific Elements**

```python
print(x[0, 0])  # First row, first column
print(x[0, 1])  # First row, second column
print(x[1, 2])  # Second row, third column
```

### âœ… Explanation:

* Direct indexing for single scalar values inside the 2D tensor.
* Each returns a **0-dimensional tensor (scalar)**, not a number yet.

---

## ğŸ”¶ 4. **Convert Scalar Tensor to Python Number**

```python
print(x[1, 2].item())
```

### âœ… Explanation:

* `.item()` converts a 0D tensor to a Python number (e.g., `float`).
* âœ… Use only for **scalar tensors** (i.e., single values), otherwise it will raise an error.

---

## ğŸ”¶ 5. **Convert Tensor to NumPy Array**

```python
print(x.numpy())
```

### âœ… Explanation:

* Converts the entire tensor `x` to a NumPy array.
* Very useful when you need to use NumPy operations on PyTorch tensors.

> âš ï¸ Works **only if the tensor is on CPU**, not on GPU.
> If `x` is on GPU, use `x.cpu().numpy()` instead.

---

## ğŸ”¶ 6. **Get Tensor Shape**

```python
print(x.shape)
```

### âœ… Explanation:

* Returns the **dimensions** of the tensor.
* In this case: `torch.Size([3, 4])`
  Which means: 3 rows, 4 columns

> âœ… You can also use `x.size()` â€” same as `x.shape`.

---

## ğŸ§  Summary Table

| Expression              | Meaning                                | Output Type         |
| ----------------------- | -------------------------------------- | ------------------- |
| `x[:, :]`               | All rows, all columns                  | 2D tensor (3x4)     |
| `x[0, :]`               | First row                              | 1D tensor (4,)      |
| `x[1, 2]`               | Element at row 1, column 2             | Scalar tensor (0D)  |
| `x[1, 2].item()`        | Convert scalar tensor to Python number | float               |
| `x.numpy()`             | Convert tensor to NumPy array          | NumPy ndarray       |
| `x.shape` or `x.size()` | Shape of tensor                        | `torch.Size([3,4])` |

---

---

## ğŸ”· Code Breakdown

```python
x = torch.rand(4, 4)
print(x)
```

* Creates a `4x4` tensor (2D) with random values.
* Shape: `(4, 4)` â†’ Total elements = **16**

---

### ğŸ”¹ Reshape with `view(16)`

```python
y = x.view(16)
print(y)
```

âœ… **Explanation**:

* `view(16)` reshapes the tensor to a **1D tensor** of 16 elements.
* Shape: `(16,)`
* âœ… No data is lost or added â€” just rearranged.

---

### ğŸ”¹ Reshape with `view(-1, 8)`

```python
y1 = x.view(-1, 8)
print(y1)
```

âœ… **Explanation of `-1`**:

* `-1` tells PyTorch:
  **"Automatically calculate this dimension based on the other one."**

* `x` has 16 elements. Youâ€™re asking PyTorch to reshape it into shape `(_, 8)`.

* So PyTorch calculates:

  $$
  \frac{16}{8} = 2
  $$

  Thus, the shape becomes `(2, 8)`

âœ”ï¸ Final shape: `torch.Size([2, 8])`

```python
print(y1.size())  # Outputs: torch.Size([2, 8])
```

---

## ğŸ”¶ ğŸ“Œ Summary: What does `-1` do in `view()`?

| Statement          | Resulting Shape | Why                      |
| ------------------ | --------------- | ------------------------ |
| `x.view(16)`       | `[16]`          | Flattens into 1D         |
| `x.view(-1, 8)`    | `[2, 8]`        | PyTorch infers 2         |
| `x.view(4, -1)`    | `[4, 4]`        | PyTorch infers 4 columns |
| `x.view(-1, 2, 2)` | `[4, 2, 2]`     | Nested reshape inferred  |

> âœ… **Rule**: Only one dimension can be `-1` in any `view()` call. Otherwise, PyTorch throws an error.

---

You're exploring **data sharing between PyTorch and NumPy**, which is a very important and subtle topic when working with both libraries. Let's break your example down and provide a detailed tutorial.

---

# ğŸ” What This Tutorial Covers

1. **Convert PyTorch Tensor â†’ NumPy Array**
2. **Convert NumPy Array â†’ PyTorch Tensor**
3. **Memory Sharing between NumPy & PyTorch**
4. **When data is copied vs shared**
5. **In-place operations and side effects**

---

## ğŸ§  Key Concepts

| Conversion Direction | Function                  | Shares Memory? |
| -------------------- | ------------------------- | -------------- |
| PyTorch â†’ NumPy      | `.numpy()`                | âœ… Yes          |
| NumPy â†’ PyTorch      | `torch.from_numpy(array)` | âœ… Yes          |
| PyTorch â†’ NumPy      | `tensor.clone().numpy()`  | âŒ No (copy)    |
| NumPy â†’ PyTorch      | `torch.tensor(array)`     | âŒ No (copy)    |

---

## âœ… Step-by-Step Explanation

### ğŸ”¹ 1. PyTorch Tensor to NumPy Array using `.numpy()`

```python
import torch

a = torch.ones(5)
b = a.numpy()
print(b)  # [1. 1. 1. 1. 1.]
print(type(b))  # <class 'numpy.ndarray'>
```

* `torch.ones(5)` creates a tensor: `[1, 1, 1, 1, 1]`
* `.numpy()` **returns a NumPy view**, not a copy.
* Both `a` and `b` point to the **same memory**.

### ğŸ§ª Try modifying `a`

```python
a[0] = 2
print(b)  # [2. 1. 1. 1. 1.]
```

* Modifying `a` updates `b` as well. âœ… They share memory.

### ğŸ§ª Try in-place addition on `a`

```python
a.add_(1)  # In-place addition: a = a + 1
print(a)  # [3. 2. 2. 2. 2.]
print(b)  # [3. 2. 2. 2. 2.]
```

* Since `add_()` is in-place, it also updates `b`.

---

### ğŸ”¹ 2. NumPy Array to PyTorch Tensor using `torch.from_numpy()`

```python
import numpy as np

x = np.ones(5)
y = torch.from_numpy(x)
print(y)  # tensor([1., 1., 1., 1., 1.])
print(type(y))  # <class 'torch.Tensor'>
```

* `torch.from_numpy()` also **shares memory**.

### ğŸ§ª Try modifying `x`

```python
x += 1
print(x)  # [2. 2. 2. 2. 2.]
print(y)  # tensor([2., 2., 2., 2., 2.])
```

* Modifying `x` affects `y` too. âœ… Shared memory.

---

## â—Important: When They DONâ€™T Share Memory

### ğŸ”¸ Using `torch.tensor()` (copies data)

```python
x = np.ones(5)
y = torch.tensor(x)  # makes a copy
x += 1
print(x)  # [2. 2. 2. 2. 2.]
print(y)  # tensor([1., 1., 1., 1., 1.])
```

* `torch.tensor()` copies data: no shared memory

### ğŸ”¸ Using `.clone()` on tensor

```python
a = torch.ones(5)
b = a.clone().numpy()
a[0] = 9
print(a)  # [9., 1., 1., 1., 1.]
print(b)  # [1., 1., 1., 1., 1.]
```

* `.clone()` ensures the NumPy array is **independent**.

---

## ğŸ“Œ When to Use Which?

| Use Case                                       | Recommended Method             |
| ---------------------------------------------- | ------------------------------ |
| Want memory-efficient data sharing             | `.numpy()` or `from_numpy()`   |
| Want independent copy (safe from side effects) | `torch.tensor()` or `.clone()` |

---

## ğŸ§  In-Place vs Out-of-Place Operations

| Operation   | Description           | Affects Shared Memory |
| ----------- | --------------------- | --------------------- |
| `a.add(1)`  | Returns a new tensor  | âŒ No                  |
| `a.add_(1)` | In-place modification | âœ… Yes                 |

So if you use `add_()`, it affects the shared memory. If you use `add()`, it creates a new object and doesnâ€™t affect the NumPy view.

---

## âœ… Summary Table

| Task                              | Code                              | Shared Memory? |
| --------------------------------- | --------------------------------- | -------------- |
| Tensor â†’ NumPy                    | `tensor.numpy()`                  | âœ… Yes          |
| Tensor â†’ NumPy (safe)             | `tensor.clone().numpy()`          | âŒ No           |
| NumPy â†’ Tensor                    | `torch.from_numpy(array)`         | âœ… Yes          |
| NumPy â†’ Tensor (safe)             | `torch.tensor(array)`             | âŒ No           |
| In-place op (`add_`, `mul_`, etc) | Changes NumPy array or tensor too | âœ… Yes          |
| Out-of-place (`add`, `mul`)       | Creates new object                | âŒ No           |

---

## ğŸ’¡ Real World Tip

When debugging unexpected tensor or NumPy array changes, always check:

* Are you using `.from_numpy()` or `.numpy()`?
* Are you using in-place operations (`add_`, `mul_`, etc)?
* Do you need memory sharing or a copy?

---



