
---

# ğŸ“˜ Documentation: Computation Graph and Backpropagation

---

## ğŸ”¹ 1. Overview

This documentation explains how a **computation graph** helps visualize operations in a model, and how **backpropagation** computes gradients used for training.

We use a simple **linear regression model** with:

* Input: `x`
* Weight (parameter): `w`
* True output: `y`
* Prediction: `Å· = w * x`
* Loss function: Squared Error â†’ `Loss = (Å· - y)Â²`

---

## ğŸ”¹ 2. Computation Graph

A **computation graph** breaks the full function into smaller operations:

```
x â†’ [ * ] â†’ Å· â†’ [ - ] â†’ s â†’ [ Â² ] â†’ L
w â†—          y â†—
```

* `[ * ]`: Multiply `w * x` â†’ Å· (prediction)
* `[ - ]`: Subtract `Å· - y` â†’ s (error)
* `[ Â² ]`: Square the error â†’ L (loss)

---

## ğŸ”¹ 3. Forward Pass (Prediction and Loss)

We compute the model's output and the loss.

### Example Values:

* `x = 1`
* `y = 2`
* `w = 1`

### Step-by-step:

1. **Prediction**:
   Å· = w Ã— x = 1 Ã— 1 = 1

2. **Error**:
   s = Å· - y = 1 - 2 = -1

3. **Loss**:
   L = sÂ² = (-1)Â² = 1

---

## ğŸ”¹ 4. Local Gradients (Each Operationâ€™s Derivatives)

To use backpropagation, we need partial derivatives from each operation.

### Multiplication (Å· = w Ã— x):

* âˆ‚Å·/âˆ‚w = x = 1
* âˆ‚Å·/âˆ‚x = w = 1

### Subtraction (s = Å· - y):

* âˆ‚s/âˆ‚Å· = 1
* âˆ‚s/âˆ‚y = -1

### Squaring (L = sÂ²):

* âˆ‚L/âˆ‚s = 2s = -2

---

## ğŸ”¹ 5. Backward Pass (Using Chain Rule)

We now compute how loss L changes with respect to each input (w, x, y).

### Step 1: âˆ‚L/âˆ‚s

$$
\frac{âˆ‚L}{âˆ‚s} = 2s = 2 Ã— (-1) = -2
$$

### Step 2: âˆ‚L/âˆ‚Å·

$$
\frac{âˆ‚L}{âˆ‚Å·} = \frac{âˆ‚L}{âˆ‚s} Ã— \frac{âˆ‚s}{âˆ‚Å·} = -2 Ã— 1 = -2
$$

### Step 3: âˆ‚L/âˆ‚w

$$
\frac{âˆ‚L}{âˆ‚w} = \frac{âˆ‚L}{âˆ‚Å·} Ã— \frac{âˆ‚Å·}{âˆ‚w} = -2 Ã— 1 = -2
$$

### Step 4: âˆ‚L/âˆ‚x

$$
\frac{âˆ‚L}{âˆ‚x} = \frac{âˆ‚L}{âˆ‚Å·} Ã— \frac{âˆ‚Å·}{âˆ‚x} = -2 Ã— 1 = -2
$$

### Step 5: âˆ‚L/âˆ‚y

$$
\frac{âˆ‚L}{âˆ‚y} = \frac{âˆ‚L}{âˆ‚s} Ã— \frac{âˆ‚s}{âˆ‚y} = -2 Ã— (-1) = 2
$$

---

## ğŸ”¹ 6. Final Gradients Summary

| Variable | Gradient (âˆ‚L/âˆ‚Variable) |
| -------- | ----------------------- |
| w        | -2                      |
| x        | -2                      |
| y        | +2                      |

---

## ğŸ”¹ 7. General Gradient Formulas

For any x, y, w:

* Prediction:

  $$
  \hat{y} = w Ã— x
  $$

* Loss:

  $$
  L = (\hat{y} - y)^2
  $$

* Gradients:

  * $$
    âˆ‚L/âˆ‚w = 2(wÃ—x - y) Ã— x
    $$
  * $$
    âˆ‚L/âˆ‚x = 2(wÃ—x - y) Ã— w
    $$
  * $$
    âˆ‚L/âˆ‚y = -2(wÃ—x - y)
    $$

---

## ğŸ”¹ 8. Second Example (Perfect Prediction)

### Input:

* x = 1, y = 2, w = 2

### Forward:

* Å· = 2 Ã— 1 = 2
* Loss = (2 - 2)Â² = 0

### Backward:

* âˆ‚L/âˆ‚w = 0
* (No update needed because the model is already correct.)

---

## ğŸ”¹ 9. Weight Update (Gradient Descent)

Using learning rate Î·, the weight update rule is:

$$
w_{new} = w - Î· Ã— âˆ‚L/âˆ‚w
$$

Example:
If Î· = 0.1 and âˆ‚L/âˆ‚w = -2,
Then:

$$
w = 1 \Rightarrow w_{new} = 1 - 0.1 Ã— (-2) = 1.2
$$

---

## ğŸ”¹ 10. Summary

| Step            | Purpose                                |
| --------------- | -------------------------------------- |
| Forward Pass    | Compute prediction and loss            |
| Local Gradients | Derivatives of each operation          |
| Backward Pass   | Use chain rule to find total gradients |
| Weight Update   | Adjust weights to reduce loss          |

Backpropagation is how neural networks **learn**. It tells us **how much each weight contributes to the error**, so we can adjust it and improve the model.

---
Absolutely! Letâ€™s build on your understanding by working through **medium** and **complex** examples of computation graphs and backpropagation using step-by-step calculations.

---

# ğŸ” More Examples: Computation Graph & Backpropagation

Weâ€™ll move from a **medium example** (still 1 layer, but with a bias term), to a **complex example** (2-layer neural network).

---

## ğŸŸ¦ Example 1: Medium â€” Linear Regression with Bias

### Model:

$$
\hat{y} = w \cdot x + b
$$

$$
\text{Loss} = (\hat{y} - y)^2
$$

### Given:

* $x = 2$
* $y = 5$
* $w = 1$
* $b = 0$

---

### âœ… Forward Pass:

1. **Prediction**:

$$
\hat{y} = w \cdot x + b = 1 \cdot 2 + 0 = 2
$$

2. **Error**:

$$
s = \hat{y} - y = 2 - 5 = -3
$$

3. **Loss**:

$$
L = s^2 = (-3)^2 = 9
$$

---

### ğŸ” Backward Pass (Gradients):

Letâ€™s compute gradients using the **chain rule**.

#### Step 1:

$$
\frac{âˆ‚L}{âˆ‚s} = 2s = 2 Ã— (-3) = -6
$$

#### Step 2:

$$
\frac{âˆ‚s}{âˆ‚\hat{y}} = 1
\Rightarrow \frac{âˆ‚L}{âˆ‚\hat{y}} = -6
$$

#### Step 3:

* $\frac{âˆ‚\hat{y}}{âˆ‚w} = x = 2$
* $\frac{âˆ‚\hat{y}}{âˆ‚b} = 1$

Then:

* $\frac{âˆ‚L}{âˆ‚w} = \frac{âˆ‚L}{âˆ‚\hat{y}} \cdot \frac{âˆ‚\hat{y}}{âˆ‚w} = -6 Ã— 2 = -12$
* $\frac{âˆ‚L}{âˆ‚b} = \frac{âˆ‚L}{âˆ‚\hat{y}} \cdot \frac{âˆ‚\hat{y}}{âˆ‚b} = -6 Ã— 1 = -6$

---

### ğŸ“Œ Final Gradients:

| Variable | Gradient (âˆ‚L/âˆ‚var) |
| -------- | ------------------ |
| w        | -12                |
| b        | -6                 |

> These tell us: Increase both `w` and `b` to reduce the loss.

---

## ğŸŸ¨ Example 2: Complex â€” 2-Layer Neural Network

### Model:

$$
\begin{aligned}
z_1 &= w_1 \cdot x + b_1 \quad &&\text{(Linear)}\\
a_1 &= \text{ReLU}(z_1) \quad &&\text{(Activation)} \\
z_2 &= w_2 \cdot a_1 + b_2 \quad &&\text{(Linear)}\\
\hat{y} &= z_2 \\
L &= (\hat{y} - y)^2 \quad &&\text{(Loss)}
\end{aligned}
$$

---

### Given:

* $x = 1$
* $y = 4$
* $w_1 = 1$, $b_1 = 0$
* $w_2 = 2$, $b_2 = 0$

---

### âœ… Forward Pass:

1. $z_1 = w_1 \cdot x + b_1 = 1 \cdot 1 + 0 = 1$
2. $a_1 = \text{ReLU}(z_1) = \max(0, 1) = 1$
3. $z_2 = w_2 \cdot a_1 + b_2 = 2 \cdot 1 + 0 = 2$
4. $\hat{y} = z_2 = 2$
5. $s = \hat{y} - y = 2 - 4 = -2$
6. $L = s^2 = 4$

---

### ğŸ” Backward Pass:

We go in reverse using the **chain rule**:

#### Step 1:

$$
\frac{âˆ‚L}{âˆ‚s} = 2s = -4
\quad \text{and} \quad \frac{âˆ‚L}{âˆ‚\hat{y}} = -4
$$

#### Step 2:

$$
\hat{y} = z_2 \Rightarrow \frac{âˆ‚L}{âˆ‚z_2} = -4
$$

#### Step 3:

$$
z_2 = w_2 \cdot a_1 + b_2
$$

* $\frac{âˆ‚z_2}{âˆ‚w_2} = a_1 = 1$
* $\frac{âˆ‚z_2}{âˆ‚a_1} = w_2 = 2$
* $\frac{âˆ‚z_2}{âˆ‚b_2} = 1$

Then:

* $\frac{âˆ‚L}{âˆ‚w_2} = -4 \cdot 1 = -4$
* $\frac{âˆ‚L}{âˆ‚b_2} = -4 \cdot 1 = -4$
* $\frac{âˆ‚L}{âˆ‚a_1} = -4 \cdot 2 = -8$

---

### Step 4: Backprop through ReLU

ReLU:

$$
a_1 = \text{ReLU}(z_1) = \max(0, z_1)
\Rightarrow \frac{da_1}{dz_1} = 1 \text{ (since } z_1 = 1 > 0\text{)}
$$

So:

$$
\frac{âˆ‚L}{âˆ‚z_1} = \frac{âˆ‚L}{âˆ‚a_1} \cdot \frac{da_1}{dz_1} = -8 Ã— 1 = -8
$$

---

### Step 5: Backprop to wâ‚ and bâ‚

$$
z_1 = w_1 \cdot x + b_1
\Rightarrow \frac{âˆ‚z_1}{âˆ‚w_1} = x = 1,\quad \frac{âˆ‚z_1}{âˆ‚b_1} = 1
$$

Then:

* $\frac{âˆ‚L}{âˆ‚w_1} = -8 \cdot 1 = -8$
* $\frac{âˆ‚L}{âˆ‚b_1} = -8 \cdot 1 = -8$

---

### ğŸ“Œ Final Gradients:

| Variable | Gradient (âˆ‚L/âˆ‚var) |
| -------- | ------------------ |
| wâ‚       | -8                 |
| bâ‚       | -8                 |
| wâ‚‚       | -4                 |
| bâ‚‚       | -4                 |

---

## ğŸ§  Summary of Insights:

| Concept                  | Key Point                                                              |
| ------------------------ | ---------------------------------------------------------------------- |
| Computation Graph        | Breaks model into nodes to track gradients easily                      |
| Forward Pass             | Calculates prediction and loss                                         |
| Backward Pass (Backprop) | Uses chain rule to compute gradients layer-by-layer                    |
| ReLU                     | Gradient is 0 when input < 0, and 1 when input > 0                     |
| Gradient Descent         | Uses gradients to update parameters in the direction of loss reduction |

---


